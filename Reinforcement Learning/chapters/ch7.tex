\chapter{Planning and Learning with Tabular Methods}

In this chapter we develop a unified view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-difference methods. These are respectively called \textbf{model-based} and \textbf{model-free} reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real differences between these two kinds of methods, there are also great similarities. In particular, \textbf{the heart of both kinds of methods is the computation of value functions}. Moreover, all the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function.

\section{Models and Planning}

By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. Given a state and an action, a model produces a prediction of the resultant next state and next reward. If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring. Some models produce a description of all possibilities and their probabilities; these we call \textbf{distribution models}. Other models produce just one of the possibilities, sampled according to the probabilities; these we call \textbf{sample models}.

The kind of model assumed in dynamic programming—estimates of the MDP's dynamics, $p(s', r|s, a)$—is a distribution model. The kind of model used in the blackjack example in Chapter 5 is a sample model. Distribution models are stronger than sample models in that they can always be used to produce samples. However, in many applications it is much easier to obtain sample models than distribution models.

Models can be used to mimic or simulate experience. Given a starting state and action, a sample model produces a possible transition, and a distribution model generates all possible transitions weighted by their probabilities of occurring. Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities. In either case, we say the model is used to simulate the environment and produce simulated experience.

The word planning is used in several different ways in different fields. We use the term to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment:

\begin{center}
model $\rightarrow$ planning $\rightarrow$ policy
\end{center}

State-space planning, which includes the approach we take in this book, is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal. Actions cause transitions from state to state, and value functions are computed over states. In what we call plan-space planning, planning is instead a search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans.

The unified view we present in this chapter is that all state-space planning methods share a common structure, a structure that is also present in the learning methods presented in this book. It takes the rest of the chapter to develop this view, but there are two basic ideas: (1) all state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and (2) they compute value functions by updates or backup operations applied to simulated experience.

Dynamic programming methods clearly fit this structure: they make sweeps through the space of states, generating for each state the distribution of possible transitions. Each distribution is then used to compute a backed-up value (update target) and update the state's estimated value. Viewing planning methods in this way emphasizes their relationship to the learning methods that we have described in this book. The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/q_planning_code.png}
\label{fig:q_planning_code}
\end{figure}

\section{Dyna: Integrated Planning, Acting, and Learning}

When planning is done online, while interacting with the environment, a number of interesting issues arise. New information gained from the interaction may change the model and thereby interact with planning. It may be desirable to customize the planning process in some way to the states or decisions currently under consideration, or expected in the near future. If decision making and model learning are both computation-intensive processes, then the available computational resources may need to be divided between them. To begin exploring these issues, in this section we present Dyna-Q, a simple architecture integrating the major functions needed in an online planning agent.

Within a planning agent, there are at least two roles for real experience: it can be used to improve the model (to make it more accurately match the real environment) and it can be used to directly improve the value function and policy using the kinds of reinforcement learning methods we have discussed in previous chapters. The former we call \textbf{model-learning}, and the latter we call \textbf{direct reinforcement learning} (direct RL).

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{assets/dyna_q_diagram.png}
\label{fig:dyna_q_diagram}
\end{figure}

Both direct and indirect methods have advantages and disadvantages. Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions. On the other hand, direct methods are much simpler and are not affected by biases in the design of the model. Some have argued that indirect methods are always superior to direct ones, while others have argued that direct methods are responsible for most human and animal learning.

Dyna-Q includes all of the processes shown in the diagram above—planning, acting, model-learning, and direct RL—all occurring continually. The planning method is the random-sample one-step tabular Q-planning method. The direct RL method is one-step tabular Q-learning. The model-learning method is also table-based and assumes the environment is deterministic. After each transition $S_t, A_t \rightarrow R_{t+1}, S_{t+1}$, the model records in its table entry for $S_t, A_t$ the prediction that $R_{t+1}, S_{t+1}$ will deterministically follow. Thus, if the model is queried with a state–action pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction.

During planning, the Q-planning algorithm randomly samples only from state–action pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{assets/dyna_q_architecture.png}
\caption{The general Dyna Architecture.}
\label{fig:dyna_q_architecture}
\end{figure}

Conceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents. For concreteness and implementation on a serial computer, however, we fully specify the order in which they occur within a time step. In Dyna-Q, the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/dyna_q_code.png}
    \label{fig:dyna_q_code}
\end{figure}

\section{When the Model Is Wrong}

In the maze example presented in the previous section, the changes in the model were relatively modest. The model started out empty, and was then filled only with exactly correct information. In general, we cannot expect to be so fortunate. Models may be incorrect because the environment is stochastic and only a limited number of samples have been observed, or because the model was learned using function approximation that has generalized imperfectly, or simply because the environment has changed and its new behavior has not yet been observed. When the model is incorrect, the planning process is likely to compute a suboptimal policy.

In some cases, the suboptimal policy computed by planning quickly leads to the discovery and correction of the modeling error. This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible. The planned policy attempts to exploit these opportunities and in doing so discovers that they do not exist.

Greater difficulties arise when the environment changes to become better than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever.

The general problem here is another version of the conflict between exploration and exploitation. In a planning context, exploration means trying actions that improve the model, whereas exploitation means behaving in the optimal way given the current model.

We want the agent to explore to find changes in the environment, but not so much that performance is greatly degraded. As in the earlier exploration/exploitation conflict, there probably is no solution that is both perfect and practical, but simple heuristics are often effective.

\section{Prioritized Sweeping}

In the Dyna agents presented in the preceding sections, simulated transitions are started in state–action pairs selected uniformly at random from all previously experienced pairs. But a uniform selection is usually not the best; planning can be much more efficient if simulated transitions and updates are focused on particular state–action pairs.

This example suggests that search might be usefully focused by working backward from goal states. Of course, we do not really want to use any methods specific to the idea of "goal state." We want methods that work for general reward functions. Goal states are just a special case, convenient for stimulating intuition. In general, we want to work back not just from goal states but from any state whose value has changed. Suppose that the values are initially correct given the model. Suppose now that the agent discovers a change in the environment and changes its estimated value of one state, either up or down. Typically, this will imply that the values of many other states should also be changed, but the only useful one-step updates are those of actions that lead directly into the one state whose value has been changed.

This general idea might be termed \textbf{backward focusing of planning computations}.

It is natural to prioritize the updates according to a measure of their urgency, and perform them in order of priority. This is the idea behind \textbf{prioritized sweeping}. A queue is maintained of every state–action pair whose estimated value would change nontrivially if updated, prioritized by the size of the change. When the top pair in the queue is updated, the effect on each of its predecessor pairs is computed. If the effect is greater than some small threshold, then the pair is inserted in the queue with the new priority. In this way the effects of changes are efficiently propagated backward until quiescence.

Prioritized sweeping is just one way of distributing computations to improve planning efficiency, and probably not the best way. One of prioritized sweeping's limitations is that it uses expected updates, which in stochastic environments may waste lots of computation on low-probability transitions.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/prioritized_sweeping_code.png}
\label{fig:prioritized_sweeping_code}
\end{figure}

We have suggested in this chapter that all kinds of state-space planning can be viewed as sequences of value updates, varying only in the type of update, expected or sample, large or small, and in the order in which the updates are done.

\section{Expected vs. Sample Updates}

Much of this book has been about different kinds of value-function updates, and we have considered a great many varieties. Focusing for the moment on one-step updates, they vary primarily along three binary dimensions. The first two dimensions are whether they update state values or action values and whether they estimate the value for the optimal policy or for an arbitrary given policy. These two dimensions give rise to four classes of updates for approximating the four value functions, $q_*$, $v_*$, $q_\pi$, and $v_\pi$. The other binary dimension is whether the updates are \textbf{expected updates}, considering all possible events that might happen, or \textbf{sample updates}, considering a single sample of what might happen.

When we introduced one-step sample updates in Chapter 6, we presented them as substitutes for expected updates. In the absence of a distribution model, expected updates are not possible, but sample updates can be done using sample transitions from the environment or a sample model. Implicit in that point of view is that expected updates, if possible, are preferable to sample updates. But are they? Expected updates certainly yield a better estimate because they are uncorrupted by sampling error, but they also require more computation, and computation is often the limiting resource in planning.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{assets/expected_vs_sample_updates_backup_diagram.png}
\caption{Backup diagrams for all the one-step updates considered in this chapter.}
\label{fig:expected_vs_sample_updates_backup_diagram}
\end{figure}

For concreteness, consider the expected and sample updates for approximating $q_*$, and the special case of discrete states and actions, a table-lookup representation of the approximate value function, $Q$, and a model in the form of estimated dynamics, $\hat{p}(s', r|s, a)$. The expected update for a state–action pair, $s, a$, is:
\begin{equation}
Q(s, a) \leftarrow \sum_{s',r} \hat{p}(s', r|s, a) \left[ r + \gamma \max_{a'} Q(s', a') \right]
\end{equation}

The corresponding sample update for $s, a$, given a sample next state and reward, $S'$ and $R$ (from the model), is the Q-learning-like update:
\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(S', a') - Q(s, a) \right]
\end{equation}

If there is enough time to complete an expected update, then the resulting estimate is generally better than that of $b$ sample updates because of the absence of sampling error. But if there is insufficient time to complete an expected update, then sample updates are always preferable because they at least make some improvement in the value estimate with fewer than $b$ updates.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/expected_vs_sample_updates_comparison.png}
\caption{Comparison of efficiency of expected and sample updates.}
\label{fig:expected_vs_sample_updates_comparison}
\end{figure}

\section{Trajectory Sampling}

In this section we compare two ways of distributing updates. The classical approach, from dynamic programming, is to perform sweeps through the entire state (or state–action) space, updating each state (or state–action pair) once per sweep. This is problematic on large tasks because there may not be time to complete even one sweep. In many tasks the vast majority of the states are irrelevant because they are visited only under very poor policies or with very low probability.

The second approach is to sample from the state or state–action space according to some distribution. One could sample uniformly, as in the Dyna-Q agent, but this would suffer from some of the same problems as exhaustive sweeps. More appealing is to distribute updates according to the \textbf{on-policy distribution}, that is, according to the distribution observed when following the current policy. One advantage of this distribution is that it is easily generated; one simply interacts with the model, following the current policy. We call this way of generating experience and updates \textbf{trajectory sampling}.

Focusing on the on-policy distribution could be beneficial because it causes vast, uninteresting parts of the space to be ignored, or it could be detrimental because it causes the same old parts of the space to be updated over and over.

\section{Real-time Dynamic Programming}

Real-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of the value-iteration algorithm of dynamic programming (DP). Because it is closely related to conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way some of the advantages that on-policy trajectory sampling can provide. RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates.

If trajectories can start only from a designated set of start states, and if you are interested in the prediction problem for a given policy, then on-policy trajectory sampling allows the algorithm to completely skip states that cannot be reached by the given policy from any of the start states: such states are irrelevant to the prediction problem. For a control problem, where the goal is to find an optimal policy instead of evaluating a given policy, there might well be states that cannot be reached by any optimal policy from any of the start states, and there is no need to specify optimal actions for these irrelevant states. What is needed is an \textbf{optimal partial policy}, meaning a policy that is optimal for the relevant states but can specify arbitrary actions, or even be undefined, for the irrelevant states.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{assets/rt_dp_schema.png}
\label{fig:rt_dp_schema}
\end{figure}

The most interesting result for RTDP is that for certain types of problems satisfying reasonable conditions, RTDP is guaranteed to find a policy that is optimal on the relevant states without visiting every state infinitely often, or even without visiting some states at all. Indeed, in some problems, only a small fraction of the states need to be visited. This can be a great advantage for problems with very large state sets, where even a single sweep may not be feasible.

\section{Planning at Decision Time}

Planning can be used in at least two ways. The one we have considered so far in this chapter, typified by dynamic programming and Dyna, is to use planning to gradually improve a policy or value function on the basis of simulated experience obtained from a model. We call planning used in this way \textbf{background planning}.

The other way to use planning is to begin and complete it after encountering each new state $S_t$, as a computation whose output is the selection of a single action $A_t$; on the next step planning begins anew with $S_{t+1}$ to produce $A_{t+1}$, and so on. Unlike the first use of planning, here planning focuses on a particular state. We call this \textbf{decision-time planning}.

\section{Heuristic Search}

The classical state-space planning methods in artificial intelligence are decision-time planning methods collectively known as \textbf{heuristic search}. In heuristic search, for each state encountered, a large tree of possible continuations is considered. The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root. The backing up within the search tree is just the same as in the expected updates with maxes discussed throughout this book. The backing up stops at the state–action nodes for the current state. Once the backed-up values of these nodes are computed, the best of them is chosen as the current action, and then all backed-up values are discarded.

In conventional heuristic search no effort is made to save the backed-up values by changing the approximate value function. In fact, the value function is generally designed by people and never changed as a result of search.

We should not overlook the most obvious way in which heuristic search focuses updates: on the current state. Much of the effectiveness of heuristic search is due to its search tree being tightly focused on the states and actions that might immediately follow the current state.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/heuristic_search_tree.png}
\caption{Heuristic search can be implemented as a sequence of one-step updates (blu lines) backing up values from the leaf nodes toward the root. The ordering shown here is for a selective depth-first search.}
\label{fig:heuristic_search_tree}
\end{figure}

\section{Rollout Algorithms}

Rollout algorithms are decision-time planning algorithms based on Monte Carlo control applied to simulated trajectories that all begin at the current environment state. They estimate action values for a given policy by averaging the returns of many simulated trajectories that start with each possible action and then follow the given policy. When the action-value estimates are considered to be accurate enough, the action (or one of the actions) having the highest estimated value is executed, after which the process is carried out anew from the resulting next state.

What then do rollout algorithms accomplish? The policy improvement theorem tells us that given any two policies $\pi$ and $\pi'$ that are identical except that $\pi'(s) = a \neq \pi(s)$ for some state $s$, if $q_\pi(s, a) \geq v_\pi(s)$, then policy $\pi'$ is as good as, or better, than $\pi$. This applies to rollout algorithms where $s$ is the current state and $\pi$ is the rollout policy.

In other words, the aim of a rollout algorithm is to improve upon the rollout policy; not to find an optimal policy. Experience has shown that rollout algorithms can be surprisingly effective.

\section{Monte Carlo Tree Search}

Monte Carlo Tree Search (MCTS) is a recent and strikingly successful example of decision-time planning. At its base, MCTS is a rollout algorithm as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively direct simulations toward more highly-rewarding trajectories. MCTS is largely responsible for the improvement in computer Go from a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015.

MCTS is executed after encountering each new state to select the agent's action for that state; it is executed again to select the action for the next state, and so on. The core idea of MCTS is to successively focus multiple simulations starting at the current state by extending the initial portions of trajectories that have received high evaluations from earlier simulations.

For the most part, the actions in the simulated trajectories are generated using a simple policy, usually called a rollout policy as it is for simpler rollout algorithms. Monte Carlo value estimates are maintained only for the subset of state–action pairs that are most likely to be reached in a few steps, which form a tree rooted at the current state. MCTS incrementally extends the tree by adding nodes representing states that look promising based on the results of the simulated trajectories.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/mcts_diagram.png}
    \caption{Monte Carlo Tree Search. When the environment changes to a new state, MCTS executes as many iterations as possible before an action needs to be selected, incrementally building a tree whose root node represents the current state. Each iteration consists of the four operations \textbf{Selection}, \textbf{Expansion}, \textbf{Simulation} and \textbf{Backup}.}
    \label{fig:mcts_diagram}
\end{figure}

Each iteration of a basica version of MCTS consists of the following four steps:
\begin{enumerate}
    \item \textbf{Selection.} Starting at the root node, a tree policy base on the action values attached to the edges of the tree traverses the tree to select a leaf node.
    \item \textbf{Expansion.} On some iterations, the tree is expanded from the selected leaf node by adding one or ore child nodes reached from the selected node via unexplored actions.
    \item \textbf{Simulation.} From the selected node or from one of its newly-added child nodes, simulation of a complete episode is run with actions selected by the rolloud policy. The result is a MC trial with actions selected first by the tree policy and beyond the tree by the rollout policy. 
    \item \textbf{Backup.} The return generated by the simulated episode is backed up to update or to initialize, the action values attached to the edges of the tree traversed by the tree policy in this iteration of MCTS. No value are saved for the states and actions visited by the rollout policy beyond the tree. 
\end{enumerate}

