


\chapter{Introduction}

\section{Models and Probability}
Machine learning studies the construction of models that learn from data. In essence, ML algorithms aim to build representations of the observed data that can be used for prediction or information extraction.

\begin{definitionblock}[Machine Learning]
    \textit{Machine learning is the study and construction of algorithms that can learn from and make predictions on data. \hfill \~ Wikipedia}
\end{definitionblock}

A \textbf{\textit{Model}} is a hypothesis stating that certain features of a complex system are well replicated in a simpler one. In particular:
\begin{itemize}
    \item A \textbf{\textit{Mathematical Model}} uses equations and inequalities to represent relationships.
    \item A \textbf{\textit{Stochastic Model}} is a mathematical model in which probability distributions describe the behavior of the system.
\end{itemize}
Modelling generally begins by defining a family of models indexed by parameters, and machine learning focuses on algorithms that automatically select a suitable model from observed data.

\subsection{Generative and Discriminative Learning}
There are two main learning paradigms:
\begin{itemize}
    \item \textbf{Generative Learning} aims to model the full probability distribution of inputs or input/output pairs:
    $$
    p(x,y)=p(x)p(y\mid x).
    $$
    \item \textbf{Discriminative Learning} focuses on modeling the conditional probability of the output given the input or directly learning a mapping $y=f(x)$.
\end{itemize}
In practice, learning tasks can be categorized as:
\begin{itemize}
    \item \textbf{Supervised Learning}: learning from labeled data.
    \item \textbf{Unsupervised Learning}: discovering hidden structure in unlabeled data.
    \item \textbf{Data Generation}: modeling $p(x)$ (or $p(x|y)$) to generate new samples.
\end{itemize}

\subsection{Inference and Estimation}
Two fundamental operations in probabilistic machine learning are:
\begin{itemize}
    \item \textbf{Inference}: computing marginals and conditional probability distributions using the rules of probability.
    \item \textbf{Estimation}: finding the best parameters or models that explain the observed data.
\end{itemize}
In the Bayesian framework, these two tasks are closely linked.

\subsection{The Importance of Uncertainty}
Uncertainty is at the core of probabilistic modeling. It informs how much we can trust a prediction. For example, if a model predicts a value of 5 with a standard deviation of 0.1, it is much more reliable than if the standard deviation were 5. Uncertainty typically appears in two forms:
\begin{itemize}
    \item \textbf{Aleatoric Uncertainty}: inherent randomness in the data.
    \item \textbf{Epistemic Uncertainty}: due to limited knowledge about the system.
\end{itemize}
In safety-critical applications (e.g. medical diagnosis or autonomous robotics), quantifying uncertainty is essential. For instance, a deep learning system might defer to a human expert if its prediction uncertainty is high.

\subsection{Probability Basics}
Probability provides the mathematical framework to model uncertainty. In our context, \textbf{\textit{Random Variables}} are functions that map outcomes of an experiment to real numbers, with their distributions (either as a probability mass function or probability density function) characterizing the likelihood of different outcomes.

\begin{exampleblock}[Random Variable]
Consider:
$$
\text{Sample Space: } \{ \text{Head, Tail} \}, \quad \text{Random Variable: } \{0,1\}, \quad \text{Distribution: } \left\{\frac{1}{2},\frac{1}{2}\right\}.
$$
Here, the second set represents the values taken by the random variable, while the third set is its probability distribution.
\end{exampleblock}

Common distributions include discrete types (e.g., Binomial, Bernoulli, Poisson) and continuous types (e.g., Gaussian, Exponential, Uniform).

\subsection{Notes' Structure}
These notes are organized to provide a progressive introduction to probabilistic modeling, inference, and learning. After this introductory chapter, subsequent chapters address:
\begin{itemize}
    \item Empirical Risk Minimization and its connection to maximum likelihood.
    \item The theoretical foundations of PAC learning, including sample complexity, VC dimension, and Rademacher complexity.
    \item Probabilistic Graphical Models and various Bayesian approaches.
\end{itemize}