\newpage
\chapter{Empirical Risk Minimization and PAC Learning}

In this chapter, we will introduce the concept of \textbf{Empirical Risk Minimization} (ERM) in which to frame learning problems, the notion of inductive bias, and the main results of algorithmic learnability, encapsulated in the definition of \textbf{Probably Approximately Correct} (PAC) Learning and of complexity of a set of hypothesis, namely
VC-dimension and Rademacher complexity.

\section{Empirical Risk Minimization}

We begin by considering a supervised learning setting in which the \textbf{input space} $X$ is a subset of $\mathbb{R}^n$, and the \textbf{output space} $Y$ can be real-valued (e.g., $Y = \mathbb{R}$), binary (e.g., $Y = \{0,1\}$), or a finite set of classes (e.g., $Y = \{0,1,\ldots,K\}$). In this probabilistic framework, each input-output pair $(x,y)$ is drawn from a joint probability distribution
$$
p(x,y) \;\in\; \mathrm{Dist}(X \times Y),
$$
often referred to as the \emph{\textbf{data generating distribution}}. 

By definition, this distribution factors into the marginal $p(x)$ and the conditional $p(y \mid x)$, so that
$$
p(x,y) \;=\; p(x)\,p(y \mid x).
$$

Because $p(x)$ and $p(y \mid x)$ describe how inputs and outputs are related, it is helpful to write them explicitly. The marginal distribution of $x$ is
$$
p(x) \;=\; \int p(x,y)\,dy,
$$
while the conditional distribution of $y$ given $x$ is
$$
p(y \mid x) \;=\; \frac{p(x,y)}{p(x)}.
$$

A typical dataset $D$ in supervised learning consists of $N$ input-output pairs drawn independently from $p(x,y)$. We denote this as
$$
D \sim p^N(x,y),
$$
which means
$$
D \;=\; \{(x_i, y_i) \,\mid\, i = 1, \dots, N\},
$$
where each $(x_i, y_i)$ is sampled according to the joint distribution $p(x,y)$. 

In many cases, we assume that $p(y \mid x)$ depends on some unknown function of $x$. Formally, one might write
$$
p(y \mid x) \;=\; p\bigl(y \mid f(x)\bigr),
$$
where $f$ is the function we aim to learn. The central objective in supervised learning—through methods such as empirical risk minimization—is to find or approximate this function $f$ by using the observed data $D$.

\section{Risk and Empirical Risk}

$h \in \mathcal{H} \quad x,y \sim p(x,y)$

\textbf{\textit{loss function}} $l(x,y,h) \in \mathbb{R_{\ge 0}}$, 

\begin{itemize}
    \item 0-1 loss: $l(x,y,h) = \mathbb{I}(h(x) \neq y)$, eith $y \in \{0, 1\}$.
    \item squared loss: $l(x,y,h) = (h(x) - y)^2$, with $y \in \mathbb{R}$.
\end{itemize}

We have a probabilistic process, so we have some inputs that are more likely than others. If a model makes a mistake on a more likely input, it should be penalized more.

\begin{definitionblock}[Risk]
The \textbf{\textit{risk}} (or \textbf{\textit{generalization error}}) is defined as:
$$
R(h) = E_{x,y \sim p(x,y)}[l(x,y,h)]
$$ 
\end{definitionblock}

\textbf{Risk minimization principle:}

The goal is to find the hypothesis $h$ that minimizes the risk.
$$
\text{find } h^* \in \mathcal{H} \text{ such that } h^* = \mathrm{arg\,min}_{h \in \mathcal{H}} R(h)
$$

\begin{definitionblock}[Empirical Risk]
The \textbf{\textit{empirical risk}} (or \textbf{\textit{training error}}) is defined as:
$$
\hat R = \dfrac 1N \sum_{i=1}^N l(x_i, y_i, h)
$$
\end{definitionblock}


\textbf{Empirical risk minimization principle:}

The goal is to find the hypothesis $h$ that minimizes the empirical risk.
$$
\text{find } h^*_D = \mathrm{arg\,min}_{h \in \mathcal{H}} \hat R(h)
$$

\subsection{Bias Variance Trade-off}

In this section, we want to analyze the generalization error and decom-
pose it according to the sources of error that we are going to commit.

In what follows, we will use the squared loss (hence we will focus on
regression problems). Considering $h \in \mathcal H$, an explicit expression of the generalization error committed when choosing hypothesis $h$ is:
$$
R(h) = E_p[l(x,y,h)] = \int \int (h(x) - y)^2 p(x,y) dx dy
$$

\newtheorem{theorem}{Theorem}
\begin{theorem}
    The minimizer of the generalization error $R$ is:
    $$
    \boxed{ g(x) = E[y|x] = \int y p(y|x) dy }
    $$
    so that $g = \mathrm{arg\,min}_h R(h),\ if\ g \in \mathcal H$
\end{theorem}

We can rewrite the risk as:
$$
R(h) = \underbrace{\int (h(x) - g(x))^2 p(x) dx}_{=\ 0 \quad iff \quad h(x) = g(x)} + \overbrace{\iint (g(x) - y)^2 p(x,y) dx dy}^{\text{independent of }h \text{: intrinsic noise}}
$$



$$
\begin{array}{rl}
E_D[R(h^*_D)]
& = \underbrace{\int ( E_D[h^*_D(x) - g(x)])^2 p(x) dx}_{bias^2} \\
& + \underbrace{\int E_D[(h^*_D(x) - E_D[h^*_D(x)])^2 p(x) dx]}_{variance} \\
& + \underbrace{\iint (g(x) - y)^2 p(x,y) dx dy}_{noise}
\end{array}
$$

\section{ERM and Maximum Likelihood}

Given a dataset $D = \{(x_i, y_i)\}_{i=1, \dots, m}$ s.t. $D \sim p^m, p = p(x,y)$

We factorize the data generating distributions as: $p(x,y) = p(x) p(y|x)$ and we make an hypothesis on $p(y|x)$, trying to express this conditional probability in a parametric form:
$$p(y|x) = p(y|x, \theta)$$

[check recording for missing part]

We consider the log Likelihood:
$$
L(\theta; D) = \sum_{i = 1}^m \log p(y_i | x_i, \theta)
$$

Then we apply the maximum likelihood principle:

$$
\begin{array}{rcl}
    \argmin{\theta} - L(\theta; D) &
    = & \argmin{\theta} - \dfrac 1m \displaystyle\sum_{i=1}^m \log p(y_i | x_i, \theta) \\
    & = & \argmin{\theta}\ E_{p(x,y)}[-\log p(y | x, \theta)]
\end{array}
$$

since the avarage is an empirical approximation of the expectation.

\begin{observationblock}[Empirical Risk]
$$
-\frac{1}{m}\sum_{i=1}^{m}\log p(y_i|x_i, \theta) \text{is known as \textbf{empirical risk}}
$$
\end{observationblock}



\section{KL divergence}

Consider a probability distribution $p(x)$, then $-\log p(x)$ is a measure of \textbf{self-information}. Indeed, if $p(x) = 1$ then $-\log p(x) = 0$ (no self-information), describing substantially out (lack of) surprise in observing the event. If instead $p(x) = 0$ then $-\log p(x) = \infty$. In general, the more rare the event is, i.e. the lower is $p(x)$, the more self-information it carries, i.e. the larger is $-\log p(x)$.

\begin{definitionblock}[Entropy]
    In an information-theoretic sense, the \textbf{entropy} is a measure of the inforamtion that is carries by a random phenomenon, expressed as the expected amount of self-information that is conveyed by a realization of the random phenomenon.
    
    It is formally defined as:
    $$
    H(p) = E_p[-\log p(x)] = -\int p(x) \log p(x) dx
    $$
    for the continuous case, and:
    $$
    H(p) = E_p[-\log p(x)] = -\sum p(x) \log p(x)
    $$
    for the discrete case.
\end{definitionblock}

For the discrete case, the maximum entropy is achieved for te uniform distrivution and is equal to $\log n$, where $n$ is the number of possible outcomes. In the continuous case, for a fixed variance, the distribution that maximizes the entropy is the Gaussian. The entropy is always 0 if we have a deterministic distribution.

\begin{definitionblock}[Kullback-Leibler divergence]
    The \textbf{Kullback-Leibler divergence} (or \textbf{relative entropy}) between two probability distributions $p(x)$ and $q(x)$ is a measure of how one distribution diverges from a second, expected probability distribution.
    
    It is formally defined as:
    $$
    D_{KL}(p||q) = E_p\left[\log \dfrac{p(x)}{q(x)}\right] = \int p(x) \log \dfrac{p(x)}{q(x)} dx
    $$
    for the continuous case, and:
    $$
    D_{KL}(p||q) = E_p\left[\log \dfrac{p(x)}{q(x)}\right] = \sum p(x) \log \dfrac{p(x)}{q(x)}
    $$
    for the discrete case.
\end{definitionblock}

Intuitively, we are taking a sort of expected difference between $p$ and $q$, expressed in terms of a log odds ratio. It tells us how different two distributions are: the larger \textit{KL} the more different are $p$ and $q$.

Properties of \textit{KL}:
\begin{itemize}
    \item $KL[q||p]$ is a convex function of $q$ and $p$ and $KL[q||p] \ge 0$
    \item KL is non-symmetric, i.e. $KL[q||p] \neq KL[p||q]$
    \item $KL[q||p] = -H[q] - \mathbb{E}_q[\log p]$, where the first term is the entropy and the second term is known as cross-entropy between $p$ and $q$.
\end{itemize}


Suppose moreover, that $p$ is fixed but unknown, $q = q_0$ can vary: what we usually do is trying to find the best $q_{\theta}$ that approximates $p$.

The \textbf{mutual information} between $x$ and $y$ is defined as:
$$
I(x,y) = KL[p(x,y)||p(x)p(y)] = \int \int p(x,y) \log \dfrac{p(x,y)}{p(x)p(y)} dx dy
$$

$KL[p(x,y)||p(x)p(y)] = 0$ iff $x$ and $y$ are independent.

Moreover, the more dependent they are, the more different is $p(x,y)$ from the product of the marginals, the more information $x$ carries about $y$ and viceversa.

In other words, the higher the mutual information is, the more knowing $y$ will tell us about $x$, the less residual uncertainty on $x$ we will have.
\newpage
Consider a dataset: $\underline{x} : x_1,\dots,x_N$:
\begin{definitionblock}[Empirical distribution]
    The \textbf{empirical distribution} of a dataset $\underline{x}$ is defined as:
    $$
    \hat p(\underline{x}) = \dfrac 1N \sum_{i=1}^N \delta(x - x_i)
    $$
    where $\delta(x)$ is the Dirac delta function.
\end{definitionblock}

It is an approximation of the input data generating function$p(x)$. Practically, the more observations we have, the more the empirical distribution will look like $p(x)$.

Given a distribution $q$, we can compute:
$$
KL[p_{emp}||q] = \mathbb{E}_{p_{emp}}\left[\log \dfrac{p_{emp}(x)}{q(x)}\right] = \dfrac 1N \sum_{i=1}^N \log \dfrac{p(x_i)}{q(x_i)}
$$

If $q = q_0$, this is $-\frac{1}{N}L(\Theta)$ plus a constant. Hence maximizing $L(\Theta)$ is essentially equivalent to minimizing the \textit{KL} between $p_{emp}$ and $q_0$. This means that we can always rephrase maximum likelihood in terms of cross-entropy.


\section{PAC Learning}
Our goal is to measure how much we can learn as a function of the model complexity. This results in the \textbf{PAC} (Probably Approximately Correct) \textbf{learning} framework, which encodes the notion of model complexity and gives also bounds on the error that we commit. 
Here we consider it in the context of (binary) classification, i.e. $y \in {0,1}$, using the 0-1 loss. 

\begin{definitionblock}[PAC Learning]
    A realizable hypothesis set $\mathcal{H}$ is \textbf{\textit{PAC learnable}} iff $\forall \epsilon, \delta \in (0,1), \forall p(x,y), \exists m_{\epsilon, \delta} \in \mathcal{N} s.t. \forall m \geq m_{\epsilon, \delta}, \exists D ~ p^m, |D| = m$ then $p_D(R(h^*_D) \leq \epsilon) \geq 1 - \delta$
\end{definitionblock}

This means that, fixing two parameters $\epsilon, \delta \in (0,1)$, governing our precision, and a data generating distribution $p(x,y)$, we can find a number of samples $m_{\epsilon, \delta}$ such that, with probability at least $1 - \delta$, the empirical risk of the best hypothesis $h^*_D$ is less than $\epsilon$. Note that the probability here is over the dataset D, meaning that our learning will succeed for a fraction $1 - \delta$ of sampled datasets. 

In a more general setting, 
\begin{definitionblock}
Given an hypothesis set $\mathcal{H}$ (not necessarily realizable) and an algorithm A, 
$\mathcal{H} is \text{\textbf{agnostic PAC-learnable}} iff \forall \epsilon, \delta \in (0,1), 
\forall p(x,y), \exists m_{\epsilon, \delta} \in \mathcal{N} ~ p^m, 
|D|  = m \geq m_{\epsilon, \delta} \Rightarrow p_D(R(h^*_D) \leq R(h^*) + \epsilon) \geq 1 - \delta$, 
being $h^A_D$ the result of applying A to $\mathcal{H}$ and D.
\end{definitionblock}

This means that, fixing two parameters $\epsilon, \delta \in (0,1)$, governing our precision, and a data generating distribution $p(x,y)$, we can find a number of samples $m_{\epsilon, \delta}$ such that, with probability at least $1 - \delta$, the empirical risk of the best hypothesis $h^*_D$ is less than the risk of the best hypothesis $h^*$ plus $\epsilon$.

\subsection*{Finite hypothesis sets}

An hypothesis set is said to be \textbf{finite} is $\mathcal{H}$ is s.t. $|\mathcal{H}| < \infty$.

Using combinatorial arguments, we can prove that finite hypothesis sets are agnostic PAC-learnable with:
$$
m_{\epsilon, \delta} \leq \lceil \frac{2\log (\frac{2\mathcal{H}}{\delta})}{\epsilon^2} \rceil
$$
hence with polynomial dependency on $\epsilon$ and $\delta$. In this framework, $\log(|\mathcal{H}|)$ is a measure of the complexity of the set $\mathcal{H}$.
\begin{warningblock}
    If $\mathcal{H}$ is described by $d$ parameters of type double when represented in a computer (64 bits), it holds that $|\mathcal{H}| \leq 2^{d\dot 64}$, so we have a finite set of hypothesis, hence we can provide a bound on every implementable set of hypothesis functions.

    In this case,
    $$
    m_{\epsilon, \delta} \leq \frac{128d + 2\lg(\frac{2}{\delta})}{\epsilon^2}
    $$
    i.e. we have linear dependency on the number of parameters.
\end{warningblock}

TO FINISH PAC LEARNING EXAMPLE...




\section{VC Dimension (Vapnik-Chervonenkis)}






