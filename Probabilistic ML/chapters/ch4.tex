\chapter{Sampling-based Inference} \label{ch:sampling}

\dots

\section{Markov chain}

\subsection{Introduction}

A Markov Chain is a stochastic process, denoted as
$$
\{X_t\}_{t \ge 0}
$$
with $t \in \mathbb N, X_0, X_1, ..., X_t \in \mathcal X$, where $\mathcal X$ can be discrete or continuous.

It can be described as a \textit{dynamical system}, i.e. a system in which we start from a certain state $x_0$ with a given probability $p_0(x)$ and that changes state according to a dynamic described by the \textit{transition kernel} $p(x_n | x_{n-1})$.

\begin{tipsblock}[Remark - memoryless property]
    Markov Chains satisfy the memoryless property, which corresponds to the assumptions encoded into the following probabilistic graphical model:
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[
            node distance=1cm and 2cm, % increase horizontal distance
            latent/.style={circle, draw, minimum size=15pt, inner sep=0pt} % smaller nodes
        ]
            \node[latent] (x1) {$\ $};
            \node[latent, right=of x1] (x2) {};
            \node[latent, right=of x2] (x3) {};
            \node[latent, right=of x3] (x4) {};
            \draw[->] (x1) -- (x2);
            \draw[->] (x2) -- (x3);
            \draw[->] (x3) -- (x4);
        \end{tikzpicture}
    \end{figure}
    Therefore we have that:
    $$
    p(x_n | x_{n-1}, ..., x_0) = p(x_n | x_{n-1})
    $$
    which is known as the \bfit{memoryless} or \bfit{Markov property}.
\end{tipsblock}

We also require the \bfit{time homogeneity} property that states that:
\vspace{0.3em}
$$
p(x_n | x_{n-1}) = p(x_1 | x_0), \quad \forall n \ge 1
$$
which means that the probability to jump from a certain state to another state stays the same for every time step.

\begin{definitionblock}[Transition Kernel]
    $p(x_n | x_{n-1})$ satisfying the time homogeneity property is called the (one step) \bfit{transition kernel}.
\end{definitionblock}

For a discrete state space we would have a transition matrix, while for a continuous state space, $p(x_n | x_{n-1})$ is a probability density on $x_n$ depending continuously on $x_{n-1}$.

Since we are interested at the behaviour of the Markov Chain as the index $n$ progresses, i.e. for large times, we need to define few more concepts.

\begin{definitionblock}[Ergodicity]
    A \textit{Markov Chain is \textbf{erodic} iff}:
    $$
    \forall x,y \in \mathcal X, \exists t \ge 0 : p(x_t = y | x_0 = x) > 0
    $$ 
\end{definitionblock}

If a Markov Chain is ergodic, it means that there is always the possibility of going from a given state $x$ to another given state $y$ if we are patient enough. This means that the entire state space is reachable, no matter where we start exploring.

\missing{MCMC, Gibbs Sampling, PGM}

\subsection{Convergence Diagnostic}

How can we check that our Markov Chain has reached the steady state? We will put forward a set of tools that can monitor one or more trajectories and roughly tell us whether we reached it or not. Notice that since we are interested in sampling the stationary probability distributions, we shall start keeping the samples only when we actually reached the steady state.

Let's define some notation for the rest of the section. We are going to denote a general function over a state of the MCMC trajectory $(x_t)_{t \ge 0}$ as $\Psi : \mathcal X \to \mathbb R$. This function can be a lot of different things, depending on what we are interested in computing, e.g. a projection on single coordinate.

We will assume that $\Psi$ has values in $\mathbb R$, and not just in a subset of it,
and, if it does, we transform the function $\Psi$ to make it compliant to this assumption (taking the logarithms of quantities in between $(0, \infty)$ for example).

Let's fix some further notation:

\begin{itemize}
    \item $x_1, \dots, x_n$ is our sampled trajectory
    \item $\Psi_j := \Psi(x_j)$
    \item $\bar \Psi := \frac 1N \sum_j \Psi_j$ is the estimate of $\mathbb E[\Psi] = \int \Psi(x)p(x)\dd x$
\end{itemize}

On a high level, the idea is to look at more than one chain and compare the distribution of the samples that we obtain and see if they look more or less the same.

Pratically,

\begin{enumerate}
    \item we sample $\frac m2 \ge 1$ trajectories from overdispersed initial points. We try to start from different states that are far away in our state space;
    \item Sample for $4n$ steps;
    \item We throw away the first half of every trajectory so that we have only $2n$ points left. This phase is known as the \bfit{burn-in} or \bfit{warm-up} phase. We do this becouse it takes time to reach the steady state (notice that is an heuristic: convergence to the stationary distribution can happen faster or slower than $2n$ steps)
    \item Then we split in 2 parts the remain trajectories, so that we are left with $m$ different trajectories each of length $n$.
\end{enumerate}

From now on we will denote each sample as $x_{ij}$ where $i \in [1,n]$ and $j \in [1,m]$, where this notation describes the $i_{th}$ smaple of the $j_{th}$ trajectory. We will also denote $\Psi(x_{ij}) = \Psi_{ij}$ and define:

$$
\begin{cases}
    \bar \Psi_j := \dfrac 1n \sum_{i = 1}^n \Psi_{ij}\\
    \bar \Psi := \dfrac 1m \sum_{j = 1}^m \bar \Psi_j
\end{cases}
$$

Hence, $\bar \Psi_j$ is the average within the trajectory $j$ and $\bar \Psi$ is the average over all the trajectories. We are also interested in the variance of $\bar \Psi$, but since our samples are not independent, we don't have that $\text{VAR}[\bar \Psi] = \dfrac 1n \text{VAR}[\Psi]$, i.e. the ariance of the estimator in this case is not just the variance of our random variable divided by the number of samples. If you think about two consecutive points in the chain, there is a high chance that the correlation between them is higher than that of two points which are sampled distantly in time from one another.

Let's define these two quantities:

$$
\begin{array}{l}
W := \dfrac 1m \sum_{j = 1}^m s_j^2,
\qquad
s_j^2 = \dfrac 1{n-1} \sum_{i = 1}^n (\Psi_{ij} - \bar \Psi_j)^2
\\[2em]
B := \dfrac n{m-1} \sum_{j = 1}^m (\bar \Psi_j - \bar \Psi)^2
\end{array}
$$

$W$ is called the \bfit{within variance} while $B$ is called the \bfit{between variance}.

We know by definition that:

$$
W \le \text{VAR}[\Psi]
$$

because when sampling single trajectories we have not necessarily explored and visited the full space. Increasing the number of samples we will converge to the true variance.  Moreover, as long as the initial states are overdispersed, it can be shown that:

$$
\text{VAR}[\Psi] \le \text{VAR}^+[\Psi] := \dfrac{n-1}m W + \dfrac 1n B
$$

Then we have both a lower and an upper bound for our variance, both converging to the true variance. Therefore we can compute the statistics:

$$
\hat R := \sqrt{\dfrac{\text{VAR}^+[\Psi]}W}
$$

which can be monitor while running the MCMC simulations. Notice that $\hat R > 1$ and that $\hat R \xrightarrow{n \to \infty} 1$. Heuristically, we can say that when $\hat R \le 1.1$ we have converged to our stationary distribution.

\subsubsection{Effective Smaple Size}

\dots

\subsubsection{Hamiltonian Monte Carlo}

Hamiltonian Monte Carlo can be considered as the state of the art method for doing Markov Chain Monte Carlo. It falls into the category of \bfit{augmented variables} Monte Carlo methods.

The idea is to turn the problem into an Hamiltonian, augmenting the state space with momentum variables that provide a sort of "kinetic energy" that allows the algorithm to move along the surface of the energy corresponding to our probability distribution.
This scheme improves a lot the mixing time, hence the efficiency of MCMC algorithms. Moreover, it can be used in the case of high-dimensional multimodal distributions
because it does not remain stuck in a single mode.

As usual, we start in a situation in which we want to sample from a distribution $p(x) = \dfrac 1Z \tilde p(x)$, and now we express our distribution as:

$$
\dfrac 1Z \tilde p(x) = \dfrac 1Z_x \exp(H_x(x))
$$

This procedure is called the \bfit{Boltzmann Trick} and it is always possible (just take the logarithm of the distribution) and turns our probability distribution in the fotm of an energy.

Then, we introduce momentum variables $y$, as many as the number of $x$ variables that we have, and we assign to them the probability distribution $p(y) = \dfrac 1Z_y \exp(H_y(y))$, where is typical to make the assumption:

$$
H_y(y) = \dfrac 12 y^\top y
$$

i.e. to consider $p(y)$ a standard Gaussian.

We are going to sample from the joint distribution, exploiting the independence of our variables:

$$
p(x,y) = p(x)p(y) = \dfrac 1{Z_x Z_y} \exp(H_x(x) + H_y(y)) = \dfrac 1Z \exp(H(x,y))
$$

The idea of the algorithm is to sample from $p(x,y)$ and then forget about $y$.

We are going to sample according to the force field defined by the Hamiltonian, i.e. along lines which keep the energy constant. This means that, given some velocity, we follow a trajectory on the probability distribution space accordingly to the equations of motion in order to explore the space without losing energy. In fact, we would like to preserve energy because we want to move between regions with high probability.

Hence we have the following algorithm:

\begin{algorithm}
    \caption{Hamiltonian Monte Carlo}
    % \Input{A starting point $x_i$}
    % \Output{}
    \begin{algorithmic}[1]
        \State We start fromn point $x_i$ 
        \State we sample $y \sim p(y)$, i.e. we randomize the momentum
        \State we choose a random direction in time, i.e. we sample from ${-1,1}$ uniformly. This makes our problem reversible and provides ergodicity
        \State we move according to Hemiltonian dynamics from $(x_i, y)$ to a candidate $(x', y')$:
    \end{algorithmic}
\end{algorithm}