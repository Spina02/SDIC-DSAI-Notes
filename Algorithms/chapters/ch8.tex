\chapter{Stream Model}

\section{Bloom Filters}

A \textbf{Bloom filter} is a space-efficient probabilistic data structure designed to test whether an element is a member of a set. It is especially useful when the \bfit{data is so large that it cannot fit in main memory}, motivating the use of a stream model. Bloom filters allow for false positives (reporting an element is present when it is not), but never false negatives.

\textbf{Problem:} Given a stream of keys, maintain a data structure to answer membership queries:
$$
\text{"Given any key, did $k$ appear in the stream?"}
$$

\subsection{Types of Bloom Filters}

\begin{itemize}
    \item \bfit{1-binary array and 1-hash function.}

    We have an array $B$ of $m$ bits and a hash function $h: \mathcal{U} \rightarrow [0, m-1]$. For each key $k$ in the stream, set $B[h(k)] \leftarrow 1$. If the cell is already 1, do nothing. Collisions can cause errors: we may answer \emph{yes} to queries for which the real answer is \emph{no} (false positives), but never the opposite. The probability of a false positive is roughly $\frac{n}{m}$.

    
    \item \bfit{i-binary array and $k$-hash functions.}

    In order to decrease the probability of error, we could store $i$ binary arrays of size $m$, such that for inserting an item $i$, we compute all its hash functions $B[h_i(k)] \leftarrow 1$. This way we can still have collisions but with a smaller probability. The probability of a false positive is:
    \vspace{0.2em}
    $$
    \Pr(\mathrm{FP}) \leq P_1^i
    $$

    \vspace{-0.4em}

    where $P_1$ denotes, among all the binary arrays used, the highest fraction of cells set to 1 ($\frac{\#1s}{m}$). 
    
    The probability of a false positive decreases exponentially as $i$ increases.
    
    \begin{figure}[H]
        \centering
        \includegraphics[height=14em]{assets/BF-1-1.png}
        \hspace{6em}
        \includegraphics[height=14em]{assets/BF-i-i.png}
        \caption{Bloom filters. 1-array and 1-hash functions (left) and $k$-array and $k$-hash functions (right).}
    \end{figure}

    \item \bfit{1-binary array and $k$-hash functions.}

        Use one binary array of size $m$ and $k$ independent hash functions. To insert $k$, set $B[h_j(k)] \leftarrow 1$ for all $j = 1, \dots, k$. The probability that a bit is 0 or 1 after $n$ insertions is:
            \begin{itemize}
                \item With 1 hash function and $n=1$, it is $P_0 = \left(1-\frac{1}{m}\right)$, then $P_1 = 1 - P_0$.

                \item With $k$ hash functions and $n$ insertions, it is $P_0 = (1 - \frac{1}{m})^{nk}$.
            \end{itemize}

        Then, the probability of getting a false positive is:
        \small
        $$
            \Pr(\mathrm{FP}) = P_1^k = (1-P_0)^k = \left(1-\left(1-\dfrac{1}{m}\right)^{nk}\right)^k = f
        \quad \Rightarrow \quad
            \Pr(\mathrm{FP}) \sim \left( 1 - e^{-\frac{nk}{m}} \right)^k = \tilde{f}
        $$
        \normalsize
        If $\frac{n}{m} = c$ is constant, the optimal number of hash functions is:
        $$
        k^* = \ln(2) \frac{m}{n}
        $$

        \vspace{-0.6em}

        For example, if $\frac{m}{n} = 10$, $k^* \approx 7$.

        The probability of reading a 0 in the Bloom filter is $\tilde{P}_0 = e^{-\frac{nk}{m}}$. With the optimal $k^* k = \ln(2) \frac{m}{n}$:
        $$
            \tilde{P}_0 = \frac{1}{2}
        $$

        \vspace{-1em}

        \begin{figure}[H]
            \centering
            \includegraphics[height=12em]{assets/BF-1-i.png}
            \caption{Bloom filter with 1 array and $k$ hash functions.}
        \end{figure}

        \vspace{-1em}

        \bfit{How many items can we insert before the Bloom filter is 50\% full?}

        A natural question is to estimate the number of insertions required before the Bloom filter's bit array becomes substantially full, for instance, 50\%. Consider the process of setting bits in the $m$-bit array, and $k = 1$ hash function:
        \begin{itemize}
            \item \textbf{First insertion:} The probability that an attempt sets a new bit is $P_0 = \frac{m-0}{m} = 1$. The expected number of attempts is $1/P_0 = 1$.
            \item \textbf{Second insertion:} The probability that an attempt sets a new bit (one of the $m-1$ remaining zero bits) is $P_1 = \frac{m-1}{m}$. The expected number of additional attempts is $1/P_1 = \frac{m}{m-1}$.
            \item \textbf{General case:} The probability that an attempt sets a new bit is $P_j = \frac{m-j}{m}$. The expected number of additional attempts to achieve this is $1/P_j = \frac{m}{m-j}$.
        \end{itemize}

        \begin{warningblock}[Notation $P_j$]
            Note that we're using $P_j$ here to represent the probability of setting a new bit during the $j$-th insertion. This is different from our earlier use of $P_0$, which represented the probability of finding a 0 in the Bloom filter after $n$ insertions.
        \end{warningblock}

        Let $X$ be the random variable representing the total number of such bit-setting attempts required to set exactly $m/2$ distinct bits in the Bloom filter to '1'. The expected value of $X$, $E[X]$, is the sum of the expected number of attempts needed to set each successive new bit, from the 1st bit up to the $(m/2)$-th bit:
        $$
            E[X] = \sum_{j=0}^{m/2-1} \frac{m}{m-j}
        $$
        This sum can be rewritten by letting $i' = m-j$. 
        $$
        \begin{cases}
            i' = m & \text{if } j = 0 \\
            i' = m/2+1 & \text{if } j = m/2-1 \\
            i' = m-j & \text{otherwise}
        \end{cases}
        \quad \Rightarrow \quad
        E[X] = m \sum_{i'=m/2+1}^{m} \frac{1}{i'}
        $$
        Using the Taylor approximation, we get:
        $$
            E[X] \approx m (\ln m - \ln(m/2)) = m \ln\left(\frac{m}{m/2}\right) = m \ln 2
        $$
        For example, if $m=100$, $E[X] \approx 100 \ln 2 \approx 69.3$. So, we expect to make around 69 "effective bit-setting attempts" before half the bits in the Bloom filter are set to 1.

    \end{itemize}

    \subsubsection{Set operations}    

    Given $BF_1$ for $S_1$ and $BF_2$ for $S_2$ (same size $m$, same $k$ hash functions), we can compute:
    
        \begin{itemize}
            \item \textbf{Union:} $B = B_1 \; \mathrm{OR} \; B_2$ represents $S_1 \cup S_2$.

            \item \textbf{Intersection:} $B = B_1 \; \mathrm{AND} \; B_2$ represents $S_1 \cap S_2$. The error rate is higher than a Bloom filter built directly for $S_1 \cap S_2$. Some 1s in the new BF may be from elements that are not in the intersection but match in both BFs.
        \end{itemize}
        
    \subsubsection{Characteristics and Trade-offs}

        Bloom filters have key characteristics that make them valuable. Their \textbf{one-sided error profile} ensures no false negatives, only false positives, making them ideal when missing a true positive is worse than occasional false ones.
        
        Performance-wise, \textbf{insertion and query operations are $O(k)$}, independent of the number of items. Hash operations can be parallelized for fast execution. Bloom filters are also \textbf{space-efficient} compared to traditional data structures, as shown in \cref{tab:bf-vs-ht}, making them perfect for large datasets or memory-constrained environments.
                
        \begin{table}[H]
            \centering
            \begin{tabular}{|c |c | c|}
                \hline
                 & \textbf{BF} & \textbf{HT}\\
                 \hline
                item & $m$ bits & $m+n$ pointers $+ n$ integers\\
                \hline
            \end{tabular}
            \caption{Simplified space comparison between Bloom filter and hash table.}
            \label{tab:bf-vs-ht}
        \end{table} 

        \vspace{-1em}

        However, these benefits come with certain \textbf{limitations}:
        
        \begin{itemize}
            \item \textbf{do not support item deletion}: attempting to delete an item by clearing its corresponding bits could inadvertently affect other items that happen to hash to one or more of the same bits, potentially leading to false negativesâ€”a violation of their core guarantee. 

            \item \textbf{cannot store associated values with keys}: they are purely membership testing structures. If you need to retrieve data associated with an item, a Bloom filter can only serve as a preliminary check to avoid costly lookups for non-existent items.
        \end{itemize}

\section{Count-Min Sketch}

While Bloom filters answer membership queries, the \textbf{Count-Min Sketch (CMS)} addresses a different problem in stream processing: estimating the \textbf{frequency} of items.

\textbf{Problem:} Given a stream of items $a_1, \dots, a_N$ (where $N$ is the total count of items in the stream, including repetitions) drawn from a universe $\mathcal{U}$, we want to maintain a compact data structure that can estimate the frequency $f(x)$ of any queried item $x \in \mathcal{U}$. The frequency $f(x)$ is the number of times $x$ has appeared in the stream processed so far.

A CMS consists of a $d \times w$ matrix of counters, denoted $C$. Initially, all $C[i][j]$ are set to 0. The structure uses $d$ independent hash functions $h_1, \dots, h_d$. Each hash function $h_j: \mathcal{U} \rightarrow [0, w-1]$ maps items to one of the $w$ columns in its respective row $j$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{assets/CMS.png}
    \caption{A $d \times w$ Count-Min Sketch. Here, $d=4$ rows (hash functions) and $w=10$ columns.}
    \label{fig:cms_structure}
\end{figure}

To \textbf{insert} an element $a_i$, we should apply all the hash functions (one per row) and increase by 1 the values of the corresponding cells.

To \textbf{query} an item $x$ (estimate its frequency), denoted $\tilde{f}(x)$, we compute $h_j(x)$ for all $j=1, \dots, d$ and retrieve the values from the corresponding counters. The estimate is the minimum of these values:
$$ \tilde{f}(x) = \min_{j \in [1,d]} \{C[j][h_j(x)]\} $$
Let $f(x)$ be the true frequency of $x$. A key property of CMS is that it \textbf{never underestimates} the frequency: $\tilde{f}(x) \geq f(x)$. Taking the minimum across different hash functions attempts to find the row with the least "collision noise."

\textbf{Accuracy Guarantee (Claim):}

The power of CMS lies in its probabilistic accuracy guarantee. If we choose the width $w = \left\lceil \frac{2}{\varepsilon} \right\rceil$ and depth $d=\left\lceil \log_2 \delta^{-1} \right\rceil$, then for any item $x \in \mathcal{U}$:
\vspace{0.4em}
$$
\Pr(\tilde{f}(x) \leq f(x) + \varepsilon n) \geq 1 - \delta
$$

\vspace{-0.6em}

being $n$ the number of items in the stream.

The parameters $\varepsilon > 0$ and $0 < \delta < 1$ control the trade-off between accuracy and space:
\begin{itemize}
    \item $\varepsilon$ determines the \textbf{error factor}: the overestimation is bounded by $\varepsilon N$. Smaller $\varepsilon$ requires larger $w$ (more columns).
    \item $\delta$ determines the \textbf{failure probability}: the probability that the error bound is violated. Smaller $\delta$ requires larger $d$ (more rows/hash functions).
\end{itemize}
In simpler terms, with probability at least $1-\delta$, the estimated frequency $\tilde{f}(x)$ overestimates the true frequency $f(x)$ by at most $\varepsilon N$.

\newpage

\textbf{Proof}: 

\begin{enumerate}
    \item Fix item $x \in S$

    \item For each row $i$, define $d$ random variables $Z_1, \dots, Z_d$ such that $Z_i = C_{i,h(x)} - f(x)$

    \item Define also random variables $Y_{i,y} = \begin{cases} 
                                        1 & \text{if } h_i(x) = h_i(y) \\
                                        0 & \text{otherwise} 
                                    \end{cases} \qquad \forall y \neq x, y \in S$
    
    \item The error $Z_i$ can be expressed as:
    $$
    Z_i = \sum_{y \neq x} (Y_{i,y} f(y))
    $$

    \item The expected value of $Z_i$ is bounded by:
    $$
    E(Z_i) = \sum_{x \neq y} E(Y_{i,y} f(y)) = \sum_{x \neq y} f(y) E(Y_{i,y}) = \sum_{x \neq y} f(y) \Pr(h_i(x) = h_i(y)) \leq \dfrac{n}{w}
    $$

    \item Since $Z_i \geq 0$, applying Markov's inequality yields:
    $$
    \Pr(Z_i \geq b \; E(Z)) \leq \dfrac{1}{b} \Rightarrow \Pr(Z_i \geq \dfrac{b\;n}{w}) \leq \dfrac{1}{b}
    $$

    \item Let $b = w \; \varepsilon$
    $$
    \Pr(Z_i \geq \varepsilon \; n) \leq \dfrac{1}{w \varepsilon} = \dfrac{1}{2}
    $$

    That bounds the probability of making an error greater than some quantity in a given row.

    \item The probability of overestimation is bounded by:
    $$
    \Pr(Z_i + f(x) \geq f(x) + \varepsilon \; n) \leq \dfrac{1}{2}
    $$

    \item The final probability bound is:
    $$
    \Pr(\forall i \in [0, d-1], Z_i \geq \varepsilon \, n) \leq \left(\dfrac{1}{2}\right)^d = \left(\dfrac{1}{2}\right)^{\log_2 \delta^{-1}} = 2^{-\log_2 \delta^{-1}} = \delta
    $$
    $$
    \Rightarrow \quad \Pr(\exists i \in [0, d-1] \text{ s.t. } Z_i < \varepsilon \; n) \geq 1 - \delta
    $$

    CMS of dimensions $O(\varepsilon^{-1} \log \delta^{-1})$ achieves performance $\forall \varepsilon, \delta$.
\end{enumerate}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        $\varepsilon$ & $\delta$ & $\lceil 2/\varepsilon \rceil \cdot \lceil \log_2 \delta^{-1} \rceil$ \\
        \hline
        $10\%$ & $0.1$ & $80$ \\
        $1\%$ & $0.01$ & $1,\!400$ \\
        $0.1\%$ & $0.001$ & $20,\!000$ \\
        $0.0001\%$ & $0.01$ & $1,\!400,\!000$ \\
        \hline
    \end{tabular}
    \caption{Number of counters required for various error ($\varepsilon$) and failure probability ($\delta$) parameters.}
\end{table}

\vspace{-1em}

Remember that $\varepsilon$ multiplies $m$, and a counter requires many (maybe 32 or 64) bits.

\begin{tipsblock}[CMS use case - heavy hitters]
    A common and important use of the Count-Min Sketch (CMS) is to identify \textbf{heavy hitters} (items whose frequency exceeds a given threshold). 
\end{tipsblock}

