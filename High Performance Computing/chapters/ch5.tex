\chapter{Parallelism}

\textbf{why parallelism?}

We wnt to parallelize our code for two main reasons:

\begin{itemize}
    \item To speed up a code
    \item The problem size exceeds the computer capacity
\end{itemize}

we have two types of problems:

\begin{enumerate}
    \item \textbf{Embarassingly paralell problems}: The problem solution for each data point is completely independent of the solution for any another data point
    \item All the rest
\end{enumerate}

Of course, normally you have some blending of computations that are independent of each other and computations that are not, with a significant umbalance towards the second case.

...
\section{Parallel Computing}

\subsection{Domain decomposition}

\textbf{Domain decomposition} is a technique used to solve large problems by dividing them into smaller, more manageable subproblems. This approach is particularly useful when the goal is to distribute the workload across multiple processors or cores.

...

when the work-load is strongly dependent on the properties of the data, and moreover those propertie change in time during the computation, there is not a general rule.

You can, for instance, sort your data by computational intensity and distribuite them to the different players, so to achieve an even work-load wirth a minimum imbalance; then, repeating the procedure as often as needed while the data evolves during a multi-step computation.

\subsection{Functional decomposition}

\textbf{Functional decomposition} is, as domain decomposition, a technique used to break down a complex problem into smaller subproblems. This time, the decomposition is based on the functions or operations that need to be performed. 

Quite often, your computation is made of several different “sections”. Let's call them tasks.

There will be, in general, a dependency graph among the tasks: some will be completely independent of any other, some will be dependent on 1, 2 or more “previous” tasks and will feed some “subsequent” tasks.

Then, you may decompose not the data but the tasks among your workers. In general, that requires some synchronization to manage the dependencies

\dots

\begin{definitionblock}[What is parallel computing?]
    \begin{enumerate}
        \item A \bfit{parallel computer} s a computational system that offers simultaneous access to many computational units fed by memory units. The computational units are required to be able to co-operate in some way, meaning exchanging data and instructions with the other computational units.
        \item \bfit{Parallel processing} is the ensemble of techniques and algorithms that makes you able to actually use a parallel computer to successfully and efficiently solve a problem.
    \end{enumerate}
\end{definitionblock}

The parallel processing is expressed by \textbf{software entities} that have an increasing level of granularity (processes, threads, routines, loops, instructions..)

The software entities run on underlying \textbf{computational hardware entities} (as processors, cores, accelerators)

The data to be processed/created live and travel in \textbf{storage hardware entities} (as Memory, caches, NVM, networks, DMA)

The exploitation/access of hardware resources (computational and storage) is \textbf{concurrent} among software entities

\subsubsection{Shared vs Distribuited Memory}

Shared-memory programming leverages a common memory accessible to all computational units, thus facilitating direct data exchange without explicit communication routines. In contrast, distributed-memory programming relies on explicit communication, often via message passing, to exchange data since each unit has its own local memory. This distinction reflects both the physical memory layout and the programming approach.

\paragraph{Distributed Memory}
A typical programming paradigm is the message-passing approach, where processes communicate via “messages” and each process has its own memory space.
Communication can happen either over the network (different protocols are possible at hardware/middleware level) or via shared memory techniques if the communicating processes can directly access the same memory. The user, however, still treats the process as if it were using message passing, and the actual communication is managed by the middleware.

A well-known standard is MPI (Message-Passing Interface). Since version~2.0, MPI has provided interfaces for direct memory access, mimicking shared-memory mechanisms.

\paragraph{Shared Memory}
A typical programming paradigm is multi-threading, where multiple threads concurrently access the same virtual address space. There are no “messages”; communications and synchronizations must be directly managed in shared memory.

A very widely used high-level standard is OpenMP (openmp.org), or Open
Multi-Processing. On all platforms, a very low-level threading library is available. On POSIX systems it is named \texttt{pthread}.

On some systems, a software middleware can hide the physical details from the programmer and expose the memory of all nodes as a unified shared memory. In reality, remote memory access may still happen over the network under the hood.

\dots

\paragraph{MPI and OpenMP in Practice}
The two (by far) most used implementations of distributed-memory and shared-memory paradigms are MPI (for distributed memory) and OpenMP (for shared memory), as mentioned above. OpenMP is delivered by the C/C++/Fortran compilers themselves by enabling OpenMP support via a command-line option (for example, \texttt{-fopenmp} for \texttt{gcc} and \texttt{icc}, \texttt{-mp} for \texttt{pgi} and \texttt{nvc}). MPI, on the other hand, is offered by various implementations such as OpenMPI (no relation to OpenMP), MPICH, and MVAPICH, among others.

\dots

\subsubsection{Flynn's Taxonomy}

The Flynn’s taxonomy helps in
understanding the logical subdivision
of parallel systems, but it is no more
up-to date; it mainly refers to HW
capabilities but in 60yrs the HW
evolved a lot and today we can
imagine it refers to a mix of HW and
SW

\begin{figure}[H]
    \centering
    \caption{Flynn's Taxonomy}
    \includegraphics[width = 0.5\textwidth]{assets/flynn_taxanomy.png}
\end{figure}

\begin{itemize}
\item \textbf{SISD} - Single Instruction on Single Data
\item \textbf{MISD} - Multiple Instructions on Single Data
\item \textbf{SIMD} - Single Instruction on Multiple Data
\item \textbf{MIMD} - Multiple Instructions on Multiple Data
\end{itemize}

[HW level - SW level table]

We can now refine our HPC definition:

High Performance Computing (HPC) is the use of servers, clusters, and supercomputers - plus associated software, tools, components, storage, and services - for scientific, engineering, or analytical tasks that are particularly intensive in computation, memory usage, or data management.

HPC is used by scientists and engineers both in research and in production across industry, government and academia.

\dots

In old times all the cores of th
e cpu were connected to a Front Side Bus, which was a single bus connected at the same time a North Bridge and a South Bridge. The North Bridge was connected to the CPU and the RAM, while the South Bridge was connected to the I/O devices. 

Nowadays, he NorthBridge has been moved inside the cpu and there are many (2-8) lanes that connect the cpu to the DRAM bank(s). The SouthBridge is what is called “the chipset”

\begin{minipage}{0.5\textwidth}
    \centering
    % \caption{Node Topology} \label{fig:node_topology}
    \includegraphics[width = \textwidth]{assets/node_topology.png}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
    \centering
    % \caption{Real Node topology} \label{fig:real_node_topology}
    \includegraphics[width = 0.9\textwidth]{assets/real_node.png}
\end{minipage}%

Nowadays, in general, as it's possible to see in the figure above HPCs has more than a socket connected to the same DRAM

\subsubsection{The overall topology}

Many (on the order of 10 to 10\textsuperscript{5}) nodes are connected by a switch-based network, whose topology may vary significantly. The details can severely affect overall performance. Typical figures for latency and bandwidth are around 1\,\textmu s and 100\,\text{Gbit/s}, respectively. The most common standards are InfiniBand and OmniPath.

\begin{figure}[H]
    \centering
    \caption{Flynn's Taxonomy}
    \includegraphics[width = 0.5\textwidth]{assets/overall_topology.png}
\end{figure}

Note that on a supercomputer there is a hybrid approach to memory placement:
\begin{itemize}
    \item The memory on a single node can be accessed directly by all its cores. This is called **shared memory**.
    \item When using many nodes together, a process cannot directly access the memory on a different node; it must issue a request. This is known as **distributed memory**.
\end{itemize}

These concepts describe physical memory accessibility but also refer to programming paradigms, as discussed later.

...

[little callback: NUMA vs UMA]

When we are in a NUMA architecture, the memory is not shared among all the cores. Each core has its own memory bank, and the access to the memory is not uniform. This means that some cores can access their own memory bank faster than others.

In this case, if a core wants to access the memory of another core, it has to go through a switch and it will take longer.
$$
\begin{array}{c|cccccccc}
    & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
    \hline
    0 & 10 & 12 & 12 & 12 & 30 & 30 & 30 & 30 \\
    1 & 12 & 10 & 12 & 12 & 30 & 30 & 30 & 30 \\
    2 & 12 & 12 & 10 & 12 & 30 & 30 & 30 & 30 \\
    3 & 12 & 12 & 12 & 10 & 30 & 30 & 30 & 30 \\
    4 & 30 & 30 & 30 & 30 & 10 & 12 & 12 & 12 \\
    5 & 30 & 30 & 30 & 30 & 12 & 10 & 12 & 12 \\
    6 & 30 & 30 & 30 & 30 & 12 & 12 & 10 & 12 \\
    7 & 30 & 30 & 30 & 30 & 12 & 12 & 12 & 10 \\
\end{array}
$$
The one in the table are the relative "distances" between the nodes. 
This distance is not the physical distance, but a figure of merit that takes into account the time it takes to access the memory of the other cores.

The first four cores (0, 1, 2, 3) are in the same socket and they can access their own memory bank in 10 cycles and the memory of the other cores in 12 cycles. But when they access the memory of the other socket (4, 5, 6, 7), it takes 30 cycles.

\section{Cache Coherence}

Cache coherence ensures that multiple cores sharing data in separate caches observe a consistent view of memory. This is crucial in multi-core architectures, where performance can degrade significantly if data synchronization is poorly managed. In particular:
\begin{itemize}
    \item When a memory location is accessed by two or more cores, each core typically holds that location in its private caches. If one core updates the data, it must be propagated to other caches holding a copy.
    \item When a thread is migrated from one core to another, its original cached data may still reside on the previous core, requiring updates or invalidations to keep the caches consistent.
\end{itemize}

Such synchronization overhead can be a severe performance bottleneck if data is frequently updated by multiple cores. Concurrent writing and migrating data between caches are two common sources of delays because hardware-level consistency mechanisms kick in often.

To address these issues, most modern systems use the \textbf{MESI} (Modified, Exclusive, Shared, Invalid) protocol (successor to the MSI protocol and precursor to MESOI). Its states are:

\begin{itemize}
    \item\textbf{Modified (M):} The core has exclusively modified this cache line, and no other core has a valid copy.
    \item\textbf{Exclusive (E):} The core owns the cache line exclusively, but there have been no modifications yet. No other core holds a copy.
    \item\textbf{Shared (S):} Multiple cores share the cache line. Write attempts require notifying other cores or changing the state.
    \item\textbf{Invalid (I):} The cache line is invalid because another core modified it, or it is simply not in use.
\end{itemize}

\begin{exampleblock}[MESI]
    Suppose three threads (on three different cores) share a variable representing the wall-clock time. A “timer” thread (Thread0) updates this variable periodically, while the other two threads (Thread1 and Thread2) read its value. The MESI protocol ensures all cores see a consistent value even though the data is replicated in each core’s local cache.

    [image]
\end{exampleblock}

\dots

Has we have seen, «performance» is a tag that can stand for many things.

In this frame, with «performance» we mean the relation between the computational requirements and the computational resources needed to meet those requirements.

$$
\text{\textbf{Performance}} \approx \dfrac 1{resources},
\qquad
\text{\textbf{Performance ratio}} \approx \dfrac {resource_1}{resource_2}
$$

where the resources are the time, the hardware (CPU, memory, etc.) and, if we want, money.

\subsubsection{Key factors}

\begin{center}
\begin{tabular}{ll}
\toprule
$n$: & problem size \\
$p$: & number of computing units \\
$T_s(n)$: & Serial run-time for a problem of size $n$ \\
$T_p(n)$: & Parallel run-time with $p$ processors for a problem of size $n$ \\
$f_n$: & Intrinsic sequential fraction of the problem \\
$k(n,t)$: & Overhead of the parallel algorithm \\
\textbf{Speedup}: & $S(n,p) = \dfrac{T_s(n)}{T_p(n)}$ \\[1em]
\textbf{Efficiency}: & $E(n,p) = \dfrac{S(n,p)}{p} = \dfrac{T_s(n)}{p T_p(n)}$ \\
\bottomrule
\end{tabular}
\end{center}

The sequential execution time for a problem of size $n$ is
$$
T_s(n) = T_s(n) \times f_n + T_s(n) \times (1 - f_n)
$$

Assuming that the parallel fraction of the computation is perfectly parallel, meaning that its run time scales as 1/p, then we can express the parallel execution time as:

$$
T_p(n) = T_s(n,1) \times f_n + \dfrac{T_s(n)}{p}\times (1 - f_n)
$$

and then:

\begin{center}
\begin{tabular}{lll}
    \textbf{Speedup}: & $S(n,p) = \dfrac{T_s(n)}{T_p(n)} = \dfrac{1}{f + \frac{1-f}p}$
    &
    $\lim_{p \gg 1} S(n,p) = \dfrac 1f$
    \\
    \textbf{Efficiency}: & $E(n,p) = \dfrac{S(n,p)}{p} = \dfrac 1{f (p-1) + 1}$
    &
    $\lim_{p \gg 1} E(n,p) = 0$
    \\
\end{tabular}
\end{center}

\dots

\subsubsection{Amdahl's Law}

\dots

\subsection{Threads and Processes}

\begin{minipage}{0.6\textwidth}
A \textbf{process} is an independent sequence of instructions and the ensemble of
resources needed for their execution.
A program needs much more than just its binary code (i.e. the list of ops to be
executed): it needs to access to a protected memory space and to access system
resources (e.g. files and network).
A “process” is then a program that
has been allocated with the necessary
resources by the operating system.
There may be different instances of the
same program as different, independent
processes
\end{minipage}%
\begin{minipage}{0.4\textwidth}
    \hfill
    \includegraphics[width = 0.95\textwidth]{assets/process.png}
\end{minipage}%

\begin{minipage}{0.4\textwidth}
    \includegraphics[width = 0.95\textwidth]{assets/threads.png}
\end{minipage}%
\begin{minipage}{0.6\textwidth}
A \textbf{thread} is an independent instance of code execution within a process. There
may be from one to many threads within the same process.
Each thread shares the same code, memory address space and resources than its father process.

While each thread has its own stack, ip and sp, the heap will be shared among threads,
which then operate in shared-memory. threads also share the stack of the father thread.
In geneal spawning threads inside a process is much less costly than creating processes.
\end{minipage}%

A thread can run either on the same computational units of its father process or on a different one.

A computational unit nowadays amounts to a core, either inside the same CPU (socket) on which the father process runs, or inside a sibling socket in the same NUMA region.

\subsection{OpenMP}

OpenMP is a set of compiler directives, library routines, and environment variables that influence run-time behavior. It is designed for shared-memory parallel programming in C, C++, and Fortran. OpenMP is not a programming language but rather an API that provides a portable and scalable model for parallel programming.

\dots

\subsubsection{OpenMP Directives}

An OpenMP directive is a specially-formatted pragma for C/C++ and comment for FORTRAN codes. Most of the directives apply to structured code block, i.e. a block with a single input and a single output points and no branch within it.

The directives allows to:
\begin{itemize}
    \item create team of threads for parallel execution
    \item manage the sharing of workload among threads
    \item specify which memory regions (i.e. variables) are shared and which are private to each threads
    \item drive the update of shared memory regions
    \item synchronize threads and determine atomic/exclusive operations
\end{itemize}

\begin{codeblock}[language=C]
#pragma omp parallel
{
    // This code block will be executed by multiple threads in parallel
    ...
}
\end{codeblock}

Every function call from within a parallel region determines the creation of a dynamic extent to which the same directives apply.

\begin{codeblock}[language=C]
// static extent:
#pragma omp parallel
{
    double *array;
    int N;
    ...
    sum = foo(array, N);
    ...
}

// dynamic extent:
double foo(double *A, int N) {
    double mysum = 0;
    #pragma parallel for reduction(+:sum) // "orphan" directive
    for (int ii = 0; ii < N; ii++) {
        mysum += A[ii];
    }
    return mysum;
}
\end{codeblock}

The dynamic extent includes the original static extent and all the instructions and further calls along the call tree.

\begin{observationblock}[Dynamic Extent]
    The functions called in the dynamic extent can contain additional OpenMP directives.
\end{observationblock}

\subsubsection{Shared / private memory}

you can specify the number of threads to be spawned in the parallel region by setting the environment variable: \plaintt{export OMP\_NUM\_THREADS=n}.

You can also specify it in each parallel region:
\plaintt{\#pragma omp parallel num\_threads(n)}

\newpage

\chapter{Lecture 15/04/2025}

\section{Parallel Regions}

The region starts at the opening \{ brace and ends at the closing \} one.

An implicit synchronization barrier is present at the end of the region.

A parallel region can be as short as a single line:
\begin{codeblock}[language = C]
#pragma omp parallel _some_clauses_here_
single-line-here
\end{codeblock}

There are no limits on the size of the code included within \{...\}:
\begin{codeblock}[language = C]
#pragma omp parallel _some_clauses_here_
{ … }
\end{codeblock}

The specific construct about for loops:
\begin{codeblock}[language = C]
#pragma omp parallel for _some_clauses_here_
{ … }
\end{codeblock}

A more general work-sharing construct
\begin{codeblock}[language = C]
#pragma omp sections _some_clauses_here_
{ … }
\end{codeblock}

This allows task-based parallelism
\begin{codeblock}[language = C]
#pragma omp task _some_clauses_here_
{ … }\end{codeblock}

\begin{observationblock}[Threads and Parallel Regions]
    For efficiency reasons, it may be, and usually it is, that the threads are not created/killed at the begin/end of each region; instead, they are created at the begin of the run and kept sleeping outside of the parallel regions.
\end{observationblock}

\subsection{shared/private memory}

Let's now start to clarify the meaning of “private” and “shared” memory, and how to specify what variables are either one.

The basic rule is that everything that is defined in a serial region is inherited as shared in a parallel region that originates from the former serial region. Global variables are always shared (we'll see an exception)

\begin{codeblock}[language = C]
int i, j, k;
double *array;

#pragma omp parallel
{
    // i,j,k and array
    // are shared here
    ...
}
\end{codeblock}

\subsubsection{The \texttt{private} directive}

You can specify that a list of variables that exists in the local stack at the moment of the creation of the parallel region are \textbf{private} to each thread inside the parallel region.

\begin{codeblock}[language = C]
int i, j, k;
double *array;

#pragma omp parallel private(i,k)
{
// j and array are shared here i and k are unique to each thread, they live in each thread's stack.

// They are different than the original j and k, and array does NOT point to the region pointed by the original array pointer
    ...
}
\end{codeblock}

That means that the needed space will be reserved in the threads' stack to host variables of the same types. As such, those memory regions are different than the original ones, although within the parallel region those variables are referred in the source code with the same names.

\begin{exampleblock}[Private variables]
    \begin{codeblock}[language = C]
int nthreads = 1;
int my_thread_id = 0;

#ifdef _OPENMP
#pragma omp parallel
{
    my_thread_id = omp_get_thread_num();
    #pragma omp master
    nthreads = omp_get_num_threads();
}
#endif
    \end{codeblock}

    \begin{itemize}
        \item all threads are writing in \texttt{my\_thread\_id}, in undefined order
        \item only the master thread is writing in \texttt{nthreads}
    \end{itemize}

    The value of \texttt{my\_thread\_id} unpredictable, because it depends on the run-time order by which the threads access it and by each a thread accesses it again to write it.
\end{exampleblock}

\begin{warningblock}[Private variables]
    A private variable used in the parallel region refers to a memory region that is different than the same variable (outside) in the serial region: as such, this coding style looks only a source of confusion and lacks of cleariness.

    It's highly recommended to avoid using the "private" directive and to use a different name for the private variable, or to use the \plaintt{threadprivate} directive (see below).
\end{warningblock}

\subsubsection{The \texttt{firstprivate} directive}

If the clause \texttt{firstprivate} is used instead, every variable listed is private in the same sense than before, but its value is not randomic but it is \textit{initialized} at the value that the corresponding variable in the serial region has at the moment of entering in the parallel region.

\begin{codeblock}[language = C]
int i, j, k;
double *array;
array = (double*)malloc(...);
#pragma omp parallel
firstprivate(array)
{
// now array is unique to each thread, BUT each copy is initialized to the value that the original array has at the entry of the parallel region.

// As such, now array can be used to access the previously allocated memory.
...
}
\end{codeblock}
    
\subsubsection{The \texttt{lastprivate} directive}

The clause \texttt{lastprivate} pertains only to the for construct. When used, every variable listed is private in the same sense than before, and its value is not initialized.

\begin{codeblock}[language = C]
double last_result;

#pragma omp parallel
lastprivate(last_result)
{
...
    #pragma omp for
    for( int j = 0; j < some_value; j++ )
    last_result = calculation( j, ...);
    ...
}

other_calculations( last_result, ... );

// at this point, last_result has the last value from the last iteration in the for loop in the parallel region
\end{codeblock}

What is affected is the value that the original variable, the one that is declared in the master thread's stack, has after the parallel region. It will have the value that the private copy of it has in the last iteration of the for loop.

\subsubsection{The \texttt{threadprivate} directive}

The clause \texttt{threadprivate} applies to global variables and has a global scope. threadprivate variables are private variables that do exist all along the lifetime of the process. 

I.e. they are private variables that do not die in between of two different parallel regions.

\begin{codeblock}[language = C]
int myN;
double *array;
#pragma omp threadprivate(myN, array)

#pragma omp parallel
{
    // array does exist and it’s private
    // to each thread
    array = ... allocate memory...;
}
// .. something serial here..
#pragma omp parallel
{
    // array does exist here and
    // the allocated memory is available
}
\end{codeblock}

\begin{warningblock}[Threadprivate and Dynamic Threads]
    When using threadprivate, the dynamic thread creation is not allowed, i.e. the number of threads in each parallel region is constant.
\end{warningblock}

\subsubsection{The \texttt{copyin} directive}

The clause \texttt{copyin} applies to parallel and worksharing (e.g. for) constructs. This clause basically provides a way to perform a \textbf{broadcast of \texttt{threadprivate} variables} from the master thread (i.e. the thread 0) to the corresponding threadprivate variables of other threads.

\begin{codeblock}[language = C]
int golden_values[3];
#pragma omp threadprivate(golden_values)

for( int i = 0; i < N; i++ )
{
    get_golden_values();
    #pragma omp parallel copyin(golden_values)
    {
        // each thread uses golden_values[]
    }
}
\end{codeblock}

The copying happens at the creation of the region, before the associated structured block is executed.

\subsubsection{The \texttt{copyprivate} directive}

The clause \texttt{copyprivate} applies only to \texttt{single} construct.

This clause provides a mechanism to propagate the values of private variables, including threadprivate, from a thread to the others inside a parallel region. 

The copying is ultimated before any threads leave the implicit barrier at then end of the construct.

\begin{codeblock}[language = C]
#pragma omp parallel
{
    double seed[2];
    
    #pragma omp single copyprivate(seed)
    {
        // something happens here and the
        // thread that executes this block
        // initializes the seed[2] array
    }
    // at this point the values of the seed[2]
    // array have been propagated to all the
    // other threads
}
\end{codeblock}