\newpage

\chapter{Lecture 18/03/2025}




\subsubsection{Polar Coordinates}

Let's consider data disposed as in figure:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\textwidth]{assets/polar.png}
    \label{fig:polar}
\end{figure}

We can convert the data from Cartesian to Polar coordinates as follows:

\begin{center}
    \begin{minipage}{0.4\textwidth}
        \begin{tikzpicture}[scale=0.5]
            % Draw x and y axes
            \draw[->] (-4,0) -- (4,0) node[right] {$x_1$};
            \draw[->] (0,-4) -- (0,4) node[above] {$x_2$};
            
            % Draw the annulus (circular corona)
            \filldraw[fill=blue!20, draw=blue, even odd rule] 
                (0,0) circle (3cm) (0,0) circle (1cm);
        \end{tikzpicture}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        $$
        \begin{cases}
            x_1 \\
            x_2
        \end{cases}
        \quad \rightarrow \quad
        \begin{cases}
            r = \sqrt{x_1^2 + x_2^2} \\
            \theta = arctg \dfrac{x_2}{x_1}
        \end{cases}
        $$
    \end{minipage}
\end{center}

\subsubsection{Other Non-Linearities}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/non-lin.png}
\end{figure}

\newtheorem{theorem}{Theorem}[chapter]

\section{Universal Approximation Theorem}

\rule{\textwidth}{0.5pt}
\begin{theorem}

    Let $\sigma$ be any continuous discriminatory function.
    
    $$
\displaystyle
\text{Then finite sums of the form}
\quad
G(x) = \sum_{j=1}^{N} \alpha_j \sigma(y_i^\top x + \theta_j)
\quad
\text{are dense in } C(I_n).
$$ 

In other words, given any $f \in C(I_n)$ and $\varepsilon > 0$, there is a sum, $G(x)$, of the above form, for wich:

$$
|G(x) - f(x)| < \varepsilon \quad \quad \forall x \in I_n
$$
\rule{\textwidth}{0.5pt}
\end{theorem}

\subsubsection{Bounds of the Universal Approximation Theorem}

Any function $f(x)$ on $\mathbb{R}^d$ with some smoothness conditions can be approximated by a single hidden layer sigmoidal neural network $f_n(x)$ with $n$ hidden units such that:

$$
\int_{B_r} \left(f(x)-f_n(x)\right)^2 \mu(dx) \le \dfrac cn
$$

where $\mu$ is a probability measure on ball $B_r = \{x:|x| < r\}$, $c$ is a constant independent of $n$ and $r$.

\begin{observationblock}[Feature Learning Advantage]

A result of the Universal Approximation Theorem is that no linear combinain of $n$ fixed basis functions yells integrated square error smaller than order:
$$
\left( \dfrac 1n \right) ^ \frac 2d
$$

\end{observationblock}

\section{Number of Linear Regions}

Be $D_i$ the size of the input layer, $D$ the size of the hidden layer, Then, given $D_i \le D$. 

The number of linear regions $N$ is given by:

$$
N \le \sum_{j=0}^{D_i} \binom{D}{j}
$$

\begin{exampleblock}
    Let's consider $D_i = 2$ and $D = 3$. The number of linear regions is given by:

    \begin{minipage}{0.65\textwidth}
        $$
        N \le \binom{3}{0} + \binom{3}{1} + \binom{3}{2} = 1 + 3 + 3 = 7
        $$
    \end{minipage}%
    \begin{minipage}{0.3\textwidth}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{assets/regions.png}
        \end{figure}
    \end{minipage}
\end{exampleblock}

\subsubsection{Fixed Functions}



% \begin{center}
%     \begin{tikzpicture}[x=2cm, y=1.5cm, every node/.style={circle, draw, minimum size=1cm}]
%         % Input layer (2 neurons)
        
%         % Hidden layer (3 neurons)
%         \foreach \j in {1,...,3} {
%             \node[] (H\j) at (1.5,-\j) {$B_{\j}(x)$};
%         }
            
%         \node[] (1) at (0,-1.5) {$\ $};

%         % Connect input layer to hidden layer
%         \foreach \i in {1,2} {
%             \draw[->] (H\i) -- (1);
%         }
%     \end{tikzpicture}
% \end{center}


$$
y = \sum_{w_i} \phi(W_{i_{D_i}} x_{D_i} + b_i) \sim \left\{B_i(x)\right\}
$$

\subsubsection{Polynomial case}

If the dimension is $DIM = 1$ we have something of the form:
$$
y = w_0 + w_1 x + w_2 x^2 + \dots + w_M x^M
$$

If the dimension is $DIM = D$, we have:

$$
y = w_o + \sum_{i = 1}^D w_ix_i + \sum_{i,j = 1}^D w_{ij}x_ix_j + \dots + \sum_{i_1, \dots, i_D = 1}^D w_{i_1, \dots, i_D}x_{i_1} \dots x_{i_D}
$$

The consequence is that the number of parameters grows exponentially with the dimension.

\subsubsection{Curse of dimensionality}

$$
B_i(x) = 
\begin{cases}
    0 & \text{if $x$ does not face in the block $b_i$}\\
    majority & \text{if the class of $x$ in the block $ A$}
\end{cases}
$$

\subsubsection{Limitations of Fixed Basis Functions}

High dimensionality introduces several challenges. One intuitive explanation comes from examining the probability density function in high-dimensional spaces.

Consider the marginal density of the radial coordinate:
$$
p(r) = \int_{d\Omega} d\Omega \int p(x)\, dx,
$$
where $d\Omega$ denotes the differential solid angle.

By switching to polar coordinates, we have:
$$
p(x) \to p(r, \Omega),
$$
so that the radial density becomes:
$$
p(r) = \dfrac{S_D \, r^{D-1}}{(2\pi \sigma^2)^{D/2}} e^{-r^2/(2\sigma^2)},
$$
with $S_D$ representing the surface area of the unit sphere in $D$ dimensions.

A key insight is that the probability mass concentrates around a specific radius. In fact, the mode of the radial density occurs approximately at:
$$
r^* \approx \sigma \sqrt{D}.
$$
Moreover, for a small deviation $\varepsilon$ from $r^*$, the density decays as:
$$
p(r^* + \varepsilon) = p(r^*) \exp \left( -\dfrac{3 \varepsilon^2}{2\sigma^2} \right).
$$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/curse_dim.png}
    \caption{Curse of dimensionality.}
    \label{fig:curse_dim}
\end{figure}


\subsubsection{Mainfolds hypothesis}

Most "naturally occuring" datasets lie on a low-dimensional mainfold


\chapter{Deep Networks}
\section{Composition of shallow networks}

The composition of shallow networks is a way to increase the number of linear regions. The idea is to stack multiple shallow networks to create a deep network.

Let's consider firstly a simple example with two shallow networks:

\begin{center}
    \begin{tikzpicture}[x=2cm, y=1.5cm, every node/.style={circle, draw, minimum size=1cm}]
        % Input layer (3 neurons)
        \node[] (X) at (0,-2) {$X$};
        
        % Hidden layer (3 neurons)
        \foreach \j in {1,...,3} {
            \node[] (H\j) at (1.5,-\j) {$h_{\j}$};
        }
        
        % Output layer (1 neuron)
        \node[] (Y) at (3,-2) {$y$};

        % Hidden layer (3 neurons)
        \foreach \j in {1,...,3} {
            \node[] (H'\j) at (4.5,-\j) {$h'_{\j}$};
        }
        
        % Output layer (1 neuron)
        \node[] (Y') at (6,-2) {$y'$};

        % Connect input layer to hidden layer
        \foreach \j in {1,...,3} {
            \draw[->] (X) -- (H\j);
        }
        
        % Connect hidden layer to output layer
        \foreach \j in {1,...,3} {
            \draw[->] (H\j) -- (Y);
        }

        % Connect hidden layer to output layer
        \foreach \j in {1,...,3} {
            \draw[->] (Y) -- (H'\j);
        }

        \foreach \j in {1,...,3} {
            \draw[->] (H'\j) -- (Y');
        }
    \end{tikzpicture}
\end{center}

In this case we can distinguish how the two network behaves:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/composition.png}
    \caption{Composition of shallow networks.}
    \label{fig:composition}
\end{figure}

We can reinterpret the composition of shallow networks as 2-layer a deep network:

\begin{center}
    \begin{tikzpicture}[x=2cm, y=1.5cm, every node/.style={circle, draw, minimum size=1cm}]
        % Input layer (3 neurons)
        \node[] (X) at (0,-2) {$X$};
        
        % Hidden layer (3 neurons)
        \foreach \j in {1,...,3} {
            \node[] (H\j) at (1.5,-\j) {$h_{\j}$};
        }
        
        % Hidden layer (3 neurons)
        \foreach \j in {1,...,3} {
            \node[] (H'\j) at (3.5,-\j) {$h'_{\j}$};
        }

            % Output layer (1 neuron)
        \node[] (Y') at (5,-2) {$y'$};
            
        % Connect input layer to hidden layer
        \foreach \j in {1,...,3} {
            \draw[->] (X) -- (H\j);
        }

        % Connect hidden layer to output layer
        \foreach \i in {1,...,3} {
            \foreach \j in {1,...,3} {
                \draw[->] (H\i) -- (H'\j);
            }
        }

        \foreach \j in {1,...,3} {
            \draw[->] (H'\j) -- (Y');
        }
    \end{tikzpicture}
\end{center}

\section{Generic MLP}

The generic Multi-Layer Perceptron (MLP) is a deep network with multiple hidden layers.

\begin{center}
    \usetikzlibrary{calc}
    \begin{tikzpicture}[x=2cm, y=1.5cm, every node/.style={circle, draw, minimum size=1cm}]
        % Input layer (3 neurons + bias)
        \foreach \i in {0,...,3} {
            \ifnum\i=0
                \node[orange] (X\i) at (0,-\i) {$\beta_0$};
            \else
                \node[] (X\i) at (0,-\i) {$X_{\i}$};
            \fi
        }
        
        % Hidden layer (4 neurons + bias)
        \foreach \j in {0,...,4} {
            \ifnum\j=0
                \node[orange] (H\j) at (1.5,0.5 -\j) {$\beta_1$};
            \else
                \node[] (H\j) at (1.5,0.5 -\j) {$h_{\j}$};
            \fi
        }
        
        % Hidden layer (2 neurons + bias)
        \foreach \j in {0,...,2} {
            \ifnum\j=0
                \node[orange] (H'\j) at (3,-0.5 -\j) {$\beta_2$};
            \else
                \node[] (H'\j) at (3,-0.5 -\j) {$h'_{\j}$};
            \fi
        }

        % Hidden layer (3 neurons + bias)
        \foreach \k in {0,...,3} {
            \ifnum\k=0
                \node[orange] (H''\k) at (4.5,-\k) {$\beta_3$};
            \else
                \node[] (H''\k) at (4.5,-\k) {$h''_{\k}$};
            \fi
        }

        % Output layer (2 neuron)
        \foreach \l in {1,...,2} {
            \node[] (Y'\l) at (6,-0.5 -\l) {$y_{\l}$};
        }

        % Connect input layer to hidden layer
        \foreach \i in {0,...,3} {
            \foreach \j in {1,...,4} {
                \ifnum\i=0
                    \draw[->, orange] (X\i) -- (H\j);
                \else
                    \draw[->] (X\i) -- (H\j);
                \fi
            }
        }

        % Connect hidden layer to output layer
        \foreach \i in {0,...,4} {
            \foreach \j in {1,...,2} {
                \ifnum\i=0
                    \draw[->, orange] (H\i) -- (H'\j);
                \else
                    \draw[->] (H\i) -- (H'\j);
                \fi
            }
        }

        \foreach \i in {0,...,2} {
            \foreach \j in {1,...,3} {
                \ifnum\i=0
                    \draw[->, orange] (H'\i) -- (H''\j);
                \else
                    \draw[->] (H'\i) -- (H''\j);
                \fi
            }
        }

        \foreach \j in {0,...,3} {
            \foreach \l in {1,...,2} {
                \ifnum\j=0
                    \draw[->, orange] (H''\j) -- (Y'\l);
                \else
                    \draw[->] (H''\j) -- (Y'\l);
                \fi
            }
        }
    \end{tikzpicture}
\end{center}