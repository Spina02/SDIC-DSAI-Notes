
\chapter{Transformers}

\section{Lecture (13/05/2025)}

Transformers are a type of neural network architecture that are particularly effective for natural language processing tasks. They are based on the \textit{attention mechanism}, which allows the model to focus on different parts of the input sequence.
\vspace{-1em}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{assets/attention.png}
\end{figure}
\vspace{-1em}
Let's consider an initial set of vectors $x_1, \ldots, x_n$, at each step of the attention mechanism, we want to compute a new set of vectors $y_1, \ldots, y_n$ (in the same space) which are a function of the $x_i$'s. In this way, each new step creates another representation of the same input token.

Let's see now how to realize this kind of architecture.

A standard neural network layer $f[x]$, takes a $D \times 1$ input $x$ and applies a linear transformation followed by an activation function like a ReLU, so:
$$
f[x] = ReLU[\beta + \Omega x]
$$
where $\beta$ contains the biases, and $\Omega$ contains the weights.

A self-attention block $\mathbf{sa}[\bullet]$ takes $N$ inputs $x_1, \ldots, x_N$, each of dimension $D \times 1$, and returns $N$ outputs, each of which is also of size $D \times 1$. In the context of NLP, each input represents a word or word fragment. First, a set of values are computed for each input:
$$
v_m = \beta_v + \Omega_v x_m
$$
where $\beta_v \in \mathbb{R}^{D \times 1}$ and $\Omega_v \in \mathbb{R}^{D \times D}$ represent biases and weights, respectively.

Then the $n^{th}$ output $\mathbf{sa}_n[x_1, \ldots, x_N]$ is a weighted sum of all the values $v_1, \ldots, v_N$:
\vspace{0.4em}
$$
\mathbf{sa}_n[x_1, \ldots, x_N] = \sum_{m=1}^{N} a[x_m, x_n] v_m
$$
The scalar weight $a[x_m, x_n]$ is the attention that the $n^{th}$ output pays to input $x_m$. The $N$ weights $a[\bullet, x_n]$ are non-negative and sum to one. Hence, self-attention can be thought of as routing the values in different proportions to create each output.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/self-attention-route.png}
    \caption{The self-attention mechanism takes $N$ inputs $x_1, \ldots, x_N \in \mathbb{R}^D$ (here $N = 3$ and $D = 4$) and processes each separately to compute $N$ value vectors. The $n^{th}$ output $\mathbf{sa}_n[x_1, \ldots, x_N]$ ($\mathbf{sa}_n[x_\bullet]$ for short) is then computed as a weighted sum of the $N$ value vectors, where the weights are positive and sum to one. \cite{understanding}}
\end{figure}

\vspace{-1em}
The value vectors $\beta_v + \Omega_v x_m$ are computed independently for each input $x_m$, and these vectors are combined linearly by the attention weights $a[x_m, x_n]$. However, the overall self-attention computation is nonlinear.

This is an example of a \bfit{hypernetwork}, where one network branch computes the weights of another.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/self-attention.png}
    \caption{Self-attention for N = 3 inputs $x_n$, each with dimension $D = 4$.}
    \label{fig:self-attention}
\end{figure}

To compute the attention, we apply two more linear transformations to the inputs:
\vspace{0.4em}
$$
q_n = \beta_q + \Omega_q x_n,
\qquad \qquad
k_m = \beta_k + \Omega_k x_m
$$
where $\{q_n\}$ and $\{k_m\}$ are termed queries and keys, respectively. Then we compute dot products between the queries and keys and pass the results through a softmax function:
\vspace{0.4em}
$$
a[x_m, x_n] = \text{softmax}_m [k^\top_\bullet q_n] = \frac{\exp(k^\top_\bullet q_n)}{\sum_{m'} \exp(k^\top_{m'} q_n)}
$$
so for each $x_n$, they are positive and sum to one. This is known as \bfit{dot-product self-attention}.

The dot product operation returns a measure of similarity between its inputs, so the weights $a[x_m, x_n]$ depend on the relative similarities between the $n^{th}$ query and all of the keys. The softmax function means that the key vectors “compete” with one another to contribute to the final result.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/dot-prod-self-att.png}
    \caption{Computing attention weights: (a) Compute the query, key, and value vectors for each input. (b) Apply softmax to query-key dot products to form non-negative attentions that sum to one. (c) These route the value vectors via the sparse matrix from \cref{fig:self-attention}.}
    \label{fig:dot-prod-self-att}
\end{figure}

\section{The transformer layer (Lecture 20/05/2025)}

The self-attention mechanism is only a part of a larger \bfit{transformer layer}.
This consist of a multi-head self-attention unit, followed by a fully connected network $\bm{mlp}[x_\bullet]$; Both units are residual networks. In addition, it is typical to add a LayerNorm operation after both the self-attention and fully connected networks.

The LayerNorm is similar to the batch norm, but normalized each embedding in each batch element separately, using statictics calculated across its $D$ dimensions.

Since we have:

$$
f_k = \beta_k + \Omega_k a(f_{k-1})
$$

this means that the norm of the $k^{th}$ layer is much lower than the norm of the $(k-1)^{th}$ layer:

$$
\| f_k \| < \| f_{k-1} \|
\qquad \text{or} \qquad
\| f_k \| < \dfrac 12 \|a(f_{k-1})\|
$$

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/transformer_layer.png}
    \caption{A transformer layer.}
    \label{fig:transformer-layer}
\end{figure}

% We have also that the derivative of th losses of the $k^th$ and $k-1^th$ layer are related by the following relationship:
% $$
%! missing
% $$

\dots

% $$
% \begin{cases}
%     f_j \sim \mathcal N(0, \sigma^2 \Omega)
%     \\
    
% \end{cases}
% $$

$$
\mathbf E(f'_j) = \mathbf E (\beta_j + \sum_e \Omega_{je} h_e) = \underbrace{\mathbf E(\beta_j)}_{=\ 0} + \mathbf E(\sum_e \underbrace{\Omega_{je} h_e}_{=\ 0}) = 0
$$

$$
\sigma^2 (f'_j) = \mathbf E({f_j'}^2) - \cancel{(\mathbf E(f_j'))^2}
= 
\sum_e \sum_j \mathbf E(\Omega_{ij} \Omega_{ie})  \mathbf E(h_j h_e)
$$

\dots

$$
\sigma^2_{f'} = \dfrac 12 D_h \cdot \sigma^2_\Omega \cdot \sigma^2_f
\qquad \Rightarrow \qquad
1 = \dfrac 12 D_h \cdot \sigma^2_\Omega
\qquad \Rightarrow \qquad
\boxed{\sigma^2_\Omega = \dfrac 2{D_h}}
$$

Transformer layers consist of two residual networks stacked together. Without proper normalization, the residual connections would experience exponential growth in size as they propagate through the network. To address this issue, we apply BatchNorm before each residual connection, which constrains the growth to be linear rather than exponential.

In this way we have that, at initialization, the deeper nodes of the network contribute less to the output, creating a sort of "effective depth" in the first layer. As the training progresses, the deeper nodes contribute more to the output, and the effective depth increases.

\section{Transformers for NLP}

A typical NLP pipeline starts with a tokenizer, which are mapped to a learning embedding, and then pass through different transformer layers.

There are different architectures we are interested in:
\begin{itemize}
    \item Encoder (BERT)
    \item Decoder (GPT)
    \item Encoder-Decoder
    \item IMG
\end{itemize}

\subsection{Encoder model: BERT}

BERT is an encoder model which uses a vocabolary of about $30.000$ tokens. Input tokens are converted into 1024 dimensional word embeddings and passed through 24 transformer layers. Each layer contains a mechanism of 16 heads. The total amount of parameters is about $340$ million.

The input tokens (and a special <cls> token denoting the start of the sequence) are converted to word embeddings. These embeddings are passed through a series of transformer layers to create a set of output embeddings.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/bert.png}
    \caption{Pre-training for BERT-like encoders.}
    \label{fig:bert}
\end{figure}

\subsubsection{Pre-training}

In the pre-training stage, the network is trained using self-supervision. For BERT, the self-supervision task consists of predicting missing words from sentences from a large internet corpus, forcing the transformer network to understand some syntax. A small fraction of the input tokens are randomly replaced with a generic <mask> token. In pre-training, the goal is to predict the missing word from the associated output embedding. To this end, the outputs corresponding to the masked tokens are passed through softmax functions, and a multiclass classification loss is applied to each. This task has the advantage that it uses both the left and right context to predict the missing word but has the disadvantage that it does not make efficient use of data; here, seven tokens need to be processed to add two terms to the loss function.

After pre-training, the encoder is fine-tuned using manually labeled data to solve a particular task. Usually, a linear transformation or a multi-layer perceptron (MLP) is appended to the encoder to produce whatever output is required.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{assets/pre-training-bert.png}
    \caption{Pre-training for BERT-like encoders.}
    \label{fig:bert-pre-training}
\end{figure}

\subsection{Decoder model: GPT}

The decoder is a transformer model with a single attention head. It's aimed to predict the next word of a sentence, and generate a coherent text passage by feeding the extended sequence back into the model.

An autoregressive model predicts the conditional distributions $\Pr(t_n | t_1, \ldots, t_{n-1})$ for each token given all the prior ones, and hence, indirectly, computes the joint distribution $\Pr(t_1, \ldots, t_N)$ of all the $N$ tokens:

$$
\Pr(t_1, \ldots, t_N) = \Pr (t_1) \prod_{n = 2}^N \Pr(t_n | t_1, \dots, t_{n-1})
$$

The autoregressive formulation demonstrates the connection between maximizing the joint probability of the tokens and the next token prediction task.

\subsubsection{Masked self-attention}

To train a decoder, we seek parameters that maximize the log probability of the input text under the autoregressive model. Ideally, we would like to replace all the log probabilities and gradients in the same forward pass rather than doing a forward pass for each token in the sentence. However, if we pass in the full sentence, the term computing $\log[\Pr(t_{next}|all\ tokens)]$ would have access to both the answer and the right context, so the model would simply copy the word after the one we want to learn, and won't train properly.

Fortunately, the tokens only interact in the self-attention layers in a transformer network. Hence, the problem can be resolved by setting the corresponding self-products in the self-attention computation to zero so that the attention is only allowed between tokens before the answer. The input tokens are passed through the softmax function. This is known as \textbf{masked self-attention}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{assets/masked_transformer.png}
    \caption{Training GPT-type decoder with masked self-attention. Each token can only attend to previous tokens (orange connections) to predict the next token in sequence.}
    \label{fig:masked-self-attention}
\end{figure}

\vspace{-1em}

The decoder network operates through the following process:

\begin{itemize}[noitemsep]
\item Input text is first tokenized and converted into \bfit{embeddings}.

\item These embeddings are processed through transformer layers that employ \bfit{masked self-attention}, ensuring that each token can only attend to itself and previous tokens in the sequence.

\item Each output embedding represents the context up to that position in the sequence, with the objective of \bfit{predicting the subsequent token}.

\item Following the transformer layers, a linear projection maps each output embedding to vocabulary size, and a softmax function transforms these logits into probability distributions over the \bfit{vocabulary}.
\end{itemize}

During training, the objective is to maximize the log-likelihood of the correct next tokens at each position in the ground truth sequence, accomplished through standard cross-entropy loss optimization.

\subsubsection{Few-Shot Learning}

Since the autoregressive language model defines a probability model over text sequences, it can be used to sample new examples of plausible text. To generate from the model, we start with an input sequence of text (which might be just the special <start> token indicating the beginning of the sequence) and feed this into the network, which then outputs the probabilities over possible subsequent tokens. The new extended sequence can be fed back into the decoder network to yield the probability distribution over the next token. By repeating this process, we can generate large bodies of text.

Large language models such as GPT-3 implement these concepts at an unprecedented scale. A characteristic of models trained at this scale is their ability to execute various tasks without requiring fine-tuning. When presented with multiple examples of accurate question-answer pairs followed by a new question, these models frequently provide correct responses to the final question by extending the sequence pattern.

\begin{exampleblock}[Correcting the grammar]
\textbf{Train:}
\begin{itemize}[noitemsep]
\item \textbf{Poor English input:} I eated the purple berries.
\item \textbf{Good English output:} I ate the purple berries.
\item \textbf{Poor English input:} Thank you for picking me as your designer. I'd appreciate it.
\item \textbf{Good English output:} Thank you for choosing me as your designer. I appreciate it.
\item \textbf{Poor English input:} The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications.
\item \textbf{Good English output:} The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications.
\end{itemize}
\rule{\textwidth}{0.5pt}
\textbf{Test:}
\begin{itemize}[noitemsep]
\item \textbf{Poor English input:} I'd be more than happy to work with you in another project.
\item \textbf{Good English output:} I'd be more than happy to work with you on another project.
\end{itemize}
\end{exampleblock}

\subsubsection{Encoder-Decoder Architecture}

Translation between languages is an example of a \emph{sequence-to-sequence} task. One common approach uses both an encoder (to compute a good representation of the source sentence) and a decoder (to generate the sentence in the target language). This is aptly called an \bfit{encoder-decoder model}.

The encoder-decoder architecture operates through the following process:

\begin{itemize}[noitemsep]
\item The \bfit{encoder} receives the source sentence and processes it through transformer layers to create output representations for each token.

\item The \bfit{decoder} receives the target sequence and processes it through modified transformer layers that employ masked self-attention to predict the next token at each position.

\item Crucially, decoder layers also attend to encoder outputs through \bfit{cross-attention}, where a new attention layer is inserted between the masked self-attention and feed-forward components.

\item In cross-attention, queries are computed from decoder embeddings while keys and values derive from encoder embeddings, enabling the decoder to access source sequence information.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{assets/encoder-decoder.png}
    \caption{Encoder-decoder architecture. Two sentences are passed to the system with the goal of translating the first into the second. a) The first sentence is passed through a standard encoder. b) The second sentence is passed through a decoder that uses masked self-attention but also attends to the output embeddings of the encoder using cross-attention (orange rectangle). The loss function is the same as for the decoder model; we want to maximize the probability of the next word in the output sequence.}
    \label{fig:encoder-decoder}
\end{figure}

\newpage

\subsubsection{Transformers fot long sequences}

\dots

\subsection{Transformers for images}

\dots

\subsubsection{Vision Transformers (ViT)}

The Vision Transformer tackled the problem of image resolution by dividing the image into $16 \times 16$ patches (\cref{fig:vit}). Each patch is mapped to an input embedding via a learned linear transformation, and these representations are fed into the transformer network

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/vit.png}
    \caption{Vision Transformer architecture.}
    \label{fig:vit}
\end{figure}

\dots