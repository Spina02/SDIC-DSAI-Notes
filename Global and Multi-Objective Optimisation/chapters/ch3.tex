% \chapter{Evolution Strategies}\label{ch:es}

% \section{Introduction}

% Evolution Strategies (ES) are a family of stochastic optimization algorithms that emerged in the 1960s. Like Genetic Algorithms, they maintain a \bfit{population} of candidate solutions and apply \bfit{variation} and \bfit{selection} to evolve toward high-quality solutions. However, ES differ in three key respects:

% \begin{itemize}
%   \item \textbf{No crossover (usually):} \bfit{Offspring} are typically generated by \bfit{mutating} parents, without recombining two genotypes.
%   \item \textbf{Truncated selection:} The most common selection schemes are the $ (\mu,\lambda) $ and $ (\mu+\lambda) $ strategies, which differ in whether parents compete with offspring.
%   \item \textbf{Real-valued representation:} Individuals are encoded directly as floating-point vectors, enabling natural exploration of continuous domains.
% \end{itemize}

% \section{Core Components}

% \subsubsection{Representation}

% In a classical ES, each \bfit{individual} is a real-valued vector $\mathbf{x}\in\mathbb{R}^n$. Often this vector is paired with a set of \bfit{strategy parameters} $\boldsymbol\sigma$ (e.g.\ mutation step sizes), yielding an augmented individual $\langle \mathbf{x}, \boldsymbol\sigma\rangle$.

% \subsubsection{Key Parameters}

% The performance and behavior of an evolution strategy depend on several critical parameters:

% \begin{itemize}
%   \item $\lambda \longrightarrow$ number of \bfit{offspring} generated each generation,
%   \item $\mu \longrightarrow$ number of \bfit{parents} (or survivors).
%   \item \textbf{$\sigma$}: Vector of \bfit{mutation strengths} (step sizes), which may self-adapt.
% \end{itemize}

% \subsubsection{Selection Scheme}

% We can distinguish two main selection schemes:

% \begin{itemize}
%       \item $ (\mu,\lambda) $-ES: select the next $\mu$ \bfit{individuals} from the $\lambda$ \bfit{offspring} only.
%       \item $ (\mu+\lambda) $-ES: select the next $\mu$ from the combined set of $\mu$ \bfit{parents} and $\lambda$ \bfit{offspring}.
% \end{itemize}

% \subsection{Selection and Variation}

% \subsubsection{Selection}
% Selection in ES is typically \bfit{truncated}: only the best $\mu$ \bfit{individuals} are selected from a pool of $\lambda$ \bfit{offspring} (or parents + offspring). This ensures strong selection pressure and rapid convergence.

% \begin{observationblock}[Truncated Selection]
% Truncated selection is less likely to maintain diversity than probabilistic selection (e.g., roulette wheel), but it is simple and effective for continuous optimization.
% \end{observationblock}

% \subsubsection{Variation (Mutation and Recombination)}

% \bfit{Mutation} is the primary variation operator in ES, introducing Gaussian noise to each coordinate:
% $$
%   x_{i,j}' = x_{i,j} + \sigma_j \cdot N(0,1).
% $$
% A \bfit{good mutation operator} should satisfy:

% \begin{definitionblock}[Properties of Mutation]
% \begin{itemize}
%   \item \textbf{Reachability:} Any point in search space is reachable in finitely many steps.
%   \item \textbf{Unbiasedness:} Mutation does not depend on fitness information.
%   \item \textbf{Scalability:} Mutation strength adapts to the landscape via $\boldsymbol\sigma$.
% \end{itemize}
% \end{definitionblock}

% Although classic ES omit crossover, one can introduce a \bfit{recombination} step:
% $$
%   (\mu/\rho,\lambda)\text{-ES},\quad (\mu/\rho+\lambda)\text{-ES},
% $$
% where each of the $\lambda$ \bfit{offspring} averages or picks components from $\rho$ \bfit{parents}.

% \begin{itemize}
%   \item \textbf{Discrete Recombination:} For each gene $j$, select $x_j$ from one of the $\rho$ \bfit{parents} at random.
%   \item \textbf{Intermediate Recombination:} Compute
%   $$
%     x_j' = \frac{1}{\rho}\sum_{i=1}^\rho x_{i,j}.
%   $$
% \end{itemize}

% \begin{tipsblock}[When to Use Recombination]
% Recombination can improve exploration in multimodal landscapes but adds complexity. For smooth, unimodal problems, mutation alone often suffices.
% \end{tipsblock}

% \section{Algorithmic Cycle}

% A generic $ (\mu,\lambda) $-ES proceeds as follows:

% \begin{enumerate}
%   \item \textbf{Sampling:} Generate $\lambda$ \bfit{offspring} by adding Gaussian perturbations:
%   $$
%     \mathbf{x}_i' = \mathbf{x}_\mathrm{parent} + \boldsymbol\varepsilon_i,\quad
%     \boldsymbol\varepsilon_i \sim \mathcal{N}(\mathbf{0}, \operatorname{diag}(\boldsymbol\sigma)^2).
%   $$
%   \item \textbf{Evaluation:} Compute the \bfit{fitness} $f(\mathbf{x}_i')$ of each \bfit{offspring}.
%   \item \textbf{Selection:} Choose the next $\mu$ \bfit{individuals} according to the scheme:
%   \begin{itemize}
%     \item $ (\mu,\lambda) $: best $\mu$ among the $\lambda$ \bfit{offspring}.
%     \item $ (\mu+\lambda) $: best $\mu$ among \bfit{parents} plus \bfit{offspring}.
%   \end{itemize}
%   \item \textbf{Adaptation:} Update the \bfit{strategy parameters} $\boldsymbol\sigma$ (e.g.\ via the one-fifth success rule or CMA).
% \end{enumerate}

% \section{Self-Adaptation of Mutation Rates}

% In self-adaptive ES, each \bfit{individual} carries its own $\boldsymbol\sigma$. Both $\mathbf{x}$ and $\boldsymbol\sigma$ are subject to \bfit{mutation}.

% \subsubsection{One-Fifth Success Rule}

% A simple, empirical method to adjust a global step size $\sigma$:

% \begin{itemize}
%   \item Monitor the fraction $p_S$ of successful \bfit{mutations} (\bfit{offspring} fitter than \bfit{parent}) over $k$ generations.
%   \item If $p_S > 1/5$, increase $\sigma$: $\sigma \leftarrow \sigma / c$.
%   \item If $p_S < 1/5$, decrease $\sigma$: $\sigma \leftarrow \sigma \cdot c$.
%   \item Otherwise, leave $\sigma$ unchanged.
% \end{itemize}

% Here $c\in(0,1]$ (commonly $0.817<c<1$) controls adaptation rate.

% \begin{tipsblock}[Using the One-Fifth Rule]
% Choosing $k$ too small yields noisy estimates of $p_S$; too large slows adaptation. A balance (e.g.\ $k=10$) is typical.
% \end{tipsblock}

% \begin{exampleblock}[Practical Example: Step Size Adaptation]
% Suppose $\sigma=0.5$ and $c=0.85$. Over $k=10$ generations, if $p_S=0.3>1/5$, then $\sigma\to\sigma/0.85\approx0.59$ (increase step size). If $p_S=0.1<1/5$, then $\sigma\to0.5\times0.85=0.425$ (decrease step size).
% \end{exampleblock}