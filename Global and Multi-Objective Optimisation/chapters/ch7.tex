\chapter{Estimation of Distribution Algorithms}\label{ch:eda}

\section{From Implicit to Explicit Models}

Traditional evolutionary algorithms, such as Genetic Algorithms (GAs), employ operators like selection, crossover, and mutation to navigate the search space. Collectively, these operators induce an \textit{implicit} model of solution quality: selection favors promising individuals, while crossover and mutation explore their neighborhoods to discover new candidates. However, this implicit modeling limits the algorithm's ability to directly capture and exploit the structure of high-quality solutions.

In contrast, \bfit{Estimation of Distribution Algorithms (EDAs)}, also referred to as \bfit{Probabilistic Model-Building Genetic Algorithms (PMBGAs)}, adopt a fundamentally different strategy. Rather than relying on traditional genetic operators, EDAs construct an \textit{explicit} probabilistic model that characterizes the distribution of promising solutions identified thus far. This model is then used to sample a new population of candidate solutions, which subsequently informs the next iteration of model refinement. The evolutionary process is thereby reframed as a cycle of model construction, sampling, and iterative improvement.

\subsection{Model Fitting via Classification}

A principled approach to explicit modeling is to recast the problem as a classification task. Here, the objective is to train a machine learning classifier to discriminate between "good" and "bad" regions of the search space, using the fitness values of individuals in the current population as labels.

\subsubsection{Learnable Evolution Models}
The \bfit{Learnable Evolution Model (LEM)} framework formalizes this classification-based approach. Its iterative procedure can be summarized as follows:
\begin{enumerate}
    \item Execute several standard evolutionary steps to generate a diverse population.
    \item Partition the population into two classes according to fitness: a "fit" set and an "unfit" set.
    \item Train a classifier (e.g., decision tree, support vector machine, or neural network) to distinguish between fit and unfit individuals. This classifier serves as an explicit model of high-fitness regions in the search space.
    \item Eliminate unfit individuals and generate new candidates by sampling from regions identified by the classifier as "fit."
    \item Repeat this process until a predefined termination criterion is satisfied.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/lem-classification.png}
    \caption{\centering The classification process in a Learnable Evolution Model.}
    \label{fig:lem_classification}
\end{figure}

\vspace{-1.5em}

\subsubsection{Generating New Individuals}
A key challenge in this approach is how to generate new individuals using the trained model. This depends on whether the model is \bfit{generative} or \bfit{discriminative}.
\begin{itemize}
    \item \bfit{Generative models} can directly produce new samples that conform to the learned data distribution.
    \item \bfit{Discriminative models}, such as most classifiers, can only determine whether a given, existing individual belongs to the "fit" class or not.
\end{itemize}
Since classifiers are typically discriminative, two main sampling techniques are used:

\begin{itemize}
    \item \textbf{Rejection Sampling}: new individuals are generated randomly from the entire search space and are then passed to the classifier. If the classifier labels an individual as "fit," it is accepted into the new population; otherwise, it is rejected, and the process repeats. This method is simple but can be highly \textit{inefficient}, especially late in the search when the "fit" region may be very small compared to the entire space, leading to a high rejection rate.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{assets/rejection-sampling.png}
    \label{fig:rejection_sampling}
\end{figure}

    \vspace{-1.5em}

    \item \textbf{Region-Based Sampling}: New individuals are generated by sampling directly from regions of the search space identified as "fit" by the classifier. For classifiers such as decision trees, these regions correspond to explicit rules or hyper-rectangles. Sampling is restricted to these regions, improving efficiency by avoiding the generation of individuals from unpromising areas. The effectiveness of this method depends on the classifier's ability to define such regions.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.55\textwidth]{assets/region-based-sampling.png}
        \label{fig:region_based_sampling}
    \end{figure}

    \vspace{-1.5em}
\end{itemize}



\section{Probabilistic Model-Based EDAs}
Instead of using a classifier, the most common family of EDAs builds and samples from an explicit \bfit{probability distribution} that represents the distribution of promising individuals in the population.

\subsection{Representing the Distribution of Promising Solutions}
The core task of an EDA is to build a probabilistic model that captures the features of high-performing individuals. The primary challenge lies in creating a representation that is both expressive enough to guide the search effectively and compact enough to be computationally feasible. A full, explicit joint probability distribution is almost always intractable due to the curse of dimensionality. Several strategies have been developed to address this.

\subsubsection{Histogram-Based Models}
A conceptually simple way to model the distribution is to create a multi-dimensional histogram. The search space is partitioned into a grid of discrete hypercubes (or "bins"), and the model stores the average fitness or count of promising solutions that fall into each bin. New solutions can then be sampled from the bins with higher scores.

However, this method's utility is severely limited by its exponential complexity. If each of the $n$ dimensions of the search space is divided into just $d$ intervals, the total number of hypercubes is $d^n$. This number grows so rapidly that the approach becomes unfeasible even for a small number of dimensions, necessitating more compact representations.

\subsubsection{Gaussian Mixture Models for Continuous Spaces}
For problems in continuous domains (e.g., $\mathbb{R}^n$), a more scalable approach is to model the probability distribution of good solutions as a \bfit{Gaussian Mixture Model (GMM)}. Instead of a discrete grid, the distribution is represented as the sum of a fixed number, $b$, of multivariate Gaussian distributions. This allows the model to represent multiple promising regions in the search space simultaneously.

A full $n$-dimensional Gaussian distribution is defined by:
\begin{itemize}
    \item A mean vector $\vec{\mu}$ of length $n$.
    \item A covariance matrix $\Sigma$ of size $n \times n$, which captures the dependencies between variables.
\end{itemize}
While more compact than a histogram, the space required to store the full covariance matrix for each component is $n^2$. The total space complexity to represent the distribution as a sum of $b$ Gaussians is $b(n + n^2)$, which can still be prohibitive if all variable interactions are modeled.

\subsubsection{Marginal Distribution Models}
To drastically reduce complexity, many EDAs adopt a powerful simplifying assumption: that the variables (genes) are \bfit{mutually independent}. This assumption allows the high-dimensional joint probability distribution to be factorized into the product of $n$ one-dimensional \bfit{marginal distributions}.
$$ P(x_1, \ldots, x_n) = P_1(x_1) \cdot P_2(x_2) \cdots P_n(x_n) $$
This is the foundation of \bfit{univariate EDAs}. Under this model, the complexity of a Gaussian mixture is greatly reduced. Each of the $n$ marginal distributions is a 1D Gaussian mixture, which only requires storing a mean and a variance for each of its $b$ components. The total space complexity falls from $O(b n^2)$ to a much more manageable $O(b n)$.

\subsubsection{Models for Finite Discrete Spaces}
The concept of using marginal distributions extends naturally to discrete domains.
\begin{itemize}
    \item If a gene can take one of $k$ possible discrete values, its marginal distribution can be represented by a probability vector of length $k-1$.
    \item For the very common case of \bfit{Boolean spaces} ($\{0,1\}^n$), the model is particularly simple. The marginal distribution for each gene can be described by a single value: the probability of that gene being 1. The entire probabilistic model is thus captured by a single $n$-dimensional vector of probabilities, which is extremely compact.
\end{itemize}

% This figure combines the four concepts from your slide into one.
% You would need to crop the original image into four separate files.
\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/eda-histogram.png}
        \subcaption{Histogram-based model.}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/eda-gaussian.png}
        \subcaption{Gaussian Mixture Model.}
    \end{minipage}
    
    \vspace{0.5cm}
    
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/eda-marginal.png}
        \subcaption{Marginal distributions from a joint distribution.}
    \end{minipage}%
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/eda-discrete.png}
        \subcaption{Marginal distributions for a finite discrete space.}
    \end{minipage}
    \caption{Four different approaches to modeling the distribution of promising solutions in EDAs.}
    \label{fig:eda_distribution_models}
\end{figure}

\vspace{-1em}

\subsection{Univariate EDAs}
\bfit{Univariate EDAs} adopt a standard approach in which the high-dimensional distribution over all genes is simplified by marginalizing everything to one dimension by modeling only the distribution of each gene individually. This means we are making a strong assumption: the value of each gene can be determined independently from the values of the other genes. While this independence assumption is often not strictly true in real-world problems, it allows for very simple and computationally efficient algorithms. We will examine two classic univariate EDA algorithms:
\begin{itemize}
    \item \bfit{Population-Based Incremental Learning (PBIL)}
    \item \bfit{Compact Genetic Algorithm (cGA)}
\end{itemize}

\subsubsection{Population-Based Incremental Learning (PBIL)}

\bfit{Population-Based Incremental Learning (PBIL)} is a foundational univariate EDA that iteratively refines a probabilistic model to guide the search for optimal solutions. 

For a problem of dimension $n$, PBIL maintains a vector of marginal distributions $D_1, D_2, \ldots, D_n$. The initial distribution is typically uniform. At each iteration:

\begin{itemize}
    \item A population of new individuals is generated by independently sampling each gene $j$ from $D_j$.
    \item The $b$ fittest individuals are selected from the population (truncated selection).
    \item For each gene $j$, let $N_j$ denote the empirical distribution of gene $j$ among the $b$ selected individuals.
    \item The probability vector is updated towards the observed distribution in the selected set:
    $$
        D_j \leftarrow (1-\alpha) D_j + \alpha N_j
    $$
    where $\alpha \in [0,1]$ is a learning rate that controls the speed of adaptation.
\end{itemize}

This process is repeated until a stopping criterion is met.

Population-Based Incremental Learning (PBIL) can be \bfit{extended to continuous domains}, such as $\mathbb{R}^n$, by representing each marginal distribution as a Gaussian distribution parameterized by a mean $\mu_{D_j}$ and a variance $\sigma^2_{D_j}$. After the selection step, for each gene $j$, the sample mean $\mu_{N_j}$ and sample variance $\sigma^2_{N_j}$ are computed from the selected individuals:
$$
\mu_{N_j} = \frac{1}{|P|} \sum_{P_i \in P} P_{i,j}
\qquad
\sigma^2_{N_j} = \frac{1}{|P|-1} \sum_{P_i \in P} (P_{i,j} - \mu_{N_j})^2
$$

The parameters of the Gaussian are then updated according to the following rules:
$$
    \mu_{D_j} \leftarrow (1-\alpha)\mu_{D_j} + \alpha \mu_{N_j}
,\qquad 
    \sigma^2_{D_j} \leftarrow (1-\alpha)\sigma^2_{D_j} + \alpha \sigma^2_{N_j}
$$
This approach enables PBIL to adapt both the central tendency and the dispersion of each marginal distribution, thereby facilitating efficient exploration and exploitation in continuous search spaces.

PBIL can also be extended to discrete spaces with more than two values per gene by representing each marginal as a categorical distribution and updating each probability component accordingly. In all cases, the learning rate $\alpha$ allows for gradual adaptation, balancing exploration and exploitation.

\subsubsection{Compact Genetic Algorithm (cGA)}

The \bfit{Compact Genetic Algorithm (cGA)} is a streamlined EDA designed for Boolean search spaces. Unlike traditional genetic algorithms, cGA does not maintain an explicit population. Instead, it iteratively updates a probability vector through pairwise competitions.

\begin{description}
    \item[1. Probability Vector:] Maintain a vector $D$, where $D_j$ is the probability of gene $j$ being \texttt{1}.
    \item[2. Sampling:] At each iteration, sample two individuals, $P_i$ and $P_k$, independently from $D$.
    \item[3. Competition:] Evaluate $P_i$ and $P_k$ and identify the winner ($U$) and loser ($V$) based on fitness.
    \item[4. Update:] For each gene $j$ where $U_j \neq V_j$, adjust $D_j$ by a small step $1/d$ such that:
    \begin{align*}
        &\text{If } U_j = 1 \text{ and } V_j = 0: \quad D_j \leftarrow D_j + 1/d \\
        &\text{If } U_j = 0 \text{ and } V_j = 1: \quad D_j \leftarrow D_j - 1/d
    \end{align*}
\end{description}

Repeat this process for a fixed number of iterations or until convergence. The cGA thus efficiently evolves the probability vector to represent promising solutions without explicitly storing a population.

\subsection{Issues of Univariate EDAs and Beyond}

Univariate EDAs assume that the distribution of each gene can be found independently from all the other genes: they do not model any linkage or interaction between genes. While this assumption simplifies the model, it is often unrealistic: in many real-world problems, genes interact in complex ways, and optimizing them independently is not effective.

\begin{itemize}
    \item \textbf{Gene independence is rarely true:} If genes were truly independent, we could optimize each one separately. In practice, dependencies between genes are common and important.
    \item \textbf{Local optima:} Because univariate EDAs cannot capture these dependencies, they may get stuck in local optima---situations where changing a single gene does not improve the solution, but changing several together would.
    \item \textbf{Need for multivariate models:} To address this, \bfit{multivariate EDAs} have been developed. These use more advanced probabilistic models that can represent and exploit dependencies between variables, allowing for more effective search in complex landscapes.
\end{itemize}

One of the most well-known multivariate EDAs is the \bfit{Bayesian Optimization Algorithm (BOA)}. Unlike PBIL, which uses marginal distributions, BOA employs a Bayesian network to model dependencies between genes. This network is used to generate new samples and is updated at each generation.

\begin{tipsblock}[BOA: Further Reading]

    The following paper \cite{Pelikan1999} describes the BOA algorithm:

    \vspace{0.5em}

    \begin{center}
        Pelikan, Martin, David E. Goldberg, and Erick Cant√∫-Paz. 

        \textit{``BOA: The Bayesian optimization algorithm.''}

        Proceedings of the genetic and evolutionary computation conference GECCO-99.
        
        Vol. 1. 1999.
    \end{center}
\end{tipsblock}