\chapter{N-Gram Language Models}

Theorized around 1980-1990, they are a probabilistic model for language. The objective is to construct a generative model able to produce sentences which are likely to be drawn from the distribution of a corpus $\mathcal{C}$.

Mathematically speaking, we want to estimate the probability of a sequence of words: 
\[
P(y_1=w_1, y_2=w_2, \dots, y_n=w_n)
\]
where $y_i$ is a random variable representing the $i$-th word in a sentence and $w_i$ is a specific word in the vocabulary $\mathcal{V}$.

Since predicting the entire sequence directly is complex, the idea is to decompose this joint probability using the \textbf{chain rule} of probability:
\begin{align*}
P(y_1, y_2, \dots, y_n) &= P(y_1) P(y_2|y_1) P(y_3|y_1, y_2) \cdots P(y_n|y_1, y_2, \dots, y_{n-1}) \\  
&= \prod_{i=1}^{n} P(y_i | y_1, y_2, \dots, y_{i-1})
\end{align*}

The question now is: how do we approximate these conditional probabilities?
In our hands we have the \textbf{corpus} $\mathcal{C}$, so we can just count how many times a sequence of words appears in it and then divide it by the sum of all the possible continuations:
\[
P(y_i | y_1, y_2, \dots, y_{i-1}) \approx \frac{\text{Count}(y_1, y_2, \dots, y_{i-1}, y_i)}{\sum_{w \in \mathcal{V}} \text{Count}(y_1, y_2, \dots, y_{i-1}, w)}
\]

\begin{warningblock}
    The problem that arises now is that if the sequence has a high length, then it is very unlikely that it appears in the corpus, leading to a zero probability estimate. This is known as the \textbf{sparsity problem}.
\end{warningblock}

\section{N-Gram Models}

This models are a first approximation to the problem of predicting the next word in a sentence if never seen before. A simple example is the following:
\begin{itemize}
    \item "John wears a mask" $\in \mathcal{C}$;
    \item "Mary wears a mask" $\notin \mathcal{C}$;
    \item $P(mask | Mary wears a) = 0$.
\end{itemize}

We use the \textbf{Markov Assumption} to simplify the problem:
\[
P(y_i | y_1, y_2, \dots, y_{i-1}) \approx P(y_i | \underbrace{y_{i-(n-1)}, \dots, y_{i-1}}_{n-1})
\]
This means that the probability of the next word depends only on the previous $n-1$ words. This leads to the definition of \textbf{N-Gram Models}, where $n$ is a hyperparameter that defines the size of the context window.

The most common choices are:
\begin{itemize}
    \item \textbf{Unigram} ($n=1$): assumes that each word is independent of the others. The probability of a word is estimated based on its overall frequency in the corpus.
    \item \textbf{Bigram} ($n=2$): considers the previous word to predict the next one. The probability is estimated based on the frequency of word pairs.
    \item \textbf{Trigram} ($n=3$): takes into account the two preceding words. The probability is estimated based on the frequency of word triplets.
\end{itemize}

\begin{observationblock}
    There is the problem of dealing with words at the beginning and at the end of a sentence. Consider the following example:
    \begin{itemize}
        \item "<s> I am Sam </s>";
        \item "<s> Sam I am </s>";
        \item <s> I do not like this course </s>";
    \end{itemize}
    Then, considering a bigram model, we have:
    \begin{itemize}
        \item $P(I | <s>) = \frac{2}{3}$;
        \item $P(Sam | <s>) = \frac{1}{3}$
        \item $P(am | I) = \frac{\text{count(I am)}}{\text{count(I)}} = \frac{1}{3}$;
    \end{itemize}
    and so on.
    \paragraph{Remark.} When using n-grams with $n>2$ and padding, we need to add at the beginning of the sentence the relative numbero of <s> tokens.
\end{observationblock}

If the \textbf{context} of the n-gram is too long, we are just generating the sentences we saw, leading to probabilities close to 1 for those sentences and close to 0 for all the others. This is known as the \textbf{overfitting problem} and is a sign that n-grams don't generalize well.
A common solution is the generation by \textbf{iterative sampling}: sample the next word from the predicted distribution and then use it as context for the next prediction. This sequence ends when we sample the end token </s>.

Another problem is the fact that we use \textbf{log probabilities}, leading to numerical instability when dealing with zeroes. This is particularly relevant since the n-gram probability vector is \textbf{sparse}. 

\missing{Example of trigram}

To solve the problem of zero probabilities, we can also use 
\begin{itemize}
    \item the \textbf{Laplace Smoothing} technique, which consists in adding a small constant (usually 1) to the count of each n-gram. This ensures that no n-gram has a zero count, thus avoiding zero probabilities.
        \[
        P(y_i | y_{i-(n-1)}, \dots, y_{i-1}) \approx \frac{\text{Count}(y_{i-(n-1)}, \dots, y_{i-1}, y_i) + 1}{\sum_{w \in \mathcal{V}} (\text{Count}(y_{i-(n-1)}, \dots, y_{i-1}, w) + 1)}
        \]
    \item the \textbf{$\alpha$-Smoothing} technique, which is a generalization of Laplace Smoothing. Instead of adding 1 to each count, we add a small constant $\alpha > 0$. This allows for more flexibility in controlling the amount of smoothing.
        \[
        P(y_i | y_{i-(n-1)}, \dots, y_{i-1}) \approx \frac{\text{Count}(y_{i-(n-1)}, \dots, y_{i-1}, y_i) + \alpha}{\sum_{w \in \mathcal{V}} (\text{Count}(y_{i-(n-1)}, \dots, y_{i-1}, w) + \alpha)}
        \]
    \item using \textbf{Interpolation} between different n-gram models. For example, we can combine a bigram and a unigram model to estimate the probability of the next word:
        \[
        P(y_i | y_{i-1}) = \lambda P_{bigram}(y_i | y_{i-1}) + (1 - \lambda) P_{unigram}(y_i)
        \]
        where $\lambda \in [0, 1]$ is a hyperparameter that controls the balance between the two models. This is useful since when we go into unigrams, we don't have zeroes anymore. 
        \item using \textbf{Stupid Backoff}, which is a simpler version of interpolation. In this approach, if the higher-order n-gram has a zero count, we "back off" to the lower-order n-gram without any interpolation. For example, in a trigram model:
        \[
        P(y_i | y_{i-2}, y_{i-1}) = \begin{cases}
        \frac{\text{Count}(y_{i-2}, y_{i-1}, y_i)}{\text{Count}(y_{i-2}, y_{i-1})} & \text{if } \text{Count}(y_{i-2}, y_{i-1}) > 0 \\
        \alpha \frac{\text{Count}(y_{i-1}, y_i)}{\text{Count}(y_{i-1})} & \text{otherwise}
        \end{cases}
        \]
        where $\alpha$ is a discount factor (usually set to 0.4) that reduces the probability mass when backing off.
\end{itemize}

\paragraph{$\infty$-Gram} \textit{J.Liu et al,COLM,2024}
\[
P_{\infty}(y_i | y_1, \dots, y_{i-1}) = \frac{count(y_1, \dots, y_{i-1}, y_i)}{count(y_1, \dots, y_{i-1})}
\] 
where 
\[
i = \max \{ j | y_{j} \text{ is the last token before } y_i \}
\] 

\begin{warningblock}[Problems of N-Grams]
    \begin{itemize}
        \item Highly dependent on the dataset;
        \item Many reasonable sentences are rate 0-probable;
        \item Depend a lot on the choice of $n$;
    \end{itemize}
\end{warningblock}