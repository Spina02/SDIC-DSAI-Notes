\chapter{3D Vision}

\section{Image Formation}

The basic model is based on the principle of \textbf{camera obscura}, a room with a hole by which the light can enter the room.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/ch2/1.png}
    \caption{Camera Obscura principle.}
    \label{fig:camera_obscura}
\end{figure}

A simple setting for creating images on a white piece of paper is by projecting a shadow on it and, in the middle of the shadow, appears a picture of the scene in front of it. 

\begin{itemize}
    \item Leonardo da Vinci (1452-1519) was the first to describe the camera obscura in his notebooks.
    \item Johann Zahn (1685-1771) designed the first portable camera obscura in 1685.
    \item Joseph Nicephore Niepce (1765-1833) created the first permanent photograph in 1826 using a camera obscura.
\end{itemize}

\begin{minipage}{0.48\textwidth}
    From a geometrical point of view, the light rays of the object hit the film plane on different points, so we don't directly have the picture of the object. BUT, if we set a barrier in the middle, we have a one-to-one correspondence between the points of the object and the points of the film plane. That is why the pinhole camera works.
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{assets/ch2/2.png}
        \caption{Pinhole camera principle.}
        \label{fig:pinhole_camera}
    \end{figure}
\end{minipage}

\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{assets/ch2/3.png}
        \caption{Pinhole camera model.}
        \label{fig:pinhole_camera_model}
    \end{figure}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{itemize}
        \item The image is reversed upside-down and left-right;
        \item A \textit{virtual image} is formed in front of the camera.
    \end{itemize}
\end{minipage}

\paragraph{Perspective Projection} is composed by a \textit{principal plane}, parallel to the image plane and that contains the \textit{optical center ($C$)}. We then create the three axes, with the $Z$ one named \textit{principal axis}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/ch2/4.png}
\end{figure}

Point $M$ is projected onto the image plane at point $M'$. By similarity of triangles, it follows that the 3D point $[X,Y,Z]^T$ is mapped to the point 
\[
[X',Y',Z']^T = \left[-\frac{f}{Z}X, -\frac{f}{Z}Y, -f\right]^T
\]
where $\frac{f}{Z}$ is the \textit{perspective scale factor}. Farther away objects (larger $Z$ appear smaller).

\paragraph{Weak Perspective} If the object is thin w.r.t. its distance from the camera, then the perspective scale factor is roughly constand:
\[
\frac{f}{Z_0 + \Delta Z} \approx \frac{f}{Z_0}
\]
Then, perspective projection can be approximated by a \textit{scaled orthographic projection}
\[
X' = -\frac{f}{Z_0}X, \quad Y' = -\frac{f}{Z_0}Y
\]
\begin{tipsblock}
    Rule of thumb due to Leonardo da Vinci: $\frac{\Delta Z}{Z_0} < \frac{1}{10}$
\end{tipsblock}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/ch2/5.png}
    \caption{Weak perspective projection.}
    \label{fig:weak_perspective}
\end{figure}

The \textbf{Aperture} is related to the amount of light that enters the camera. A larger aperture (smaller $f$-number) allows more light to enter, resulting in a brighter image. However, a larger aperture also reduces the depth of field, which is the range of distances within which objects appear sharp in the image.
\begin{itemize}
    \item \textbf{Pinhole too big} $\rightarrow$ many directions are averaged, blurring the image; sharp but dark image, because little light reaches the sensor;
    \item \textbf{Pinhole too small} $\rightarrow$ diffraction effects blur the image; brught but blurred image, because many directions are averaged.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/6.png}
    \caption{Aperture effects.}
    \label{fig:aperture_effects}
\end{figure}

Solution? \textbf{Lenses}!

\newpage
\subsection{Lenses}

A lens collects rays departing from the same points and focuses them onto the screen. There is a specific distance at which objects are in focus. Perspective projection is still valid within the thin lens assumption.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/7.png}
    \caption{Lens principle. The gray areas represent the set of rays originated from the object.}
    \label{fig:lens_principle}
\end{figure}

\begin{itemize}
    \item A \textit{thin lens} is composed of a single piece of glass with very low, equal curvature on both sides;
    \item Any ray that enters parallel to the axis on one side of the lens proceeds towards the \textbf{focal point $F$};
    \item Any ray that passes through the center of the lens $C$ does not change its direction;
    \item The distance $D$ from the center to the focal point is called \textbf{focal length $f$};
    \item The image $M'$ of $M$ can be found by intersecting two rays.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/8.png}
\end{figure}

Let's now try to use mathematical notation. Based on triangle similarity:
\[
\frac{Y'}{Y} = \frac{Z'}{Z}, \quad \frac{Y'}{Y} = \frac{Z' - D}{D}
\]
Thus, we get the \textbf{thin lens equation}, also known as the lensmaker's formula:
\[
\frac{1}{Z} + \frac{1}{Z'} = \frac{1}{D}
\]
which basically tells that points that are far away from the lens ($Z \to \infty$) are focused at the focal length ($Z' = D = f$).
Point $M$ is projected, when in focus, into the same position of a pinhole model having the optical center located in the lens center $C$.

\begin{minipage}{0.48\textwidth}
    \begin{itemize}
        \item Two points lying on opposite sides of the lens at distances that satisfy the thin lens equation are \textbf{conjugate points};
        \item In Figure \ref{fig:conjugate_points}, $A$ and $A'$ are conjugate;
        \item Parallel rays from infinity focus at distance $D$ from the lens;
        \item As a source of light rays moves closer to the lens, they focus further away on the other side;
        \item Rays originating from a point at a distance $D$ from the lens become parallel after passing throught the lens, i.e., they focus at infinity;
        \item To focus on objects at different distances, move sensor relative to the lens;
        \item at $Z = Z' = 2D$ we have $1 : 1$ imaging, since $\frac{1}{2D} + \frac{1}{2D} = \frac{1}{D}$;
        \item Can't focus on objects closer to the lens than its focal length $D$.
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{assets/ch2/9.png}
        \caption{Conjugate points.}
        \label{fig:conjugate_points}
    \end{figure}
\end{minipage}

\vspace{0.5cm}

The \textbf{field of view} of a camera is the portion of the scene space that actually projects onto the sensor. It depends on the focusing distance and on the sensor width $w$. Conventionally it is defined when focusing at infinity:
\[
f.o.v. = 2\phi = 2\arctan \frac{w}{2D}
\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/ch2/10.png}
    \caption{Field of view.}
    \label{fig:field_of_view}
\end{figure}

\vspace{0.5cm}

The \textbf{blur circle} is the circle of confusion created when a point is not in focus. It is formed by rays and its size depends on $\Delta Z'$ and on the lens diameter $d$:
\[
\text{blur circle diameter} = d\frac{|\Delta Z'|}{Z'}
\]

The smaller the diameter, the smaller the blur circle, the sharper the image.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{assets/ch2/11.png}
    \caption{Blur circle formation.}
    \label{fig:blur_circle}
\end{figure}

Once fixed the diameter of the blur circle, locate the farthest and the closest planes whose points are projected inside the blur circle: the distance between these planes is known as \textbf{depth of field}. Moreover, for a fixed diameter of the blur circle, the depth of field changes as the diameter of the lens (aperture) changes; in particular, the depth of field increases as the aperture decreases.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/ch2/12.png}
    \caption{Depth of field.}
    \label{fig:depth_of_field}
\end{figure}

\begin{observationblock}[Aperture size]
    The aperture size is usually expressed by the $f$-number, defined as
    \[
f\text{-number} = \frac{D}{d}
    \]
    where $D$ is the focal length and $d$ is the diameter of the aperture. Smaller $f$-numbers correspond to larger apertures, allowing more light to enter the camera, while larger $f$-numbers correspond to smaller apertures, reducing the amount of light.
\end{observationblock}

\newpage 

\begin{minipage}[t]{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{assets/ch2/13.png}
        \caption{Lens aberrations.}
        \label{fig:lens_aberrations}
    \end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \vspace{0.5cm}
    Now, if we put a barrier with a hole in correspondence of the focus $F$, the light beams from $M$ are all blocked, except those parallel to the optical axis. The image of $M$ does not depend on $Z$ (depth), but only on $X$ and $Y$ and so the device performs an orthographic projection. This is a \textbf{Telecentric camera}.
\end{minipage}

Real (thick) lenses introduce several types of \textbf{aberrations} that cause deviations from the ideal pinhole camera model:
\begin{itemize}
    \item \textbf{Radial distortion} causes straight lines to appear curved in the image, especially towards the edges. It can be corrected using calibration techniques.
    \begin{itemize}
        \item \textbf{Barrel distortion} makes lines bulge outward from the center of the image.
        \item \textbf{Pincushion distortion} makes lines pinch inward towards the center of the image.
    \end{itemize}
    \item \textbf{Chromatic aberration} occurs because different wavelengths of light are focused at different points, leading to color fringing around high-contrast edges.
    \item \textbf{Spherical aberration} arises when light rays passing through the edges of a spherical lens focus at different points than those passing through the center, resulting in a blurred image.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/14.png}
    \caption{Examples of radial distortion}
    \label{fig:radial_distortion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/15.png}
    \caption{Example of chromatic aberration.}
    \label{fig:chromatic_aberration}
\end{figure}

\paragraph{Vignetting} is the darkening of image corners due to lens limitations or improper lens hoods. It can be corrected in post-processing. It is caused by \textit{compound lenses}, which may reduce aberrations but can introduce vignetting. In them, light beams emanating from object points located off-axis are partially blocked by individual lens components, resulting in a reduction of brightness toward the image periphery.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/16.png}
    \caption{Vignetting effect.}
    \label{fig:vignetting_effect}
\end{figure}

\paragraph{Sensing}, instead, is the process of converting the continuous image formed on the sensor into a discrete digital image. This involves sampling the continuous image at regular intervals (pixels) and quantizing the intensity values into discrete levels (bit depth). Since the sensor is a rectangular array of sensing elements (pixels), the image formed on the sensor is a rectangular portion of the image formed by the lens. The light charges a capacitor in each pixel, which is then read out and converted into a digital value. The longer the exposure time, the more light is collected, resulting in a brighter image. However, longer exposure times can also lead to motion blur if the camera or subject moves during the exposure. Finally, the capacitor is discharged periodically (the period is called \textit{shutter time}).

A simple sensor model is composed by:
\begin{itemize}
    \item an integrator that collects light over the exposure time $T$;
    \item a sampler that samples the integrated light at each pixel location;
    \item a quantizer that converts the sampled light intensity into discrete digital values.
\end{itemize}

Moreover, there are two approaches for sensing color:
\begin{itemize}
    \item the \textbf{3-sensor method}: a set of glass prisms uses a combination of internal reflection and refraction to split the incoming image into three, each reaching a different sensor;
    \item the \textbf{Bayer Mosaic}: half the pixels are sensitive to the green band of the light spectrum, while one quarter each to blue and red. This is consistent with the distribution of color-sensitive cones in the human retina. Color components that are not sensed (two per pixel), are inferred by interpolation from neighboring pixels.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/ch2/17.png}
    \caption{3-sensor beam splitter on the left, Bayer mosaic on the right.}
    \label{fig:color_sensing}
\end{figure}

\section{Camera Model}

The \textbf{simplified model} of a camera is based on the pinhole camera, where the image plane is placed in front of the pinhole to avoid the inversion of the image. 

\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{assets/ch2/18.png}
        \caption{Simplified pinhole camera model.}
        \label{fig:simplified_pinhole_camera}
    \end{figure}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{itemize}
        \item $(X,Y,Z)$ is the world reference frame, centered in $\mathcal{C} \in \mathcal{F}$;
        \item The $Z$ axis corresponds to the optical axis;
        \item $(u,v)$ is the image frame, centered in the intersection between the optical axis and the image plane;
        \item $u$ and $v$ are oriented as $X$ and $Y$;
        \item The perspective projection is a map $\mathbb{R}^3 \to \mathbb{R}^2$ such that 
        \[
        (x,y,z) \rightarrow \left(-\frac{fx}{z}, -\frac{fy}{z}\right)
        \]
        \item The map is nonlinear (due to the division by $z$);
        \item A linear map can be recovered by using homogeneous coordinates.
    \end{itemize}
\end{minipage}

\textbf{Homogeneus coordinates} allow to add \textbf{improper points} (points ad infinite) to the usual Euclidean plane (or space). The basic idea is to represent a point $(x,y)$ of the plane as a triplet (u,v,w) of real numbers, in such a way that:
\[
x = \frac{u}{w} \quad \text{and} \quad y = \frac{v}{w}
\]
where $w \neq 0$. The triplet (u,v,w) is called the \textbf{homogeneous representation} of the point (x,y). The null triplet (0,0,0) does not represent any point. Moreover, any two triplets (u,v,w) and (ku,kv,kw), with $k \neq 0$, represent the same point (x,y).

\begin{definitionblock}[Projective plane]
    The \textbf{projective plane} $\mathbb{P}^2$ is the set of equivalence classes of non-zero triplets (u,v,w), where two triplets are equivalent if they differ by a non-zero scale factor $k$. 
    \[
    (u,v,w) \sim \lambda(u,v,w) \quad \forall \lambda \neq 0
    \]
    The projective plane includes all points in the Euclidean plane (where $w \neq 0$) as well as points at infinity (where $w = 0$).
\end{definitionblock}

A similar definition holds for the \textbf{projective space} $\mathbb{P}^3$, where points are represented by quadruplets (X,Y,Z,W) and two quadruplets are equivalent if they differ by a non-zero scale factor $k$.

\begin{exampleblock}
    Consider the vector $(x',y',0)$. We have points that specify a direction but are at an infinite distance from focal point. So,
    \[
    \begin{pmatrix}
    x' \\
    y' \\
    0
    \end{pmatrix} \to \begin{pmatrix}
    \frac{x'}{0} \\ 
    \frac{y'}{0} \\
    \end{pmatrix} \to \begin{pmatrix}
    \infty \\
    \infty
    \end{pmatrix}
    \]
    Points whose last coordinate is zero are said to be \textbf{improper}.
\end{exampleblock}

Improper points represent a point at infinite along a direction specified by the non-zero coordinates. Points whose last coordinate is not zero, are said to be \textbf{proper}. To represent them, we can simply "add" a coordinate. With no loss of generality it may be taken as equal to 1, obtaining the so called \textbf{augmented vector}.
\[
(x,y) \to \begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
\quad 
(x,y,z) \to \begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}
\] 

Converting from homogeneus coordinates to Cartesian coordinates is called \textbf{dehomogenization} and amounts to dividing by th elast coordinate:
\[
\begin{bmatrix}
u \\
v \\
w
\end{bmatrix} \to \begin{bmatrix}
\frac{u}{w} \\
\frac{v}{w}
\end{bmatrix}
\quad
\begin{bmatrix} 
\frac{u}{w} \\
\frac{v}{w} \\
1
\end{bmatrix}
\]

\subsection{Transformations} 
Let the augmented vector corresponding to $x$ be denoted by $\bar{x}$. 
\begin{itemize}
    \item \textbf{Translation}: a translation of $t$ may be written as 
    \[
    x' = x + t 
    \]
    or 
    \[
    x' = \left[I | t\right]\bar{x}
    \] 
    or 
    \[
    \bar{x}' = \begin{bmatrix}
    I & t \\
    0^T & 1
\end{bmatrix} \bar{x}
    \]   
    \item \textbf{Rotation and translation} (rigid body motion):
    \[
    x' = Rx + t 
    \]
    or 
    \[
    x' = \left[R | t\right]\bar{x}
    \]
    or
    \[
    \bar{x}' = \begin{bmatrix}
    R & t \\
    0^T & 1
\end{bmatrix} \bar{x}
    \]
    where $R$ is an orthonormal matrix, thus $RR^T = R^TR = I$ and $\det(R) = 1$. 
    \begin{observationblock}
        There is also $\det(R) = -1$, which encodes a \textbf{mirroring} effect, not allowed in rigid transformations.
    \end{observationblock}
    In the 2D case, given the angle $\theta$ of rotation, $R$ takes the form 
    \[R = \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
    \end{bmatrix}\]
    where $R$ is a rotation matrix.
    \item \textbf{Similarity transformation} (where $s$ is the scale factor):
    \[
    x' = sRx + t 
    \]
    or 
    \[
    x' = \left[sR | t\right]\bar{x}
    \]
    or
    \[
    \bar{x}' = \begin{bmatrix}
    sR & t \\
    0^T & 1
\end{bmatrix} \bar{x}
    \]
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/ch2/19.png}
    \caption{2D Transformations}
    \label{fig:transformations}
\end{figure}

Let $m$ and $m'$ be the vectors of homogeneous coordinates of $M$ (w.r.t. the camera frame) and $M'$ (w.r.t. the image frame):
\[
m = \begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}, \quad
m' = \begin{bmatrix}
u \\
v \\
1
\end{bmatrix}
\]
Then 
\[
z \begin{bmatrix}
u \\
v \\    
1
\end{bmatrix} = \begin{bmatrix}
    -\frac{fx}{z} \\ 
    -\frac{fy}{z} \\
    1
\end{bmatrix} = \begin{bmatrix}
    -fx \\ 
    -fy \\
    z
\end{bmatrix} = \underbrace{\begin{bmatrix}
    -f & 0 & 0 & 0 \\
    0 & -f & 0 & 0 \\
    0 & 0 & 1 & 0 \\
\end{bmatrix}}_{\text{Projection Matrix}} \begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}
\]

Thus, in matrix notation 
\[
m' = \frac{1}{z} Pm 
\]

or equivalently 
\[
m' \sim Pm
\]
where $\sim$ means equality up to a scale factor.

\subsection{General Model}
\begin{minipage}{0.48\textwidth}
    For a more general camera model we need to consider:
    \begin{itemize}
        \item the rigid transformation from the world reference frame to the camera frame;
        \item the \textbf{pixelization} (from meters to pixel, in the image reference frame);
        \item The pixelization amounts to a \textbf{rescaling} followed by a translation:
        \[
        V = \begin{bmatrix}
            -k_u & 0 & u_0 \\ 
            0 & -k_v & v_0 \\
            0 & 0 & 1 \\
        \end{bmatrix}
        \]
        where $(u_0, v_0)$ are the coordinates (in pixels) of $\mathcal{C'}$;
        \item $k_u$ and $k_v$ the reciprocal of the pixel size along $u$ and $v$:
        \[
        k_u = \frac{1}{s_u} \quad k_v = \frac{1}{s_v} \quad \left[\text{pixel}\times m^{-1}\right]
        \]
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\textwidth]{assets/ch2/20.png}
        \caption{General camera model.}
        \label{fig:general_camera_model}
    \end{figure}
\end{minipage}

\vspace{0.5cm}

Thus the new $P$ taking into account the sensor is:
\[
P = \begin{bmatrix}
    -k_u & 0 & u_0 \\
    0 & -k_v & v_0 \\
    0 & 0 & 1 \\
\end{bmatrix} \begin{bmatrix}
    -f & 0 & 0 & 0 \\
    0 & -f & 0 & 0 \\
    0 & 0 & 1 & 0 \\
\end{bmatrix} = \begin{bmatrix}
    f k_u & 0 & -u_0 & 0 \\
    0 & f k_v & -v_0 & 0 \\
    0 & 0 & 1 & 0 \\
\end{bmatrix} = K\left[I | 0\right]
\]
where, by posing $fk_u = \alpha_u$ and $fk_v = \alpha_v$, we have 
\[
K = \begin{bmatrix}
    \alpha_u & 0 & u_0 \\
    0 & \alpha_v & v_0 \\
    0 & 0 & 1 \\
\end{bmatrix} \quad \text{and} \quad \left[I | 0\right] = \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
\end{bmatrix}
\]

\begin{itemize}
    \item $K$ is the \textbf{calibration matrix};
    \item $K$ describes the camera intrinsics (those parameters that depend on the camera itself, as opposite to the camera's pose in space, which are called extrinsics);
    \item Notice that $K$ is upper triangular.
\end{itemize}

Let's now take into account the rigid transformation from the world reference frame to the camera frame and the pixelization. Let $m_c$ be the vector of homogeneous coordinates of $M$ with respect to the camera reference frame and let 
\[
G = \begin{bmatrix}
    R & t \\
    0^T & 1 \\
\end{bmatrix}
\]
be the rigid body motion from the world frame to the camera frame (a rotation $R$ followed by a translation $t$). It follows that 
\[
m_c = G m
\]
The matrix $G$ represents the camera extrinsics. The perspective projection is thus:
\[
m' \sim K\left[I | 0\right] Gm 
\]
The perspective projection matrix in the general case is 
\[
P = K\left[I | 0\right] G = K\left[R | t\right]
\]
By partitioning $R$ and $t$ row-wise:
\[
R = \begin{bmatrix}
r_1^T \\
r_2^T \\
r_3^T \\
\end{bmatrix}, \quad t = \begin{bmatrix}
t_1 \\
t_2 \\
t_3 \\
\end{bmatrix}
\]
we obtain 
\[
P = \begin{bmatrix}
    \alpha_u r_1^T + u_0 r_3^T & \alpha_u t_1 + u_0 t_3 \\
    \alpha_v r_2^T + v_0 r_3^T & \alpha_v t_2 + v_0 t_3 \\
    r_3^T & t_3 \\
\end{bmatrix}
\]

\begin{observationblock}
    \begin{enumerate}
        \item A more general form of $K$ takes into account a further intrinsic parameter (the skew angle between the $u$ and $v$ axes, usually very close to $\frac{\pi}{2}$), thus 
        \[
K = \begin{bmatrix}
    \alpha_u & \alpha_u \cot \theta & u_0 \\
    0 & \frac{\alpha_v}{\sin \theta} & v_0 \\
    0 & 0 & 1 \\
\end{bmatrix}
        \]
        \item $P$ is defined up to a scale factor (for $\lambda \neq 0$, $\lambda P$ results in the same projection as $P$). A prospective projection matrix is said to be \textit{normalized} if has the form 
        \[
        \begin{bmatrix}
            * & * & * & \vline & * \\
            * & * & * & \vline & * \\
            \hline
            & a^T & & \vline & * \\
        \end{bmatrix}
        \]
        \item $P$ has 12 entries, but (due to the scale factor) 11 degrees of freedom. Indeed, it depends on $6+5=11$ independent parameters (6 extrinsic and 5 intrinsic).
        \item Partitioning $P$ row-wise
        \[
        P = \begin{bmatrix}
            p_1^T \\
            p_2^T \\
            p_3^T \\
        \end{bmatrix}
        \]
        we get 
        \[
        m' = Pm = \begin{bmatrix}
            p_1^T m \\
            p_2^T m \\
            p_3^T m \\
        \end{bmatrix} \rightarrow^{\text{back to inhomogeneous}} \begin{cases}
            u = \frac{p_1^T m}{p_3^T m} \\
            v = \frac{p_2^T m}{p_3^T m} \\
        \end{cases}
        \]
        where $u$ and $v$ are the Cartesian coordinates of point $M'$ in the image plane. Thus the last pair or equations is the generalization of the basic perspective projection equations.
    \end{enumerate}
\end{observationblock}

\newtheorem{theorem}{Theorem}
\begin{theorem}
    Let $P = \left[Q | q\right]$ be $3\times 4$ matrix and let $q_i^t, i=1,2,3$ denote the rows of $Q$.
    \begin{itemize}
        \item A necessary and sufficient condition for $P$ to be a perspective projection matrix is that 
        \[
        \det(Q) \neq 0
        \]
        \item A necessary and sufficient condition for $P$ to be a zero-skew perspective projection matrix is that 
        \[
        \det(Q) \neq 0 \quad \text{and} \quad (q_1 \times q_3)^T(q_2 \times q_3) = 0
        \]
    \end{itemize}
\end{theorem}

\subsection*{Center of Projection}

\begin{minipage}{0.48\textwidth}
    The focal plane $\mathcal{F}$ contains the points that are projected to the infinite (because the straight line through $C$ does not intersect the image plane). Thus its equation is
    \[
    p_3^T x = 0
    \]
    This is the equation of the focal plane and all points that satisfy it are in the focal plane, meaning that their projection goes to infinity. The planes $p_1^T x = 0$ and $p_2^T x = 0$ project to the axes $u = 0$ and $v = 0$, respectively. 

    The center of projection $C$ belongs to all the three planes, thus it is defined by 
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{assets/ch2/21.png}
        \caption{Center of projection.}
        \label{fig:center_of_projection}    
    \end{figure}
\end{minipage}

\[
\begin{cases}
p_1^T x = 0 \\
p_2^T x = 0 \\
p_3^T x = 0 \\
\end{cases}
\]
In other words, $C$ is represented (in homogeneous coordinates) by the kernel of $P$. 

$P$ has rank 3 by construction (both $K$ and $R$ are invertible) hence, by the Rank-Nullity Theorem 
\[
\text{dim}(\text{Ker}(P)) = 4 - \text{rank}(P) = 4 - 3 = 1
\]
Let
\[
\tilde{c} = \left[x_C y_C z_C\right]^t
\]
be the vector of the Cartesian coordiantes of $C$ w.r.t. the world frame. We can write 
\[
P_c = \left[Q | q\right] \begin{bmatrix}
\tilde{c} \\
1
\end{bmatrix} = 0
\]
then, solving for $\tilde{c}$ we get
\[
\tilde{c} = -Q^{-1} q
\]

\subsection*{Optical Ray}
\begin{minipage}{0.48\textwidth}
    The optical ray of an image point $M'$ is the locus of points in space that project onto $M'$. May be expressed as the line through $C$ and a point $\alpha$ located at infinity that projects onto $M'$ 
    \[
\alpha = \begin{bmatrix}
    Q^{-1} m' \\
    0
    \end{bmatrix}
    \]
    thus a parametrics representation (in homogeneous coordinates) of the optical ray is 
    \[
    m = c + \lambda\begin{bmatrix}
    Q^{-1} m' \\
    0
    \end{bmatrix}, \quad \lambda \in \mathbb{R}
    \]
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{assets/ch2/22.png}
        \caption{Optical ray.}
        \label{fig:optical_ray}
    \end{figure}
\end{minipage}

\begin{advancedblock}[Depth of a point]
    Consider a point $M$ whose Cartesian coordinates w.r.t. the world reference frame are $\tilde{m} = \left[x_M y_M z_M\right]^T$. The projection equation with explicit scale factor $\zeta$ is 
    \[
    \zeta m' = Pm 
    \]
    Representing $M$ in homogeneous coordinates as $m = \left[\tilde{m}^T 1\right]^T$ we get 
    \[
    \begin{bmatrix}
    \zeta u \\
    \zeta v \\
    \zeta z \\
    \end{bmatrix} = P \begin{bmatrix}
        \tilde{m} \\    
        1 \\
    \end{bmatrix} 
    \]
    If $P$ is normalized, we obtain, from the third scalar equation, that
    \[
    \zeta = r_3^T \tilde{m} + t_3
    \]
    Recalling that $m_c = Gm$, it follows that $\zeta$ is the third coordinate of $M$ w.r.t. the camera frame or the distance from $M$ to the focal plane (depth of the point $M$). In other words, if
    \begin{enumerate}
        \item the last homogeneous coordinate is chosen as 1;
        \item $P$ is normalized;
    \end{enumerate}
    then the last homogeneous projected coordinate $\zeta$ is the depth of the point. 
\end{advancedblock}

\section{Camera Calibration}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ch2/23.png}
    \caption{Camera calibration.}
    \label{fig:camera_calibration}
\end{figure}

Camera calibration is the process of estimating the intrinsic and extrinsic parameters of a camera. This is typically done by capturing images of a known calibration pattern (e.g., a checkerboard) from different viewpoints and using these images to solve for the camera parameters.
\begin{itemize}
    \item \textbf{Camera calibration problem}: estimate intrinsic and extrinsic camera parameters, based on some measurements;
    \item \textbf{Calibration pattern}: a 3D object of known geometry and generating image features which can be located accurately;
    \item Parameters may be estimated based on a set of known 3D-2D correspondences;
    \item \textbf{Direct methods}: formulate a problem in which the camera parameters are the unknowns;
    \item \textbf{Indirect methods}: first estimate the projection matrix $P$ and then decompose it to get intrinsic and extrinsic parameters.
\end{itemize}

\subsection{Direct Linear Transform}