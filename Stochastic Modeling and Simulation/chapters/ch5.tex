\newpage
\chapter{Lecture 17/03/2025}


\section{Probability Density Function and Markov Processes}

Consider a stochastic process governed by the stochastic differential equation
$$
dx = a(x)\,dt + b(x)\,dW.
$$
Let $\rho(x,t)$ denote the probability density function (PDF) of $x(t)$, so that the probability of finding $x(t)$ in the interval $[\hat{x},\,\hat{x}+d\hat{x}]$ is given by
$$
\Pr\Bigl[x(t) \in [\hat{x},\,\hat{x}+d\hat{x}]\Bigr] = \rho(\hat{x}, t)\,d\hat{x}.
$$
In this way, the state of the system $x(t)$ is fully characterized by its PDF, $\rho(x,t)$.

More generally, if we consider
$$
x \in \mathbb{R}, \quad t \in \mathbb{R},
$$
the stochastic process $x(t)$ has the state space (SSP) $\mathbb{R}$ and evolves in continuous time. The probability that the process takes a value in a small interval at time $t$ depends on its past history,
$$
\Pr\Bigl[x(t) \in [\hat{x},\,\hat{x}+d\hat{x}]\Bigr] = \kappa\Bigl[\{x(\theta) \}_{0 \le \theta \le t}\Bigr],
$$
where $\kappa$ represents the functional dependence on the trajectory $\{x(\theta)\}$ for $0\le\theta\le t$.

\subsubsection{Markov Process}

A process is said to possess the \textbf{Markov property} if its future evolution depends solely on its present state rather than the entire past history. For the SDE above, the increment over an infinitesimal time interval $dt$ can be written as
$$
x(t+dt) = x(t) + a(x)\,dt + b(x)G_t\sqrt{dt},
$$
where $G_t$ is a Gaussian random variable with mean $0$ and variance $1$. Note that the update depends only on the current state $x(t)$, which exemplifies the Markov property.

To further illustrate this idea, consider a simple discrete deterministic process:
$$
x_{t+1} = a\, x_t, \quad t \in \mathbb{N}_0.
$$
Its solution is given by
$$
x_t = a^t x_0.
$$
Now, if we add a stochastic term to account for random fluctuations, we obtain
$$
x_{t+1} = a\, x_t + \omega\, \nu_t,
$$
where $\nu_t$ is a random variable representing noise. In this context, the distribution of $x_t$ at time $t$, denoted by $\rho(x,t)$, evolves according to the stochastic dynamics. Often, this distribution can be expressed as
$$
\rho(x,t) = L(x_t),
$$
where $L(x_t)$ denotes the law governing the evolution of the process.

This example highlights that in a Markov process the next state is determined exclusively by the most recent state rather than by the full history of the process.
