\chapter{Raw Lecture Notes} \label{ch:raw}

\begin{warningblock}[Raw Lecture Notes]
    The following chapters contain unstructured notes taken during lectures and may require further organization and refinement.
\end{warningblock}


\section{Lecture 11/04/2025}

---

$$
\dot x = a(x) + b(x) \eta_h (t)
$$

$$
\langle \eta_h(t)\rangle = 0
$$

$\mathbb R_h$ is a function with a peak at $|z| < h$ and $\mathbb R_h (z) = 0$ for $|z| > h$.

\vspace{0.5em}

If the limit $\lim_{h \to 0^+} \mathbb R_h (z) = \delta(\tau)$, then:

$$
\dot x_h = a(x_h) + b(x_h) \eta_h (t)
\quad \to \quad
\dot x = a(x) + b(x) \circ \xi(t)
$$

---

Let's consider a population and two opinions:

$$
x + y = 1
$$

People can change their opinion with a rate $R$ and the ratio of changing from $x$ to $y$ is $\theta$, and from $y$ to $x$ is $k$.

$$
\begin{cases}
    \dot x = + \theta xy - k yx - \varepsilon x + \varepsilon y \\
    \dot y = - \theta xy + k yx + \varepsilon x - \varepsilon y
\end{cases}
$$

where $\varepsilon$ is the rate of changing opinion. (?)

Substituting $y = 1 - x$ we can rewrite the first equation as:
$$
\dot x = x(1-x) (\theta - k) + \varepsilon (1-x) - \varepsilon x
$$

$$
\dfrac{dx}{dt} = \lambda x(1-x) + 1 - 2 x, \quad \lambda \to \lambda + \alpha \xi(t)
$$

$$
dx = \underbrace{\{\lambda x(1-x) + 1 - 2 x\}}_{a(x)} dt + \underbrace{\alpha x(1-x)}_{b(x)} \circ dW
$$

\dots

$$
dx = \left\{
    1 - 2x
\right\} dt + \alpha x(1-x) \circ dW
$$

$$
a(x) = \tfrac 12 b'(x) b(x),
\qquad
b(x) = \alpha (x - x^2),
\qquad
b'(x) = \alpha (1 - 2x)
$$

so

$$
1 - 2x = \dfrac {\alpha^2}2 x(1-x)(1-2x)
$$

which has two equilibrium points:

$$
x_1 = \dfrac 12, \qquad x_2: 1 = \dfrac {\alpha^2}2 x(1-x) \ \ (?)
$$

\dots

---

$$
m \ddot x = - \gamma_T \dot x + F_T(x) + \omega_T \xi(t)
$$

$$
\begin{cases}
    \dot x = v \\
    \dot v = - \dfrac{\gamma_T}m v + \dfrac{F_T(x)}m + \dfrac{\omega_T}m \xi(t)
\end{cases}
$$

to simplify the notation we can write:
$$
\gamma = \dfrac{\gamma_T}m, \qquad
F = \dfrac{F_T(x)}m, \qquad
\omega = \dfrac{\omega_T}m, \qquad
\underbrace{U = \int F_T(x) dx, \qquad
U' = \dfrac{dU}{dx} = F_T(x)}_{?}
$$

we get:

$$
\begin{cases}
    \dot x = v \\
    \dot v = - \gamma v + F(x) + \omega \xi(t)
\end{cases}
$$

$$
\dfrac{\partial p}{\partial t} = - v \dfrac{\partial p}{\partial x} - \dfrac{\partial}{\partial v} \left[
    (F(x) - \gamma v) p
\right] + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
\dfrac{\partial p}{\partial t} = - v \dfrac{\partial p}{\partial x} - F(x) \dfrac{\partial p}{\partial v} + \gamma \dfrac{\partial}{\partial v}(vp) + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
0 = - v \dfrac{\partial p}{\partial x} + U'(x) \dfrac{\partial p}{\partial v} + \gamma p + \gamma v \dfrac{\partial p}{\partial v} + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
p(x,v) = A(x)B(v)
$$
$$
-v A'(x) B(v) + U'(x) A(x) B'(v) + \gamma A(x) B(v) + \gamma v A(x) B'(v) + \dfrac {\omega^2} 2 B''(v) = 0
$$

$$
\overbrace{\underbrace{- v \dfrac{A'(x)}{A(x)} + U'(x) \dfrac{B'(v)}{B(v)}}_{= \ 0}}^{1^{st} \text{ term}} + 1\overbrace{\gamma + \gamma v \dfrac{B'(v)}{B(v)} + \dfrac {\omega^2} 2 \dfrac{B''(v)}{B(v)}}^{2^{nd} \text{ term}} = 0
$$

We have to options:
 
\begin{enumerate}
    \item set the second term to zero
    \item boh
\end{enumerate}

Let's consider the first option and let's define some "test" variables $B_T$ and $B'_T$. We have:

$$
\dfrac{B'_T(v)}{B_T(v)} = - \eta v 
\quad \Rightarrow \quad
B'_T(v) = - \eta v B_T(v)
\quad \Rightarrow \quad
B(v) = C e^{-\eta v^2/2}
$$

we have

$$
B'(v) = - \eta v B(v), \qquad
B''(v) = - \eta v B'(v) = - \eta v \left( - \eta v B(v) \right)
$$

\missing{boh}

$$
P_s = \dfrac 1z e ^{- \tfrac \gamma{\omega^2} v^2 - \tfrac{2 \gamma}{\omega^2} U(x)}
=
\dfrac 1z e ^{- \tfrac {2\gamma}{\omega^2} \left[
    \tfrac{v^2}2 + U(x)
\right]}
$$

Applying back the transformation we have:

$$
p(x,v) = \dfrac 1z e ^{- \tfrac {2\gamma_T}{\omega_T} \left[
    \tfrac{mv^2}2 + U_T(x)
\right]}
$$
so:
$$
\iint p_s(x,v)dxdv = 1, \qquad
\dfrac 1z \iint e ^{- \tfrac {2\gamma_T}{\omega_T^2} E_T(x,v)} dxdv = 1
$$

\todo{check if this is correct}

\missing{fishes example ?}

$$
dx = f(x) dt - \underbrace{(cx dt + \omega x dW)}_{\# \text{fishes killed in } (t,\ t + dt)}
$$

We want the number of fishes to be positive.

\missing{end of the lecture}

\section{Lecture: 05/05/2025}

\dots

... if there is no linearity, ...

$$
\dfrac{\partial P}{\partial t} = - \dfrac{\partial}{\partial x} \left\{
    (\theta \int z P(z,t) \dd z + (1-\theta)x - x^3)P
\right\} + \dfrac{\omega^2}2 \dfrac {\partial^2 P}{\partial x^2} \qquad N \gg 1
$$

$$
M(t) = \int_{\mathbb R} x P(x,t) dx
$$

$$
P_s(x,M_s) = C(M_s) \exp \left\{
    \theta M_s x + ...
\right\}
$$

\missing{end of the formula above}

This solution is not actually so "usable"

$$
M_s = \int_{\mathbb R} x P_s(x; M_s) dx \quad \Rightarrow \quad M_s = \Psi (M_s)
$$

$$
M_s = \Psi(M_s) \quad \to \quad \text{"unique solution"}
$$

There are more interesting cases, for instance when $\Psi(M_s)$ has more than one solution:

In this case, our system has more than one steady states. It means that we loose the unicity of the solution (so there is no more global actractiveness)

\vspace{0.5em}

E.g: 

$$\dot X_i = f(x_i, <x>) + g(x_i) \xi_i \quad N \gg 1$$

$$
\dot x = f(x,M(t)) + g(x) \xi(t)
$$

$$
M(t) = \int z P(z,t) dt
$$

The Fokker-Plank equation will be:

$$
\dfrac{\partial P}{\partial t} = - \dfrac \partial{\partial x} [f(x, M(t))] + \dfrac 12 \dfrac {\partial^2}{\partial x^2} [g^2(x)P]
$$

The steady state solutions will be the solutions of the following equation:

$$
\begin{cases}
0 = -\dfrac {\dd}{\dd x} [t(x,M_s)P] + \dfrac {\dd^2}{\dd x^2} \left[
    \dfrac{g^2(x)}2 P
\right]
\\[1em]
M_s = \int z P_s (z; M_s) dx
\end{cases}
$$

$$
\boxed{
    M_s = \Psi(M_s)
}
$$

\todo{add linking sentence}

$$
P(x, M_s, \theta) = C(M, \theta) \exp \left[
    \dfrac 2\omega \left(
        \theta M_s x + (1-\theta) \dfrac {x^2}2 - \dfrac {x^4}4
    \right)
\right]
$$

$$
\boxed{M_s = 0}
$$

\dots (?)

$0 < \theta < \theta_c$

\dots (?)

So we have two solutions:

$$
\begin{array}{l}
    M_s = a\\
    M_s = -a
\end{array}
$$

\begin{exampleblock}
    $$
    M_s = \Psi(M_s; \theta)
    $$

    $$
    \begin{cases}
        y = M_s\\
        y = \Psi(M_s; \theta)
    \end{cases}
    \quad \Rightarrow \quad
    \begin{array}{c}
        P_s(x;M_1)\\
        P_s(x;M_2)\\
        P_s(x;M_3)
    \end{array}
    $$

    so for $\theta = \theta_1$ we have \bfit{multistability}, while for $\theta = \theta_2$ we have \bfit{monostability}.

\end{exampleblock}

\begin{theorem}
    $$
    \left|
        \left.\dfrac {\dd \Psi}{\dd M_s}
    \right|_{M_s = M_c}\right| < 1
    \quad \Rightarrow \quad
    P_s(x;M_1, \theta^1) \text{ is locally stable}
    $$
    $$
    \left|
        \left.\dfrac {\dd \Psi}{\dd M_s}
    \right|_{M_s = M_c}\right| > 1
    \quad \Rightarrow \quad
    P_s(x;M_2, \theta^2) \text{ is locally unstable}
    $$
\end{theorem}

\begin{center}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=0.8\textwidth]{assets/ex.png}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=0.8\textwidth]{assets/ex2.png}
    \end{minipage}%
\end{center}

\dots

\newpage

$$
\dot x = (ax + x^3 - x^5) - D(x - M(t)) + \alpha (1+x^2) \odot \xi(t)
$$

$$
M = \Psi(M;D,\alpha)
$$

we have that for small $D$ and $\alpha$ we have a unique solution, while for large $D$ and $\alpha$ we have 5 different solutions.

\begin{center}
    \begin{minipage}{0.4\textwidth}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{assets/ex_2.png}
            \caption{1 solution}
        \end{figure}
        \end{minipage}%
    \begin{minipage}{0.4\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{assets/ex_1.png}
        \caption{5 solutions}
    \end{figure}
    \end{minipage}
\end{center}

So 0 is always a solution, and from a certain value of $D$ we have 5 solutions, 3 of which are stable and 2 are unstable.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{assets/ex_3.png}
    \caption{Stable solutions of the system}
\end{figure}

\dots

$$
\dot x = F(x_i, <x>) + g(x_i, <x>) \xi_i(t)
$$

An example iis the movement of a guitar string that vibrates.

$$
m_i \ddot x_i = - \gamma \dot x_i
$$

If the deviation is big, we have a function in the copmplex space, but if the deviation is small, we can use the linear approximation.

$$
m_j \ddot z_j = - \gamma \dot z_j - k(z_j - z_{j-1}) - k(z_j - z_{j+1}) = - \gamma \dot z_j - k \left(z_{j-1} - 2z_j + z_{j+1}\right)
$$

we have now a discretization of the position of the string $z(t,x)$:

$$
m_j \dfrac {\dd^2 z}{\dd t^2} (t, x_j) = - \gamma \dfrac {\dd z}{\dd t} (t,x_j) + k [z(t, x_j + D) - 2z(t,x_j) + z(t, x_j - D)] 
$$

$$
\mu \dfrac{\partial^2 z}{\partial t^2} = - \gamma \dfrac{\partial z}{\partial t} + c \dfrac{\partial^2 z}{\partial x^2} + \hat \omega \xi(x,t)
$$

We can see the stochastic term as the wind that moves the string.

\section{Lecture: 09/05/2025}

\subsection{Spatiotemporal noisy model}

$$
\dfrac {\partial \phi}{\partial t} = f(\phi) + g(\phi) \xi_m(r,t) + D \mathcal L[\phi] + h(\phi) F(t) + \xi_a(r,t)
$$

\begin{itemize}
\item $f(\phi)$: deterministic part
\item $g(\phi)$: multiplicative noise
\item $D \mathcal L[\phi]$: linear part
\item $h(\phi)$: additive noise
\end{itemize}

L[\phi] is a Laplacian or a integral operator

Examples:

\begin{itemize}
\item
$$
\mathcal L[\phi] = \nabla^2 \phi
$$
\item
$$
\mathcal L[\phi] = -a_0 \nabla^2 \phi - \nabla^4 \phi
$$
\item
$$
\mathcal L[\phi] = -(\nabla^2 + k_0^2)^2 \phi = - (K_0^2 + 2 k_0 \nabla^2 + \nabla^4) \phi
$$
\end{itemize}

\begin{observationblock}
    If we apply the fourier transform of:
    $$
    \mathcal L[\phi] = -(\nabla^2 + k_0^2)^2 \phi = - (K_0^2 + 2 k_0 \nabla^2 + \nabla^4) \phi
    $$

    we get:

    $$
    ... F(\phi)
    $$

\end{observationblock}

$$
\mathcal L[\phi(r)] = \int \phi(r') \omega(r-r') \dd r'
$$

How do we simulate this equation? We can simply obtain the domain and discretize it.

Lattice.based Approximation:

$$
...
$$

Field coupling approximation:

$$
l(\phi_i, \phi_j) = w_i \phi_i + \sum_{j \in nn(i)} w_j \phi_j
$$

For example:

$$
\mathcal L[\phi] = \nabla^2 \phi \approx l(\phi_i, \phi_j) = \dfrac 1{\Delta^2} \sum_{j \in nn(i)} (\phi_j - \phi_i)
$$

If we have a stochastic process which is discrete in time and space, we have:

$$
\left\langle \xi(r,t) \xi(r',t') \right\rangle = sC \left(
    \dfrac{|r-r'|}{d}, \dfrac{|t-t'|}{\tau_c}
\right)
$$

As in the purely temporal noise, $\tau_c$ is a measure of the temporal memory of the noise, $d$ is the spatial memory of the noise.

The spatiotemporal brother of the ornstein-uhlenbeck noise is "Ojalvo e al" noise.

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \nabla^2 \phi + \xi_{gn}
$$

\begin{observationblock}[Ojalvo e al and the Ornstein-Uhlenbeck process]
    If we set $D = 0$ we have a series of Ornstein-Uhlenbeck processes at each point of the domain.
    $$
    \dfrac{\partial \phi}{\partial t} = a \phi + \xi_{gn}
    $$
\end{observationblock}

Noise induced pattenrs ยง

$$
\dfrac{\partial \phi}{\partial t} = f(\phi) + g(\phi) \xi_m(r,t) + D \mathcal L[\phi] + ...
$$
\dots

Perturbed Swift-Hohenberg model:

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \mathcal L[\phi] + \xi_{gn} ...
$$

$$
\dfrac{\partial \phi}{\partial t} = f(\phi) + D \mathcal L[\phi] = a\phi - D (\nabla^2 + k_0^2)^2 \phi
$$

Transitory pattern that disappear.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{assets/perturbed_swift_hohenberg_model.png}
%     \caption{Perturbed Swift-Hohenberg model}
%     \label{fig:perturbed_swift_hohenberg_model}
% \end{figure}

Additive noise generate pattenrs

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \mathcal L[\phi] + \xi_{gn}, \qquad \mathcal L[\phi] = - (\nabla^2 + k_0^2)^2 \phi
$$

Permanent patterning: details change in time.

we can distinguish two cases:

\begin{itemize}
\item $a < 0$ 

\item $a > 0$ 

\end{itemize}

with multiplicative noise it can induce bimodality in the pdf of $\phi$:

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{assets/pdf_bimodality.png}
%     \caption{Perturbed Swift-Hohenberg model}
%     \label{fig:perturbed_swift_hohenberg_model}
% \end{figure}

$$
\dfrac{\partial \phi}{\partial t} = a \phi - phi^3 + \phi \xi_{gn} + D \mathcal L[\phi]
$$ 

---

A bad model of glaciations

$$
\dd x = [x(a-x^2) + A \cos \Omega t] \dd t
$$

where

\begin{itemize}
\item $x$: is the (normalized) Earth's temperature
\item $A \cos(\Omega t)$: small periodic variations of the solar irradiation. 
\end{itemize}

We have that if $A$ is small, $x(t)$ fluctuates around $+\sqrt(a)$.

The model fails.

Including stochastic noise:

$$
\dd x = [x(a-x^2) + A \cos \Omega t] \dd t + \varepsilon \dd W
$$

where $\varepsilon$ is the noise intensity.

This time we have a white noise, according to:

\begin{itemize}
    \item $\varepsilon$ is small: the noise is negligible
    \item $\varepsilon$ is large: the noise is dominant
\end{itemize}

they finally managed to model the glaciations.

---

\subsubsection{Spatial Stochastic Resonance}

$$
\dfrac{\partial \phi}{\partial t} = a\phi - \phi^3 + D \dfrac{\partial^2 \phi}{\partial x^2} + F(t) + \varepsilon \xi_{gn}
$$

\todo{check the formula}

\dots