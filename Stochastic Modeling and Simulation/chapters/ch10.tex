\chapter{Raw Lecture Notes} \label{ch:raw}

\begin{warningblock}[Raw Lecture Notes]
    The following chapters contain unstructured notes taken during lectures and may require further organization and refinement.
\end{warningblock}

\section{Lecture: 04/04/2025}

\dots

$$
\dfrac{\partial P}{\partial t} (x,t) = \int_{S \in s} P(s,t) \Omega(s, x) ds - P(x,t) \int_{a \in s} \Omega(x, a) da
$$ 

\dots

$$
\dfrac{\partial P}{\partial t} (x,t) = \underbrace{r P(x+1, t)}_{backward} + \underbrace{r P(x-1, t)}_{forward} - 2 r P(x,t)
$$

\dots

More in general we can write

$$
\Omega(s, a) = \sum_{j \in \mathbb Z} K_{s,a} \delta(s-a-j)
$$
$$
\Rightarrow \quad
\dfrac{\partial P}{\partial t} (x,t) = \sum_{j \in \mathbb Z} \int P(s,t) K_{s,x} \delta(s-x-j) ds - P(x,t) \sum_{j \in \mathbb Z} \int K_{x,a} \delta(x-a-j) da
$$

$$
\dfrac{\partial P (x,t)}{\partial t} = \left(
    \sum_{j \in \mathbb Z} K_{x,x+j} K_{x+j,x} P(x+j,t)
    \right)
    - P(x,t) \left( \sum_{j \in \mathbb Z} K_{x,x-j} \right)
$$

\dots

$$
\begin{cases}
    S' = - \beta \dfrac IN S\\
    I' = \beta \dfrac IN S - \gamma I
\end{cases}
$$

This system represents the dynamics of a population of individuals that can be in one of two states: susceptible (S) or infected (I). The parameter $\beta$ represents the rate at which susceptible individuals become infected, while $\gamma$ represents the rate at which infected individuals recover.

We have $(R = N - S - I)$ and:

$$I(t) \ge 0; \quad S(t) \ge 0; \quad R(t) \ge 0$$ 

$$(t, t + dt) X(t) = (S(t), I(t)) \in \mathbb{R}^2$$

The "removal" of individuals accounts for the recovery of infected individuals, its probability is given by:

$$
\Pr \left[
    \big(S(t+dt), I(t + dt)\big) = \big(S(t), I(t) - 1\big)\ \bigg|\ \big(s(t), I(t)\big)
\right] = \gamma dt
$$

\dots

$$
\underbrace{\binom SI}_t \to \underbrace{\binom S{I-1} = \binom SI + \binom{0}{-1}}_{t + dt}
$$

The contagion process instead is given by:

$$
\underbrace{\binom{S}{I}}_t \to \underbrace{\binom{S}{I} + \binom{-1}{1}}_{t + dt}
$$

\dots

We can think of $\sigma$ and $\alpha$ as the states of the system at time $t$ and $t + dt$, respectively: 

$$\displaystyle \sigma = \binom {S_\sigma}{I_\sigma},\qquad \alpha = \binom{S_\alpha}{I_\alpha}$$

\dots

$$
\Omega(\sigma, \alpha) = \gamma I_{\sigma} \delta 
\left(
    \sigma - \alpha - \binom 0{-1}
\right) + \beta \dfrac{I_{\sigma}}N S_\sigma \Omega 
\left(
    \sigma - \alpha - \binom {-1}{1}
\right)
$$

\dots

$$
\dfrac{\partial P(S, I, t)}{\partial t} = \beta \dfrac{(I-1)}{N} (S + 1) P(S + 1, I - 1, t) + \gamma (I + 1) P(S, I + 1, t) - \left( \gamma I + \beta \dfrac{I}{N} S \right) P(S, I, t)
$$

Your starting point is an integer position; whatever jump you take, x + dt must be another integer position.
In fact, the probability sitribution is non-zero only for integer positions.

\section{Discrete time, Discrete state space}

State $s$ is discrete

Time $t \subseteq \mathbb N \cup \{0\}$

$$
P \{
    x(t+1) = \alpha \big| x(t) = \sigma
\} = \theta_{\sigma \alpha} \in [0,1]
$$

$$
P\{
    x(t) = \omega
\} = P_\omega(t)
$$

The probability of beiing in a state $\alpha$ at time $t + 1$ is given by the probability of being in state $\sigma$ at time $t$ multiplied by the transition probability from $\sigma$ to $\alpha$:

$$
\boxed{P_\alpha(t+1) = \sum_{\sigma \in S} \Pr_\sigma(t) \theta_{\sigma \alpha} }$$
$$
P_\alpha(t+1) = P_\alpha (t) \theta_{\alpha \alpha} + \sum_{\sigma \in S  \backslash \{\alpha\}} P_\sigma(t) \theta_{\sigma \alpha}
$$

$$
\theta_{\alpha, \alpha} = 1 - \sum_{\beta \in S \backslash \{\alpha\}}
$$

$$
P_\alpha(t+1) = P_\alpha(t) \left[
    1 - \sum_{\beta \in S \backslash \{\alpha\}} \theta_{\alpha, \beta}
\right] + \sum_{\sigma \in S \backslash \{\alpha\}} P_\sigma(t) \theta_{\sigma, \alpha}
$$

$$
P_\alpha(t+1) = P_\alpha(t) + \sum_{\sigma} P_\sigma(t) \theta_{\sigma, \alpha} - P_\alpha(t) \sum_{\beta \in S \backslash \{\alpha\}} \theta_{\alpha, \beta}
$$
$$
P_\alpha(t+1) - P_\alpha(t) = \sum_{\sigma} P_\sigma(t) \theta_{\sigma, \alpha} - P_\alpha(t) \sum_{\beta \in S \backslash \{\alpha\}} \theta_{\alpha, \beta}
$$

$$
\dfrac{P_\alpha(t+1) - P_\alpha(t)}U = \sum_{\sigma} P_\sigma(t) \dfrac {\theta_{\sigma, \alpha}}{U} - P_\alpha(t) \sum_\beta \dfrac {\theta_{\alpha, \beta}}{U}
$$

\dots

\section{SIS model}

A SIS model is a simple model of disease spread in a population. In this model, individuals can be in one of two states: susceptible (S) or infected (I). The dynamics of the system are governed by two parameters: the infection rate $\beta$ and the recovery rate $\gamma$. The model assumes that individuals can move between these two states, with susceptible individuals becoming infected at a rate proportional to the number of infected individuals they come into contact with, and infected individuals recovering at a constant rate.

Moreover, $\mu$ is the natural death rate of the population, which is assumed to be the same as the birth rate. This means that the population size remains constant over time, and the total number of individuals in the population is given by $N = S + I$.

$$
\begin{cases}
    S' = \mu - \mu S - \beta I S \\
    I' = \beta I S - (\mu + \gamma) I
\end{cases},
\qquad
\begin{cases}
    \mu \to \mu + \omega_\mu \xi_\mu \\
    \beta \to \beta + \omega_\beta \xi_\beta \\
\end{cases},
\qquad
\xi (t) = \binom {\xi_\mu}{\xi_\beta}
.
$$

This is actually a stochastic model of disease spread, where the parameters $\mu$, $\beta$, are subject to random fluctuations. 

\dots

[a lot of stuff missing]

\dots

Case whit indipendent noise and ...

$$
x_j = x(jh)
$$
$$
x_{j+1} = x_j + \alpha (x_j) h + \beta (x_j) \left[
    \begin{smallmatrix}
        G_j \\
        G_{j+1}
    \end{smallmatrix}
\right] \sqrt{h}
$$

\dots

\section{ma che cazzo ne so}

Suppose we are in a region of $R^N$ and all the points follows the following law:
$$
\dot x_i = f(x_i)
$$
$$
n(x, 0) = \tilde O (x)
$$
$$
\inf n(x,t)dx = N
$$
$$
\int n(x,0) dx = \int \tilde O (x) dx = N 
$$
$$
c = [a,b]
$$
$$
N_c(t) = \int_a^b n(x,t)dx
$$
$$
P[x(t) \in [a,b]] \simeq \dfrac {N_c(t)}n
$$

\dots

\section{ma che cazzo ne so pt.2}

Let's consider an interval $[t, t + dt]$ and some particles that follows the law:

$$
\dot x = f(x) \equiv v(x)
$$

Then, the number of particles that enter and exit the interval $[x, x + dx]$ at time $t$ is given by:

$$
\left[
\begin{array}{ll}
    Enter: & n(x,t) v(x) dt \\
    Exit: & n(x + dt,t) v(x + dt) dt \\
\end{array}
\right.
$$

\dots

[missing a lot of stuff]

\dots

the product $n \cdot v$ is called \bfit{current} density and is denoted by $J(x,t)$:

$$
\begin{cases}
    \dfrac{\partial n}{\partial t} + \dfrac{\partial}{\partial x} J(x,t) = 0 \\
    J(x,t) = n(x,t) v(x)
\end{cases},
\qquad
\dfrac{\partial n}{\partial t} + \text{div} J(x,t) = 0
$$

\dots

... probabiulity current ...


\section{Lecture 11/04/2025}

---

$$
\dot x = a(x) + b(x) \eta_h (t)
$$

$$
\langle \eta_h(t)\rangle = 0
$$

$\mathbb R_h$ is a function with a peak at $|z| < h$ and $\mathbb R_h (z) = 0$ for $|z| > h$.

\vspace{0.5em}

If the limit $\lim_{h \to 0^+} \mathbb R_h (z) = \delta(\tau)$, then:

$$
\dot x_h = a(x_h) + b(x_h) \eta_h (t)
\quad \to \quad
\dot x = a(x) + b(x) \circ \xi(t)
$$

---

Let's consider a population and two opinions:

$$
x + y = 1
$$

People can change their opinion with a rate $R$ and the ratio of changing from $x$ to $y$ is $\theta$, and from $y$ to $x$ is $k$.

$$
\begin{cases}
    \dot x = + \theta xy - k yx - \varepsilon x + \varepsilon y \\
    \dot y = - \theta xy + k yx + \varepsilon x - \varepsilon y
\end{cases}
$$

where $\varepsilon$ is the rate of changing opinion. (?)

Substituting $y = 1 - x$ we can rewrite the first equation as:
$$
\dot x = x(1-x) (\theta - k) + \varepsilon (1-x) - \varepsilon x
$$

$$
\dfrac{dx}{dt} = \lambda x(1-x) + 1 - 2 x, \quad \lambda \to \lambda + \alpha \xi(t)
$$

$$
dx = \underbrace{\{\lambda x(1-x) + 1 - 2 x\}}_{a(x)} dt + \underbrace{\alpha x(1-x)}_{b(x)} \circ dW
$$

\dots

$$
dx = \left\{
    1 - 2x
\right\} dt + \alpha x(1-x) \circ dW
$$

$$
a(x) = \tfrac 12 b'(x) b(x),
\qquad
b(x) = \alpha (x - x^2),
\qquad
b'(x) = \alpha (1 - 2x)
$$

so

$$
1 - 2x = \dfrac {\alpha^2}2 x(1-x)(1-2x)
$$

which has two equilibrium points:

$$
x_1 = \dfrac 12, \qquad x_2: 1 = \dfrac {\alpha^2}2 x(1-x) \ \ (?)
$$

\dots

---

$$
m \ddot x = - \gamma_T \dot x + F_T(x) + \omega_T \xi(t)
$$

$$
\begin{cases}
    \dot x = v \\
    \dot v = - \dfrac{\gamma_T}m v + \dfrac{F_T(x)}m + \dfrac{\omega_T}m \xi(t)
\end{cases}
$$

to simplify the notation we can write:
$$
\gamma = \dfrac{\gamma_T}m, \qquad
F = \dfrac{F_T(x)}m, \qquad
\omega = \dfrac{\omega_T}m, \qquad
\underbrace{U = \int F_T(x) dx, \qquad
U' = \dfrac{dU}{dx} = F_T(x)}_{?}
$$

we get:

$$
\begin{cases}
    \dot x = v \\
    \dot v = - \gamma v + F(x) + \omega \xi(t)
\end{cases}
$$

$$
\dfrac{\partial p}{\partial t} = - v \dfrac{\partial p}{\partial x} - \dfrac{\partial}{\partial v} \left[
    (F(x) - \gamma v) p
\right] + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
\dfrac{\partial p}{\partial t} = - v \dfrac{\partial p}{\partial x} - F(x) \dfrac{\partial p}{\partial v} + \gamma \dfrac{\partial}{\partial v}(vp) + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
0 = - v \dfrac{\partial p}{\partial x} + U'(x) \dfrac{\partial p}{\partial v} + \gamma p + \gamma v \dfrac{\partial p}{\partial v} + \dfrac{\partial^2}{\partial v^2} \dfrac {\omega^2} 2
$$

$$
p(x,v) = A(x)B(v)
$$
$$
-v A'(x) B(v) + U'(x) A(x) B'(v) + \gamma A(x) B(v) + \gamma v A(x) B'(v) + \dfrac {\omega^2} 2 B''(v) = 0
$$

$$
\overbrace{\underbrace{- v \dfrac{A'(x)}{A(x)} + U'(x) \dfrac{B'(v)}{B(v)}}_{= \ 0}}^{1^{st} \text{ term}} + 1\overbrace{\gamma + \gamma v \dfrac{B'(v)}{B(v)} + \dfrac {\omega^2} 2 \dfrac{B''(v)}{B(v)}}^{2^{nd} \text{ term}} = 0
$$

We have to options:
 
\begin{enumerate}
    \item set the second term to zero
    \item boh
\end{enumerate}

Let's consider the first option and let's define some "test" variables $B_T$ and $B'_T$. We have:

$$
\dfrac{B'_T(v)}{B_T(v)} = - \eta v 
\quad \Rightarrow \quad
B'_T(v) = - \eta v B_T(v)
\quad \Rightarrow \quad
B(v) = C e^{-\eta v^2/2}
$$

we have

$$
B'(v) = - \eta v B(v), \qquad
B''(v) = - \eta v B'(v) = - \eta v \left( - \eta v B(v) \right)
$$

\missing{boh}

$$
P_s = \dfrac 1z e ^{- \tfrac \gamma{\omega^2} v^2 - \tfrac{2 \gamma}{\omega^2} U(x)}
=
\dfrac 1z e ^{- \tfrac {2\gamma}{\omega^2} \left[
    \tfrac{v^2}2 + U(x)
\right]}
$$

Applying back the transformation we have:

$$
p(x,v) = \dfrac 1z e ^{- \tfrac {2\gamma_T}{\omega_T} \left[
    \tfrac{mv^2}2 + U_T(x)
\right]}
$$
so:
$$
\iint p_s(x,v)dxdv = 1, \qquad
\dfrac 1z \iint e ^{- \tfrac {2\gamma_T}{\omega_T^2} E_T(x,v)} dxdv = 1
$$

\todo{check if this is correct}

\missing{fishes example ?}

$$
dx = f(x) dt - \underbrace{(cx dt + \omega x dW)}_{\# \text{fishes killed in } (t,\ t + dt)}
$$

We want the number of fishes to be positive.

\missing{end of the lecture}

\section{Lecture: 05/05/2025}

\dots

... if there is no linearity, ...

$$
\dfrac{\partial P}{\partial t} = - \dfrac{\partial}{\partial x} \left\{
    (\theta \int z P(z,t) \dd z + (1-\theta)x - x^3)P
\right\} + \dfrac{\omega^2}2 \dfrac {\partial^2 P}{\partial x^2} \qquad N \gg 1
$$

$$
M(t) = \int_{\mathbb R} x P(x,t) dx
$$

$$
P_s(x,M_s) = C(M_s) \exp \left\{
    \theta M_s x + ...
\right\}
$$

\missing{end of the formula above}

This solution is not actually so "usable"

$$
M_s = \int_{\mathbb R} x P_s(x; M_s) dx \quad \Rightarrow \quad M_s = \Psi (M_s)
$$

$$
M_s = \Psi(M_s) \quad \to \quad \text{"unique solution"}
$$

There are more interesting cases, for instance when $\Psi(M_s)$ has more than one solution:

In this case, our system has more than one steady states. It means that we loose the unicity of the solution (so there is no more global actractiveness)

\vspace{0.5em}

E.g: 

$$\dot X_i = f(x_i, <x>) + g(x_i) \xi_i \quad N \gg 1$$

$$
\dot x = f(x,M(t)) + g(x) \xi(t)
$$

$$
M(t) = \int z P(z,t) dt
$$

The Fokker-Plank equation will be:

$$
\dfrac{\partial P}{\partial t} = - \dfrac \partial{\partial x} [f(x, M(t))] + \dfrac 12 \dfrac {\partial^2}{\partial x^2} [g^2(x)P]
$$

The steady state solutions will be the solutions of the following equation:

$$
\begin{cases}
0 = -\dfrac {\dd}{\dd x} [t(x,M_s)P] + \dfrac {\dd^2}{\dd x^2} \left[
    \dfrac{g^2(x)}2 P
\right]
\\[1em]
M_s = \int z P_s (z; M_s) dx
\end{cases}
$$

$$
\boxed{
    M_s = \Psi(M_s)
}
$$

\todo{add linking sentence}

$$
P(x, M_s, \theta) = C(M, \theta) \exp \left[
    \dfrac 2\omega \left(
        \theta M_s x + (1-\theta) \dfrac {x^2}2 - \dfrac {x^4}4
    \right)
\right]
$$

$$
\boxed{M_s = 0}
$$

\dots (?)

$0 < \theta < \theta_c$

\dots (?)

So we have two solutions:

$$
\begin{array}{l}
    M_s = a\\
    M_s = -a
\end{array}
$$

\begin{exampleblock}
    $$
    M_s = \Psi(M_s; \theta)
    $$

    $$
    \begin{cases}
        y = M_s\\
        y = \Psi(M_s; \theta)
    \end{cases}
    \quad \Rightarrow \quad
    \begin{array}{c}
        P_s(x;M_1)\\
        P_s(x;M_2)\\
        P_s(x;M_3)
    \end{array}
    $$

    so for $\theta = \theta_1$ we have \bfit{multistability}, while for $\theta = \theta_2$ we have \bfit{monostability}.

\end{exampleblock}


\newtheorem{theorem}{Theorem}

\begin{theorem}
    $$
    \left|
        \left.\dfrac {\dd \Psi}{\dd M_s}
    \right|_{M_s = M_c}\right| < 1
    \quad \Rightarrow \quad
    P_s(x;M_1, \theta^1) \text{ is locally stable}
    $$
    $$
    \left|
        \left.\dfrac {\dd \Psi}{\dd M_s}
    \right|_{M_s = M_c}\right| > 1
    \quad \Rightarrow \quad
    P_s(x;M_2, \theta^2) \text{ is locally unstable}
    $$
\end{theorem}

\begin{center}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=0.8\textwidth]{assets/ex.png}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \includegraphics[width=0.8\textwidth]{assets/ex2.png}
    \end{minipage}%
\end{center}

\dots

\newpage

$$
\dot x = (ax + x^3 - x^5) - D(x - M(t)) + \alpha (1+x^2) \odot \xi(t)
$$

$$
M = \Psi(M;D,\alpha)
$$

we have that for small $D$ and $\alpha$ we have a unique solution, while for large $D$ and $\alpha$ we have 5 different solutions.

\begin{center}
    \begin{minipage}{0.4\textwidth}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{assets/ex_2.png}
            \caption{1 solution}
        \end{figure}
        \end{minipage}%
    \begin{minipage}{0.4\textwidth}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{assets/ex_1.png}
        \caption{5 solutions}
    \end{figure}
    \end{minipage}
\end{center}

So 0 is always a solution, and from a certain value of $D$ we have 5 solutions, 3 of which are stable and 2 are unstable.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{assets/ex_3.png}
    \caption{Stable solutions of the system}
\end{figure}

\dots

$$
\dot x = F(x_i, <x>) + g(x_i, <x>) \xi_i(t)
$$

An example iis the movement of a guitar string that vibrates.

$$
m_i \ddot x_i = - \gamma \dot x_i
$$

If the deviation is big, we have a function in the copmplex space, but if the deviation is small, we can use the linear approximation.

$$
m_j \ddot z_j = - \gamma \dot z_j - k(z_j - z_{j-1}) - k(z_j - z_{j+1}) = - \gamma \dot z_j - k \left(z_{j-1} - 2z_j + z_{j+1}\right)
$$

we have now a discretization of the position of the string $z(t,x)$:

$$
m_j \dfrac {\dd^2 z}{\dd t^2} (t, x_j) = - \gamma \dfrac {\dd z}{\dd t} (t,x_j) + k [z(t, x_j + D) - 2z(t,x_j) + z(t, x_j - D)] 
$$

$$
\mu \dfrac{\partial^2 z}{\partial t^2} = - \gamma \dfrac{\partial z}{\partial t} + c \dfrac{\partial^2 z}{\partial x^2} + \hat \omega \xi(x,t)
$$

We can see the stochastic term as the wind that moves the string.

\section{Lecture: 09/05/2025}

\subsection{Spatiotemporal noisy model}

$$
\dfrac {\partial \phi}{\partial t} = f(\phi) + g(\phi) \xi_m(r,t) + D \mathcal L[\phi] + h(\phi) F(t) + \xi_a(r,t)
$$

\begin{itemize}
\item $f(\phi)$: deterministic part
\item $g(\phi)$: multiplicative noise
\item $D \mathcal L[\phi]$: linear part
\item $h(\phi)$: additive noise
\end{itemize}

L[\phi] is a Laplacian or a integral operator

Examples:

\begin{itemize}
\item
$$
\mathcal L[\phi] = \nabla^2 \phi
$$
\item
$$
\mathcal L[\phi] = -a_0 \nabla^2 \phi - \nabla^4 \phi
$$
\item
$$
\mathcal L[\phi] = -(\nabla^2 + k_0^2)^2 \phi = - (K_0^2 + 2 k_0 \nabla^2 + \nabla^4) \phi
$$
\end{itemize}

\begin{observationblock}
    If we apply the fourier transform of:
    $$
    \mathcal L[\phi] = -(\nabla^2 + k_0^2)^2 \phi = - (K_0^2 + 2 k_0 \nabla^2 + \nabla^4) \phi
    $$

    we get:

    $$
    ... F(\phi)
    $$

\end{observationblock}

$$
\mathcal L[\phi(r)] = \int \phi(r') \omega(r-r') \dd r'
$$

How do we simulate this equation? We can simply obtain the domain and discretize it.

Lattice.based Approximation:

$$
...
$$

Field coupling approximation:

$$
l(\phi_i, \phi_j) = w_i \phi_i + \sum_{j \in nn(i)} w_j \phi_j
$$

For example:

$$
\mathcal L[\phi] = \nabla^2 \phi \approx l(\phi_i, \phi_j) = \dfrac 1{\Delta^2} \sum_{j \in nn(i)} (\phi_j - \phi_i)
$$

If we have a stochastic process which is discrete in time and space, we have:

$$
\left\langle \xi(r,t) \xi(r',t') \right\rangle = sC \left(
    \dfrac{|r-r'|}{d}, \dfrac{|t-t'|}{\tau_c}
\right)
$$

As in the purely temporal noise, $\tau_c$ is a measure of the temporal memory of the noise, $d$ is the spatial memory of the noise.

The spatiotemporal brother of the ornstein-uhlenbeck noise is "Ojalvo e al" noise.

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \nabla^2 \phi + \xi_{gn}
$$

\begin{observationblock}[Ojalvo e al and the Ornstein-Uhlenbeck process]
    If we set $D = 0$ we have a series of Ornstein-Uhlenbeck processes at each point of the domain.
    $$
    \dfrac{\partial \phi}{\partial t} = a \phi + \xi_{gn}
    $$
\end{observationblock}

Noise induced pattenrs ยง

$$
\dfrac{\partial \phi}{\partial t} = f(\phi) + g(\phi) \xi_m(r,t) + D \mathcal L[\phi] + ...
$$
\dots

Perturbed Swift-Hohenberg model:

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \mathcal L[\phi] + \xi_{gn} ...
$$

$$
\dfrac{\partial \phi}{\partial t} = f(\phi) + D \mathcal L[\phi] = a\phi - D (\nabla^2 + k_0^2)^2 \phi
$$

Transitory pattern that disappear.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{assets/perturbed_swift_hohenberg_model.png}
%     \caption{Perturbed Swift-Hohenberg model}
%     \label{fig:perturbed_swift_hohenberg_model}
% \end{figure}

Additive noise generate pattenrs

$$
\dfrac{\partial \phi}{\partial t} = a \phi + D \mathcal L[\phi] + \xi_{gn}, \qquad \mathcal L[\phi] = - (\nabla^2 + k_0^2)^2 \phi
$$

Permanent patterning: details change in time.

we can distinguish two cases:

\begin{itemize}
\item $a < 0$ 

\item $a > 0$ 

\end{itemize}

with multiplicative noise it can induce bimodality in the pdf of $\phi$:

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{assets/pdf_bimodality.png}
%     \caption{Perturbed Swift-Hohenberg model}
%     \label{fig:perturbed_swift_hohenberg_model}
% \end{figure}

$$
\dfrac{\partial \phi}{\partial t} = a \phi - phi^3 + \phi \xi_{gn} + D \mathcal L[\phi]
$$ 

---

A bad model of glaciations

$$
\dd x = [x(a-x^2) + A \cos \Omega t] \dd t
$$

where

\begin{itemize}
\item $x$: is the (normalized) Earth's temperature
\item $A \cos(\Omega t)$: small periodic variations of the solar irradiation. 
\end{itemize}

We have that if $A$ is small, $x(t)$ fluctuates around $+\sqrt(a)$.

The model fails.

Including stochastic noise:

$$
\dd x = [x(a-x^2) + A \cos \Omega t] \dd t + \varepsilon \dd W
$$

where $\varepsilon$ is the noise intensity.

This time we have a white noise, according to:

\begin{itemize}
    \item $\varepsilon$ is small: the noise is negligible
    \item $\varepsilon$ is large: the noise is dominant
\end{itemize}

they finally managed to model the glaciations.

---

\subsubsection{Spatial Stochastic Resonance}

$$
\dfrac{\partial \phi}{\partial t} = a\phi - \phi^3 + D \dfrac{\partial^2 \phi}{\partial x^2} + F(t) + \varepsilon \xi_{gn}
$$

\todo{check the formula}

\dots